---
tags:
  - opencode
  - ollama
  - local-ai
  - prompt
  - request
  - ai-assistant
created: <% tp.file.creation_date() %>
---

# OpenCode 一键调用 / One-Click OpenCode Call

> [!info] 使用说明
> 此模板实现从 Obsidian 直接调用 OpenCode 本地模型，无需手动复制命令到终端。

## 📋 当前文件信息

> [!info] 文件上下文
> - **路径**: `<% tp.file.path %>`
> - **标题**: `<% tp.file.title %>`
> - **创建时间**: `<% tp.file.creation_date() %>`

<%*
// 获取文件上下文
const ctx = await tp.user.getFileContext(tp);

if (ctx) {
-%>

### 📊 内容统计
| 指标 | 值 |
|------|-----|
| **字符数** | `<% ctx.charCount %>` |
| **段落数** | `<% ctx.content.split('\n\n').length %>` |
| **链接数** | `<% ctx.links.length %>` |
| **标签** | `<% ctx.tags.length > 0 ? ctx.tags.map(t => '`' + t + '`').join(', ') : '无' %>` |

<%
} else {
-%>
> [!warning] 无法获取文件上下文

<%
}
-%>

---

## 🤖 执行 OpenCode

<%*
// 调用一键执行函数
const result = await tp.user.oneClickExecute(tp);
-%>

<% result %>

---

## 📝 后续操作

> [!tip] 完成后操作
>
> - [ ] 检查输出结果是否符合预期
> - [ ] 如需要，重新运行并调整参数
> - [ ] 将有用的内容整理到对应笔记
> - [ ] 记录问题或改进建议

---

## 🔧 故障排除

### 常见问题

| 问题 | 可能原因 | 解决方案 |
|------|----------|----------|
| Ollama 服务不可用 | Ollama 未启动 | 运行 `ollama serve` |
| 命令执行失败 | Templater 未启用系统命令 | 在插件设置中启用 `enable_system_commands` |
| 模型未找到 | 模型未下载 | 使用 `ollama pull <model>` 下载模型 |
| 输出被截断 | 输出超过缓冲区大小 | 分块处理或增加任务范围 |
| 执行超时 | 任务太复杂或模型太大 | 增加 timeout 或使用更小模型 |

### 调试模式

<details>
<summary>点击查看调试信息</summary>

```bash
# 检查 Ollama 服务状态
curl http://localhost:11434/api/tags

# 检查可用模型
ollama list

# 测试 OpenCode 连接
opencode --version

# 手动执行命令测试
opencode run "测试" --model ollama/qwen2.5:7b
```

</details>

---

## 📚 快捷命令参考

```bash
# 查看 Ollama 模型列表
ollama list

# 下载新模型
ollama pull qwen2.5:7b
ollama pull qwen2.5-coder:7b

# 查看模型详情
ollama show qwen2.5:7b

# 重启 Ollama 服务
pkill ollama && ollama serve &

# 查看 OpenCode 帮助
opencode --help
```

---

*本模板由 OpenCode 本地模型集成计划生成 / Generated by OpenCode Local Integration Plan*
