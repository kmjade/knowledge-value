# ğŸ¤– GPT-oss:120b-cloudäº‘ç«¯é…ç½®æŒ‡å—

> ğŸ¯ **ç« èŠ‚ç›®æ ‡**ï¼šé…ç½®GPT-oss:120b-cloudäº‘ç«¯å¤§è¯­è¨€æ¨¡å‹ä¸ç³»ç»Ÿé›†æˆ | â±ï¸ **é¢„è®¡æ—¶é—´**ï¼š60åˆ†é’Ÿ | ğŸ“Š **éš¾åº¦ç­‰çº§**ï¼šâ­â­

## ğŸ“– æ¦‚è¿°

GPT-oss:120b-cloudæ˜¯ä¸€ä¸ª1200äº¿å‚æ•°çš„äº‘ç«¯å¤§è¯­è¨€æ¨¡å‹ï¼Œæä¾›å¼ºå¤§çš„è‡ªç„¶è¯­è¨€ç†è§£ã€ä»£ç ç”Ÿæˆã€çŸ¥è¯†æ¨ç†ç­‰èƒ½åŠ›ã€‚æœ¬æŒ‡å—å°†è¯¦ç»†ä»‹ç»å¦‚ä½•é…ç½®GPT-oss:120b-cloudå¹¶ä¸Obsidianã€OpenCodeã€Ollamaæ— ç¼é›†æˆã€‚

## ğŸŒŸ GPT-oss:120b-cloudç‰¹æ€§

### ğŸ§  æ ¸å¿ƒèƒ½åŠ›
| èƒ½åŠ›ç±»å‹ | è¯¦ç»†æè¿° | åº”ç”¨åœºæ™¯ |
|----------|----------|----------|
| **è‡ªç„¶è¯­è¨€ç†è§£** | æ·±åº¦ç†è§£æ–‡æœ¬è¯­ä¹‰å’Œä¸Šä¸‹æ–‡ | æ™ºèƒ½é—®ç­”ã€æ–‡æ¡£åˆ†æ |
| **ä»£ç ç”Ÿæˆ** | é«˜è´¨é‡ä»£ç ç”Ÿæˆå’Œä¼˜åŒ– | ç¼–ç¨‹è¾…åŠ©ã€ä»£ç å®¡æŸ¥ |
| **çŸ¥è¯†æ¨ç†** | å¤æ‚é€»è¾‘æ¨ç†å’Œé—®é¢˜è§£å†³ | å­¦æœ¯ç ”ç©¶ã€å†³ç­–æ”¯æŒ |
| **å¤šè¯­è¨€æ”¯æŒ** | æ”¯æŒä¸­è‹±æ–‡ç­‰å¤šç§è¯­è¨€ | å›½é™…åŒ–åº”ç”¨ã€ç¿»è¯‘ |
| **é•¿ä¸Šä¸‹æ–‡å¤„ç†** | æ”¯æŒ8192+tokenä¸Šä¸‹æ–‡ | é•¿æ–‡æ¡£åˆ†æã€å¤æ‚ä»»åŠ¡ |

### âš¡ æ€§èƒ½ä¼˜åŠ¿
- **é«˜ååé‡**ï¼šæ¯ç§’å¤„ç†æ•°åƒä¸ªtoken
- **ä½å»¶è¿Ÿ**ï¼šå¹³å‡å“åº”æ—¶é—´<500ms
- **é«˜å¯ç”¨æ€§**ï¼š99.9%æœåŠ¡å¯ç”¨æ€§
- **å¼¹æ€§æ‰©å±•**ï¼šæ ¹æ®è´Ÿè½½è‡ªåŠ¨æ‰©å®¹

## ğŸ”‘ APIå¯†é’¥è·å–

### ğŸ“ æ³¨å†Œè´¦æˆ·

#### 1. è®¿é—®å®˜ç½‘
```bash
# è®¿é—®GPT-osså®˜ç½‘
https://cloud.gpt-oss.com
```

#### 2. åˆ›å»ºè´¦æˆ·
```bash
# æ³¨å†Œæ–°ç”¨æˆ·æµç¨‹
1. ç‚¹å‡»"ç«‹å³æ³¨å†Œ"
2. å¡«å†™é‚®ç®±ã€å¯†ç 
3. éªŒè¯é‚®ç®±åœ°å€
4. å®Œå–„ä¸ªäººä¿¡æ¯
5. é€‰æ‹©å¥—é¤è®¡åˆ’
```

#### 3. å®åè®¤è¯
```bash
# å®åè®¤è¯æ‰€éœ€ææ–™
- èº«ä»½è¯æ­£åé¢ç…§ç‰‡
- è”ç³»ç”µè¯éªŒè¯
- é‚®ç®±éªŒè¯ç 
- äººè„¸è¯†åˆ«ï¼ˆå¯é€‰ï¼‰
```

### ğŸ”‘ APIå¯†é’¥ç®¡ç†

#### ç”ŸæˆAPIå¯†é’¥
```bash
# ç”Ÿæˆæ–°APIå¯†é’¥
curl -X POST "https://api.gpt-oss.com/v1/keys" \
  -H "Authorization: Bearer YOUR_MASTER_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "name": "Obsidian Integration",
    "description": "ç”¨äºObsidiançŸ¥è¯†åº“é›†æˆ",
    "permissions": ["read", "write"],
    "rate_limit": 1000
  }'
```

#### å¯†é’¥å“åº”ç¤ºä¾‹
```json
{
  "key_id": "key_123456789",
  "api_key": "sk-gpt-oss-xxxxx-xxxxx-xxxxx",
  "created_at": "2026-01-21T10:00:00Z",
  "expires_at": "2027-01-21T10:00:00Z",
  "permissions": ["read", "write"],
  "rate_limit": 1000
}
```

#### å¯†é’¥å®‰å…¨å­˜å‚¨
```bash
# åˆ›å»ºå®‰å…¨é…ç½®æ–‡ä»¶
mkdir -p ~/.config/gpt-oss
chmod 700 ~/.config/gpt-oss

# åˆ›å»ºå¯†é’¥æ–‡ä»¶
cat > ~/.config/gpt-oss/keys.json << EOF
{
  "production": "sk-gpt-oss-prod-xxxxx-xxxxx-xxxxx",
  "development": "sk-gpt-oss-dev-xxxxx-xxxxx-xxxxx",
  "test": "sk-gpt-oss-test-xxxxx-xxxxx-xxxxx"
}
EOF

# è®¾ç½®æ–‡ä»¶æƒé™
chmod 600 ~/.config/gpt-oss/keys.json
```

## ğŸ”§ APIé…ç½®

### ğŸ“‹ åŸºç¡€é…ç½®

#### ç¯å¢ƒå˜é‡è®¾ç½®
```bash
# ~/.bashrc æˆ– ~/.zshrc
export GPT_OSS_API_KEY="sk-gpt-oss-xxxxx-xxxxx-xxxxx"
export GPT_OSS_BASE_URL="https://api.gpt-oss.com/v1"
export GPT_OSS_MODEL="gpt-oss-120b-cloud"
export GPT_OSS_TIMEOUT=30
export GPT_OSS_MAX_TOKENS=4096
```

#### é…ç½®æ–‡ä»¶
```yaml
# ~/.config/gpt-oss/config.yaml
version: "1.0"
environment: "production"

# APIé…ç½®
api:
  base_url: "https://api.gpt-oss.com/v1"
  api_key: "${GPT_OSS_API_KEY}"
  model: "gpt-oss-120b-cloud"
  timeout: 30
  max_retries: 3
  retry_delay: 1

# è¯·æ±‚å‚æ•°
defaults:
  temperature: 0.7
  top_p: 0.9
  max_tokens: 4096
  stream: false
  presence_penalty: 0.0
  frequency_penalty: 0.0

# é«˜çº§è®¾ç½®
advanced:
  context_length: 8192
  response_format: "auto"
  user_id: "obsidian-user"
  session_id: "default"

# å®‰å…¨è®¾ç½®
security:
  validate_ssl: true
  check_content_policy: true
  filter_sensitive_data: true
```

### ğŸ›¡ï¸ å®‰å…¨é…ç½®

#### SSLè¯ä¹¦éªŒè¯
```yaml
# SSLé…ç½®
ssl:
  enabled: true
  verify_peer: true
  verify_host: true
  ca_bundle: "/etc/ssl/certs/ca-certificates.crt"
  cert_file: null
  key_file: null
```

#### è®¿é—®æ§åˆ¶
```yaml
# è®¿é—®æ§åˆ¶é…ç½®
access_control:
  allowed_origins:
    - "http://localhost:3000"
    - "https://yourdomain.com"
  
  allowed_ips:
    - "127.0.0.1"
    - "10.0.0.0/8"
    - "192.168.0.0/16"
  
  rate_limiting:
    enabled: true
    requests_per_minute: 60
    burst_size: 10
    penalty_duration: 300
```

## ğŸ”— ç³»ç»Ÿé›†æˆ

### ğŸ“ Obsidiané›†æˆ

#### Text Generatoræ’ä»¶é…ç½®
```yaml
# Obsidian Text Generatoré…ç½®
text_generator:
  provider: "gpt-oss"
  api_key: "${GPT_OSS_API_KEY}"
  base_url: "https://api.gpt-oss.com/v1"
  model: "gpt-oss-120b-cloud"
  
  # ç”Ÿæˆå‚æ•°
  temperature: 0.7
  max_tokens: 2000
  top_p: 0.9
  frequency_penalty: 0.0
  presence_penalty: 0.0
  
  # åŠŸèƒ½è®¾ç½®
  features:
    auto_complete: true
    smart_suggestions: true
    context_aware: true
    multi_language: true
  
  # ç•Œé¢è®¾ç½®
  ui:
    show_temperature: true
    show_model_selector: true
    confirm_before_generate: true
    streaming_response: false
```

#### æ™ºèƒ½ç¬”è®°æ¨¡æ¿
```yaml
# AIå¢å¼ºç¬”è®°æ¨¡æ¿
ai_note_template:
  name: "GPT-osså¢å¼ºç¬”è®°"
  
  fields:
    - name: "title"
      type: "text"
      required: true
      ai_generated: false
    
    - name: "summary"
      type: "textarea"
      required: true
      ai_generated: true
      ai_prompt: "è¯·ä¸ºæ ‡é¢˜'{{title}}'ç”Ÿæˆä¸€ä¸ªç®€æ´çš„æ€»ç»“"
    
    - name: "key_points"
      type: "textarea"
      required: true
      ai_generated: true
      ai_prompt: "åŸºäºæ ‡é¢˜'{{title}}'å’Œæ€»ç»“'{{summary}}'ï¼Œæå–3-5ä¸ªå…³é”®è¦ç‚¹"
    
    - name: "tags"
      type: "text"
      required: false
      ai_generated: true
      ai_prompt: "ä¸ºæ ‡é¢˜'{{title}}'æ¨è3-5ä¸ªç›¸å…³æ ‡ç­¾ï¼Œç”¨é€—å·åˆ†éš”"
```

### ğŸ’» OpenCodeé›†æˆ

#### OpenCodeé…ç½®æ–‡ä»¶
```yaml
# OpenCode GPT-ossé…ç½®
opencode:
  ai_provider: "gpt-oss"
  
  gpt_oss:
    api_key: "${GPT_OSS_API_KEY}"
    base_url: "https://api.gpt-oss.com/v1"
    model: "gpt-oss-120b-cloud"
    
    # ä»£ç ç”Ÿæˆå‚æ•°
    code_generation:
      temperature: 0.3
      max_tokens: 3000
      top_p: 0.9
      presence_penalty: 0.1
      frequency_penalty: 0.1
    
    # ä»£ç å®¡æŸ¥å‚æ•°
    code_review:
      temperature: 0.1
      max_tokens: 2000
      strict_mode: true
      security_check: true
    
    # æ–‡æ¡£ç”Ÿæˆå‚æ•°
    documentation:
      temperature: 0.4
      max_tokens: 2500
      style: "professional"
      language: "auto"
```

#### æ™ºèƒ½ä»£ç åŠ©æ‰‹é…ç½®
```python
# æ™ºèƒ½ä»£ç åŠ©æ‰‹é…ç½®
class GPTOSSCodeAssistant:
    def __init__(self, api_key, base_url="https://api.gpt-oss.com/v1"):
        self.api_key = api_key
        self.base_url = base_url
        self.model = "gpt-oss-120b-cloud"
    
    def generate_code(self, prompt, language="python", temperature=0.3):
        """ç”Ÿæˆä»£ç """
        system_prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„{language}ç¨‹åºå‘˜ã€‚
è¯·æ ¹æ®éœ€æ±‚ç”Ÿæˆé«˜è´¨é‡ã€å¯è¯»æ€§å¼ºçš„ä»£ç ã€‚
ä»£ç åº”è¯¥éµå¾ªæœ€ä½³å®è·µå’Œç¼–ç è§„èŒƒã€‚"""
        
        response = self._call_api(
            system_prompt=system_prompt,
            user_prompt=prompt,
            temperature=temperature
        )
        
        return response
    
    def review_code(self, code, language="python"):
        """ä»£ç å®¡æŸ¥"""
        system_prompt = f"""ä½ æ˜¯ä¸€ä¸ªèµ„æ·±çš„{language}ä»£ç å®¡æŸ¥ä¸“å®¶ã€‚
è¯·ä»ä»¥ä¸‹è§’åº¦å®¡æŸ¥ä»£ç ï¼š
1. ä»£ç è´¨é‡å’Œå¯è¯»æ€§
2. æ€§èƒ½ä¼˜åŒ–å»ºè®®
3. å®‰å…¨æ€§é—®é¢˜
4. æœ€ä½³å®è·µå»ºè®®"""
        
        response = self._call_api(
            system_prompt=system_prompt,
            user_prompt=f"è¯·å®¡æŸ¥ä»¥ä¸‹{language}ä»£ç ï¼š\n\n{code}",
            temperature=0.1
        )
        
        return response
    
    def explain_code(self, code, language="python"):
        """ä»£ç è§£é‡Š"""
        system_prompt = f"""ä½ æ˜¯ä¸€ä¸ª{language}æ•™å­¦ä¸“å®¶ã€‚
è¯·ç”¨ç®€å•æ˜“æ‡‚çš„è¯­è¨€è§£é‡Šä»£ç çš„åŠŸèƒ½å’ŒåŸç†ã€‚
é€‚åˆä¸åŒæ°´å¹³çš„å¼€å‘è€…ç†è§£ã€‚"""
        
        response = self._call_api(
            system_prompt=system_prompt,
            user_prompt=f"è¯·è§£é‡Šä»¥ä¸‹{language}ä»£ç çš„åŠŸèƒ½ï¼š\n\n{code}",
            temperature=0.5
        )
        
        return response
```

### ğŸ¦™ Ollamaé›†æˆ

#### æ¨¡å‹è·¯ç”±é…ç½®
```yaml
# Ollama + GPT-ossæ··åˆé…ç½®
model_routing:
  # è·¯ç”±ç­–ç•¥
  strategy: "smart_routing"
  
  # æ¨¡å‹é…ç½®
  models:
    local:
      provider: "ollama"
      models:
        - "llama2:7b"
        - "codellama:7b"
      cost_per_token: 0.0
      max_tokens: 4096
      speed: "fast"
    
    cloud:
      provider: "gpt-oss"
      models:
        - "gpt-oss-120b-cloud"
      cost_per_token: 0.00002
      max_tokens: 8192
      speed: "medium"
  
  # è·¯ç”±è§„åˆ™
  routing_rules:
    - condition: "task_type == 'simple_chat'"
      target: "local"
      model: "llama2:7b"
    
    - condition: "task_type == 'code_simple'"
      target: "local"
      model: "codellama:7b"
    
    - condition: "task_type == 'complex_reasoning'"
      target: "cloud"
      model: "gpt-oss-120b-cloud"
    
    - condition: "token_count > 4000"
      target: "cloud"
      model: "gpt-oss-120b-cloud"
    
    - condition: "user_preference == 'quality_first'"
      target: "cloud"
      model: "gpt-oss-120b-cloud"
```

#### æ™ºèƒ½è·¯ç”±å®ç°
```python
# æ™ºèƒ½è·¯ç”±å®ç°
class SmartModelRouter:
    def __init__(self, routing_config):
        self.config = routing_config
        self.ollama_client = OllamaClient()
        self.gpt_oss_client = GPTOSSClient()
    
    def route_request(self, prompt, context=None):
        """æ™ºèƒ½è·¯ç”±è¯·æ±‚"""
        # åˆ†æè¯·æ±‚ç‰¹å¾
        task_type = self._analyze_task_type(prompt)
        token_count = self._estimate_tokens(prompt)
        user_preference = self._get_user_preference()
        
        # æ ¹æ®è·¯ç”±è§„åˆ™é€‰æ‹©æ¨¡å‹
        target_model = self._select_model(
            task_type, token_count, user_preference
        )
        
        # å‘é€è¯·æ±‚
        if target_model["provider"] == "ollama":
            return self.ollama_client.generate(
                model=target_model["name"],
                prompt=prompt,
                context=context
            )
        else:
            return self.gpt_oss_client.generate(
                model=target_model["name"],
                prompt=prompt,
                context=context
            )
    
    def _analyze_task_type(self, prompt):
        """åˆ†æä»»åŠ¡ç±»å‹"""
        if "ä»£ç " in prompt or "ç¼–ç¨‹" in prompt:
            if "ç®€å•" in prompt or "åŸºç¡€" in prompt:
                return "code_simple"
            else:
                return "code_complex"
        elif "æ¨ç†" in prompt or "åˆ†æ" in prompt:
            return "complex_reasoning"
        else:
            return "simple_chat"
    
    def _estimate_tokens(self, text):
        """ä¼°ç®—tokenæ•°é‡"""
        return len(text) // 4  # ç®€å•ä¼°ç®—
    
    def _get_user_preference(self):
        """è·å–ç”¨æˆ·åå¥½"""
        return self.config.get("user_preference", "balanced")
    
    def _select_model(self, task_type, token_count, user_preference):
        """é€‰æ‹©æœ€ä¼˜æ¨¡å‹"""
        for rule in self.config["routing_rules"]:
            condition = rule["condition"]
            if self._evaluate_condition(condition, task_type, token_count, user_preference):
                return {
                    "provider": rule["target"],
                    "name": rule["model"]
                }
        
        # é»˜è®¤é€‰æ‹©
        return {
            "provider": "local",
            "name": "llama2:7b"
        }
    
    def _evaluate_condition(self, condition, task_type, token_count, user_preference):
        """è¯„ä¼°è·¯ç”±æ¡ä»¶"""
        # ç®€åŒ–çš„æ¡ä»¶è¯„ä¼°é€»è¾‘
        if "task_type" in condition:
            if condition["task_type"] == task_type:
                return True
        if "token_count" in condition:
            if token_count > int(condition["token_count"].split(">")[1]):
                return True
        if "user_preference" in condition:
            if condition["user_preference"] == user_preference:
                return True
        return False
```

## ğŸ“Š æˆæœ¬ç®¡ç†

### ğŸ’° å®šä»·ç­–ç•¥

#### è®¡è´¹æ¨¡å¼
| æœåŠ¡ç±»å‹ | ä»·æ ¼ | è®¡è´¹æ–¹å¼ | å…è´¹é¢åº¦ |
|----------|------|----------|----------|
| **APIè°ƒç”¨** | Â¥0.02/1K tokens | æŒ‰é‡è®¡è´¹ | 100K tokens/æœˆ |
| **ä»£ç ç”Ÿæˆ** | Â¥0.025/1K tokens | æŒ‰é‡è®¡è´¹ | 50K tokens/æœˆ |
| **æ–‡æ¡£å¤„ç†** | Â¥0.015/1K tokens | æŒ‰é‡è®¡è´¹ | 200K tokens/æœˆ |
| **æ‰¹é‡å¤„ç†** | Â¥0.018/1K tokens | æŒ‰é‡è®¡è´¹ | 150K tokens/æœˆ |

#### å¥—é¤è®¡åˆ’
```yaml
# å¥—é¤é…ç½®
pricing_plans:
  free:
    name: "å…è´¹ç‰ˆ"
    price: 0
    monthly_quota: 100000
    features:
      - "åŸºç¡€APIè®¿é—®"
      - "æ ‡å‡†å“åº”é€Ÿåº¦"
      - "ç¤¾åŒºæ”¯æŒ"
    
  professional:
    name: "ä¸“ä¸šç‰ˆ"
    price: 99
    monthly_quota: 1000000
    features:
      - "ä¼˜å…ˆAPIè®¿é—®"
      - "å¿«é€Ÿå“åº”é€Ÿåº¦"
      - "é«˜çº§åŠŸèƒ½"
      - "é‚®ä»¶æ”¯æŒ"
    
  enterprise:
    name: "ä¼ä¸šç‰ˆ"
    price: 499
    monthly_quota: 10000000
    features:
      - "ä¸“äº«APIè®¿é—®"
      - "æé€Ÿå“åº”é€Ÿåº¦"
      - "å…¨éƒ¨é«˜çº§åŠŸèƒ½"
      - "ä¸“å±å®¢æˆ·ç»ç†"
      - "å®šåˆ¶åŒ–æœåŠ¡"
```

### ğŸ“ˆ ä½¿ç”¨ç›‘æ§

#### æˆæœ¬ç›‘æ§è„šæœ¬
```python
# cost_monitor.py
import requests
import json
from datetime import datetime, timedelta

class CostMonitor:
    def __init__(self, api_key, base_url="https://api.gpt-oss.com/v1"):
        self.api_key = api_key
        self.base_url = base_url
    
    def get_usage_stats(self, start_date, end_date):
        """è·å–ä½¿ç”¨ç»Ÿè®¡"""
        url = f"{self.base_url}/usage/stats"
        headers = {"Authorization": f"Bearer {self.api_key}"}
        params = {
            "start_date": start_date,
            "end_date": end_date
        }
        
        response = requests.get(url, headers=headers, params=params)
        return response.json()
    
    def calculate_monthly_cost(self, year, month):
        """è®¡ç®—æœˆåº¦æˆæœ¬"""
        start_date = f"{year}-{month:02d}-01"
        end_date = f"{year}-{month:02d}-31"
        
        stats = self.get_usage_stats(start_date, end_date)
        
        total_tokens = stats["total_tokens"]
        cost = self._calculate_cost(total_tokens)
        
        return {
            "month": f"{year}-{month:02d}",
            "total_tokens": total_tokens,
            "cost": cost,
            "free_quota_used": min(total_tokens, stats["free_quota"]),
            "paid_tokens": max(0, total_tokens - stats["free_quota"])
        }
    
    def _calculate_cost(self, tokens):
        """è®¡ç®—æˆæœ¬"""
        if tokens <= 100000:  # å…è´¹é¢åº¦
            return 0
        else:
            return (tokens - 100000) * 0.00002
    
    def generate_cost_report(self):
        """ç”Ÿæˆæˆæœ¬æŠ¥å‘Š"""
        today = datetime.now()
        current_month = self.calculate_monthly_cost(today.year, today.month)
        
        # è·å–æœ€è¿‘6ä¸ªæœˆçš„æˆæœ¬
        report = {
            "current_month": current_month,
            "recent_months": []
        }
        
        for i in range(1, 7):
            date = today - timedelta(days=i*30)
            month_cost = self.calculate_monthly_cost(date.year, date.month)
            report["recent_months"].append(month_cost)
        
        return report
```

#### é¢„ç®—å‘Šè­¦é…ç½®
```yaml
# é¢„ç®—å‘Šè­¦é…ç½®
budget_alerts:
  enabled: true
  
  # æœˆåº¦é¢„ç®—è®¾ç½®
  monthly_budget:
    amount: 500  # æœˆåº¦é¢„ç®—é™é¢
    warning_threshold: 0.8  # 80%æ—¶å‘Šè­¦
    critical_threshold: 0.95  # 95%æ—¶å‘Šè­¦
  
  # å‘Šè­¦æ–¹å¼
  alert_methods:
    email:
      enabled: true
      recipients:
        - "admin@company.com"
        - "finance@company.com"
      template: "budget_alert_email"
    
    webhook:
      enabled: true
      url: "https://yourcompany.com/webhooks/budget-alert"
      headers:
        Authorization: "Bearer your-webhook-token"
    
    slack:
      enabled: false
      webhook_url: "https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK"
  
  # å‘Šè­¦æ¶ˆæ¯æ¨¡æ¿
  templates:
    email:
      subject: "GPT-ossæœˆåº¦é¢„ç®—å‘Šè­¦ - {alert_level}"
      body: |
        äº²çˆ±çš„ç®¡ç†å‘˜ï¼Œ
        
        æ‚¨çš„GPT-ossæœˆåº¦ä½¿ç”¨æƒ…å†µå¦‚ä¸‹ï¼š
        
        - æœˆåº¦é¢„ç®—ï¼š{monthly_budget}å…ƒ
        - å½“å‰æ¶ˆè´¹ï¼š{current_cost}å…ƒ
        - ä½¿ç”¨ç‡ï¼š{usage_percentage}%
        - å‰©ä½™é¢åº¦ï¼š{remaining_quota} tokens
        
        {alert_message}
        
        è¯·åŠæ—¶å…³æ³¨ä½¿ç”¨æƒ…å†µï¼Œé¿å…è¶…å‡ºé¢„ç®—ã€‚
        
        GPT-ossç®¡ç†å›¢é˜Ÿ
```

## ğŸš€ é«˜çº§åŠŸèƒ½

### ğŸ¯ ä¸“ç”¨æ¨¡å‹é…ç½®

#### ä»£ç ä¸“ç”¨æ¨¡å‹
```yaml
# GPT-oss Codeé…ç½®
gpt_oss_code:
  model: "gpt-oss-120b-code"
  specialization: "code_generation"
  
  # ä»£ç ç”Ÿæˆä¼˜åŒ–
  code_generation:
    languages:
      - "python"
      - "javascript"
      - "java"
      - "go"
      - "rust"
      - "cpp"
    
    frameworks:
      python:
        - "django"
        - "flask"
        - "fastapi"
        - "pandas"
        - "numpy"
      javascript:
        - "react"
        - "vue"
        - "angular"
        - "nodejs"
        - "express"
    
    quality_settings:
      follow_best_practices: true
      include_comments: true
      add_error_handling: true
      ensure_type_safety: true
```

#### æ–‡æ¡£ä¸“ç”¨æ¨¡å‹
```yaml
# GPT-oss Docsé…ç½®
gpt_oss_docs:
  model: "gpt-oss-120b-docs"
  specialization: "documentation"
  
  # æ–‡æ¡£ç”Ÿæˆä¼˜åŒ–
  documentation:
    formats:
      - "markdown"
      - "html"
      - "pdf"
      - "word"
    
    sections:
      required:
        - "title"
        - "introduction"
        - "usage"
        - "examples"
        - "api_reference"
    
    style_guides:
      technical:
        - "formal_tone"
        - "precise_terminology"
        - "clear_structure"
      user_friendly:
        - "conversational_tone"
        - "simple_explanations"
        - "practical_examples"
```

### ğŸ”„ æ‰¹é‡å¤„ç†

#### æ‰¹é‡APIè°ƒç”¨
```python
# batch_processor.py
import asyncio
import aiohttp
import json
from typing import List, Dict

class BatchProcessor:
    def __init__(self, api_key, base_url="https://api.gpt-oss.com/v1"):
        self.api_key = api_key
        self.base_url = base_url
        self.max_concurrent = 10  # å¹¶å‘é™åˆ¶
        self.semaphore = asyncio.Semaphore(self.max_concurrent)
    
    async def process_batch(self, requests: List[Dict], model="gpt-oss-120b-cloud"):
        """æ‰¹é‡å¤„ç†è¯·æ±‚"""
        tasks = []
        for i, request in enumerate(requests):
            task = self._process_single_request(request, model, i)
            tasks.append(task)
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        return results
    
    async def _process_single_request(self, request: Dict, model: str, index: int):
        """å¤„ç†å•ä¸ªè¯·æ±‚"""
        async with self.semaphore:
            try:
                async with aiohttp.ClientSession() as session:
                    headers = {
                        "Authorization": f"Bearer {self.api_key}",
                        "Content-Type": "application/json"
                    }
                    
                    payload = {
                        "model": model,
                        "messages": request["messages"],
                        "temperature": request.get("temperature", 0.7),
                        "max_tokens": request.get("max_tokens", 4096)
                    }
                    
                    async with session.post(
                        f"{self.base_url}/chat/completions",
                        headers=headers,
                        json=payload,
                        timeout=30
                    ) as response:
                        result = await response.json()
                        return {
                            "index": index,
                            "status": "success",
                            "result": result
                        }
            
            except Exception as e:
                return {
                    "index": index,
                    "status": "error",
                    "error": str(e)
                }
    
    async def process_text_batch(self, texts: List[str], prompt_template: str):
        """æ‰¹é‡å¤„ç†æ–‡æœ¬"""
        requests = []
        for i, text in enumerate(texts):
            request = {
                "messages": [
                    {
                        "role": "system",
                        "content": "ä½ æ˜¯ä¸€ä¸ªæ–‡æœ¬å¤„ç†åŠ©æ‰‹ã€‚"
                    },
                    {
                        "role": "user",
                        "content": prompt_template.format(text=text)
                    }
                ],
                "temperature": 0.3,
                "max_tokens": 500
            }
            requests.append(request)
        
        results = await self.process_batch(requests)
        return results

# ä½¿ç”¨ç¤ºä¾‹
async def main():
    processor = BatchProcessor(api_key="your-api-key")
    
    texts = [
        "Pythonæ˜¯ä¸€ç§ç¼–ç¨‹è¯­è¨€",
        "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªåˆ†æ”¯",
        "æ·±åº¦å­¦ä¹ ä½¿ç”¨ç¥ç»ç½‘ç»œ"
    ]
    
    prompt_template = "è¯·æ€»ç»“ä»¥ä¸‹æ–‡æœ¬ï¼š{text}"
    
    results = await processor.process_text_batch(texts, prompt_template)
    
    for result in results:
        if result["status"] == "success":
            print(f"æ–‡æœ¬ {result['index']}: {result['result']['choices'][0]['message']['content']}")
        else:
            print(f"å¤„ç†å¤±è´¥ {result['index']}: {result['error']}")

if __name__ == "__main__":
    asyncio.run(main())
```

### ğŸ”— APIä»£ç†

#### æœ¬åœ°APIä»£ç†
```python
# api_proxy.py
from flask import Flask, request, jsonify
import requests
import os

app = Flask(__name__)

class GPTOSSProxy:
    def __init__(self):
        self.api_key = os.getenv("GPT_OSS_API_KEY")
        self.base_url = "https://api.gpt-oss.com/v1"
        self.cache = {}
        self.cache_enabled = True
    
    def _get_cache_key(self, payload):
        """ç”Ÿæˆç¼“å­˜é”®"""
        import hashlib
        cache_str = json.dumps(payload, sort_keys=True)
        return hashlib.md5(cache_str.encode()).hexdigest()
    
    def _get_from_cache(self, cache_key):
        """ä»ç¼“å­˜è·å–ç»“æœ"""
        if self.cache_enabled and cache_key in self.cache:
            return self.cache[cache_key]
        return None
    
    def _set_cache(self, cache_key, result):
        """è®¾ç½®ç¼“å­˜"""
        if self.cache_enabled:
            self.cache[cache_key] = result
    
    def _call_gpt_oss(self, payload):
        """è°ƒç”¨GPT-oss API"""
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        
        response = requests.post(
            f"{self.base_url}/chat/completions",
            headers=headers,
            json=payload,
            timeout=30
        )
        
        return response.json()

gpt_oss = GPTOSSProxy()

@app.route("/v1/chat/completions", methods=["POST"])
def chat_completions():
    """ä»£ç†èŠå¤©å®Œæˆæ¥å£"""
    try:
        payload = request.json
        
        # æ£€æŸ¥ç¼“å­˜
        cache_key = gpt_oss._get_cache_key(payload)
        cached_result = gpt_oss._get_from_cache(cache_key)
        if cached_result:
            return jsonify(cached_result)
        
        # è°ƒç”¨API
        result = gpt_oss._call_gpt_oss(payload)
        
        # è®¾ç½®ç¼“å­˜
        gpt_oss._set_cache(cache_key, result)
        
        return jsonify(result)
    
    except Exception as e:
        return jsonify({"error": str(e)}), 500

@app.route("/health", methods=["GET"])
def health_check():
    """å¥åº·æ£€æŸ¥"""
    return jsonify({"status": "healthy"})

if __name__ == "__main__":
    app.run(host="0.0.0.0", port=8080, debug=False)
```

## âœ… é…ç½®æ£€æŸ¥æ¸…å•

### ğŸ”‘ åŸºç¡€é…ç½®
- [ ] è·å–å¹¶éªŒè¯APIå¯†é’¥
- [ ] è®¾ç½®ç¯å¢ƒå˜é‡
- [ ] åˆ›å»ºé…ç½®æ–‡ä»¶
- [ ] æµ‹è¯•åŸºç¡€APIè°ƒç”¨

### ğŸ”— é›†æˆé…ç½®
- [ ] Obsidian Text Generatoré…ç½®
- [ ] OpenCodeé›†æˆé…ç½®
- [ ] Ollamaè·¯ç”±é…ç½®
- [ ] æµ‹è¯•é›†æˆåŠŸèƒ½

### ğŸ’° æˆæœ¬ç®¡ç†
- [ ] äº†è§£å®šä»·ç­–ç•¥
- [ ] è®¾ç½®é¢„ç®—å‘Šè­¦
- [ ] éƒ¨ç½²æˆæœ¬ç›‘æ§
- [ ] é…ç½®ä½¿ç”¨é™é¢

### ğŸš€ é«˜çº§åŠŸèƒ½
- [ ] é…ç½®ä¸“ç”¨æ¨¡å‹
- [ ] éƒ¨ç½²æ‰¹é‡å¤„ç†
- [ ] è®¾ç½®APIä»£ç†
- [ ] æµ‹è¯•é«˜çº§åŠŸèƒ½

## ğŸ¯ ä¸‹ä¸€æ­¥

é…ç½®å®Œæˆåï¼Œæ‚¨å¯ä»¥ï¼š

1. **ğŸ“ åœ¨Obsidianä¸­ä½¿ç”¨GPT-oss**ï¼šä½“éªŒæ™ºèƒ½ç¬”è®°ç”Ÿæˆ
2. **ğŸ’» åœ¨OpenCodeä¸­ä½¿ç”¨**ï¼šäº«å—é«˜è´¨é‡ä»£ç ç”Ÿæˆ
3. **ğŸ¦™ é…ç½®æ™ºèƒ½è·¯ç”±**ï¼šä¼˜åŒ–æœ¬åœ°+äº‘ç«¯æ¨¡å‹ä½¿ç”¨
4. **ğŸ“Š ç›‘æ§ä½¿ç”¨æƒ…å†µ**ï¼šè·Ÿè¸ªæˆæœ¬å’Œæ€§èƒ½

> ğŸ’¡ **æç¤º**ï¼šå»ºè®®å…ˆåœ¨æµ‹è¯•ç¯å¢ƒä¸­éªŒè¯æ‰€æœ‰é…ç½®ï¼Œå†éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒã€‚

---

**ğŸ“ åˆ›å»ºæ—¶é—´**ï¼š2026-01-21 | **ğŸ”„ æœ€åæ›´æ–°**ï¼š2026-01-21 | **ğŸ‘¥ ç»´æŠ¤è€…**ï¼šGPT-oss Cloud Team