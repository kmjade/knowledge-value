# å·¥ä½œæµ

# å·¥ä½œæµ

## ğŸ“– æ¦‚è¿°

# å·¥ä½œæµ

# å·¥ä½œæµ

### ğŸ”„ æ•´é«”æµç¨‹åœ–
```mermaid
graph TB
# ç®¡ç†
        A[ObsidiançŸ¥è­˜åº«]
        B[æ™ºèƒ½ç­†è¨˜å‰µå»º]
        C[çŸ¥è­˜åœ–è­œ]
    end
    
    subgraph "é–‹ç™¼å±¤"
        D[OpenCode]
        E[æ™ºèƒ½ä»£ç¢¼ç”Ÿæˆ]
        F[ä»£ç¢¼å¯©æŸ¥å„ªåŒ–]
    end
    
    subgraph "AIæœå‹™å±¤"
        G[Ollamaæœ¬åœ°æ¨¡å‹]
        H[GPT-ossé›²ç«¯æ¨¡å‹]
        I[æ™ºèƒ½è·¯ç”±]
    end
    
    subgraph "è‡ªå‹•åŒ–å±¤"
        J[CI/CDæµç¨‹]
        K[è‡ªå‹•åŒ–æ¸¬è©¦]
        L[æ–‡æª”ç”Ÿæˆ]
    end
    
    A --> G
    A --> H
    B --> I
    C --> I
    
    D --> G
    D --> H
    E --> I
    F --> I
    
    D --> J
    A --> L
    J --> K
```

### ğŸ¯ ä½¿ç”¨å ´æ™¯
| å ´æ™¯ | ä¸»è¦å·¥å…· | AIæ¨¡å‹ | é æœŸæ”¶ç›Š |
|------|----------|---------|----------|
# æ•ˆç‡
| **ä»£ç¢¼é–‹ç™¼** | OpenCode | CodeLLaMA/GPT-oss | é–‹ç™¼æ™‚é–“æ¸›å°‘40%+ |
| **æŠ€è¡“æ–‡æª”** | Obsidian+OpenCode | GPT-oss | æ–‡æª”è³ªé‡æå‡60%+ |
# æ•ˆç‡

# å·¥ä½œæµ

### ğŸ§  æ™ºèƒ½ç­†è¨˜å‰µå»º

# å·¥ä½œæµ
```yaml
knowledge_management:
  # ç­†è¨˜å‰µå»ºæµç¨‹
  note_creation:
    trigger: "manual"
    steps:
      - name: "ä¸»é¡Œç¢ºå®š"
        ai_assistant: true
        model: "mistral:7b"
        prompt_template: "æ ¹æ“š{topic}ç”Ÿæˆç­†è¨˜å¤§ç¶±"
      
      - name: "å…§å®¹ç”Ÿæˆ"
        ai_assistant: true
        model: "gpt-oss:120b-cloud"
        context_sources:
          - "obsidian_vault"
          - "external_docs"
        prompt_template: "è©³ç´°é—¡è¿°{outline}çš„å…§å®¹"
      
      - name: "æ¨™ç±¤æ¨è–¦"
        ai_assistant: true
        model: "llama2:7b"
        prompt_template: "ç‚ºç­†è¨˜{title}æ¨è–¦3-5å€‹ç›¸é—œæ¨™ç±¤"
      
      - name: "éˆæ¥å»ºç«‹"
        ai_assistant: true
        model: "mistral:7b"
        similarity_threshold: 0.8
  
  # çŸ¥è­˜çµ„ç¹”æµç¨‹
  knowledge_organization:
    trigger: "daily"
    steps:
      - name: "è­˜åˆ¥å­¤å…’ç­†è¨˜"
        ai_assistant: true
        model: "mistral:7b"
      
      - name: "å»ºè­°åˆä½µ"
        ai_assistant: true
        model: "gpt-oss:120b-cloud"
        merge_threshold: 0.9
      
# æ›´æ–°
        ai_assistant: true
        model: "local"
        update_frequency: "daily"
```

#### å¯¦ç¾è…³æœ¬
```python
# knowledge_workflow.py
import json
import requests
from datetime import datetime
from pathlib import Path

class KnowledgeWorkflow:
    def __init__(self, obsidian_path, ai_config):
        self.obsidian_path = Path(obsidian_path)
        self.ai_config = ai_config
    
    def create_smart_note(self, topic, complexity="medium"):
        """å‰µå»ºæ™ºèƒ½ç­†è¨˜"""
        # 1. ç”Ÿæˆå¤§ç¶±
        outline = self._generate_outline(topic, complexity)
        
        # 2. ç”Ÿæˆå…§å®¹
        content = self._generate_content(topic, outline, complexity)
        
        # 3. æ¨è–¦æ¨™ç±¤
        tags = self._recommend_tags(topic, content)
        
        # 4. å»ºç«‹éˆæ¥
        links = self._establish_links(topic, content)
        
        # 5. å„²å­˜ç­†è¨˜
        note = self._format_note(topic, outline, content, tags, links)
        self._save_note(note)
        
        return note
    
    def _generate_outline(self, topic, complexity):
        """ç”Ÿæˆç­†è¨˜å¤§ç¶±"""
        model = self._select_model("outline_generation", complexity)
        
        prompt = f"""
# ç®¡ç†

è¦æ±‚ï¼š
1. çµæ§‹æ¸…æ™°ï¼Œå±¤æ¬¡åˆ†æ˜
2. é‚è¼¯å®Œæ•´ï¼Œæ¶µè“‹å…¨é¢
3. é‡é»çªå‡ºï¼Œä¾¿æ–¼ç†è§£
4. åŒ…å«å¯¦éš›æ‡‰ç”¨

è«‹ä»¥Markdownæ ¼å¼è¼¸å‡ºå¤§ç¶±ã€‚
"""
        
        response = self._call_ai(model, prompt)
        return response
    
    def _generate_content(self, topic, outline, complexity):
        """ç”Ÿæˆç­†è¨˜å…§å®¹"""
        model = self._select_model("content_generation", complexity)
        
        prompt = f"""
è«‹åŸºæ–¼ä»¥ä¸‹å¤§ç¶±ï¼Œç‚ºä¸»é¡Œ"{topic}"ç”Ÿæˆè©³ç´°å…§å®¹ï¼š

å¤§ç¶±ï¼š
{outline}

è¦æ±‚ï¼š
1. å…§å®¹æº–ç¢ºï¼Œæœ‰æ“šå¯æŸ¥
2. èªè¨€æµæš¢ï¼Œæ˜“æ–¼ç†è§£
3. åŒ…å«å¯¦éš›æ¡ˆä¾‹å’Œæ‡‰ç”¨
4. é©ç•¶ä½¿ç”¨ä»£ç¢¼ç¤ºä¾‹ï¼ˆå¦‚é©ç”¨ï¼‰
"""
        
        response = self._call_ai(model, prompt)
        return response
    
    def _recommend_tags(self, topic, content):
        """æ¨è–¦æ¨™ç±¤"""
        model = "llama2:7b"
        
        prompt = f"""
åŸºæ–¼ä»¥ä¸‹ç­†è¨˜å…§å®¹ï¼Œæ¨è–¦3-5å€‹ç›¸é—œæ¨™ç±¤ï¼š

ä¸»é¡Œï¼š{topic}
å…§å®¹æ‘˜è¦ï¼š{content[:500]}...

è¦æ±‚ï¼š
1. æ¨™ç±¤æ‡‰è©²ç°¡æ½”æ˜ç¢º
2. æ¨™ç±¤æ‡‰è©²èˆ‡å…§å®¹é«˜åº¦ç›¸é—œ
3. æ¨™ç±¤æ‡‰è©²ä¾¿æ–¼æœå°‹å’Œåˆ†é¡
4. ç”¨è‹±æ–‡é€—è™Ÿåˆ†éš”è¼¸å‡º
"""
        
        response = self._call_ai(model, prompt)
        return [tag.strip() for tag in response.split(',')]
    
    def _establish_links(self, topic, content):
        """å»ºç«‹ç­†è¨˜éˆæ¥"""
        # æœå°‹ç›¸ä¼¼ç­†è¨˜
        similar_notes = self._search_similar_notes(topic, content)
        
        # AIè©•ä¼°éˆæ¥è³ªé‡
        model = "mistral:7b"
        links = []
        
        for note in similar_notes:
            relevance = self._evaluate_link_relevance(topic, content, note)
            if relevance > 0.7:
                links.append({
                    "title": note["title"],
                    "path": note["path"],
                    "relevance": relevance
                })
        
# æ’åº
        links.sort(key=lambda x: x["relevance"], reverse=True)
        return links[:10]
    
    def _format_note(self, topic, outline, content, tags, links):
        """æ ¼å¼åŒ–ç­†è¨˜"""
        now = datetime.now().isoformat()
        
        note = f"""---
title: {topic}
created: {now}
tags: {', '.join(tags)}
ai_generated: true
---

# {topic}

## ğŸ“‹ å¤§ç¶±
{outline}

## ğŸ“ è©³ç´°å…§å®¹
{content}

## ğŸ”— ç›¸é—œç­†è¨˜
"""
        
        for link in links:
            note += f"- [[{link['path']}|{link['title']}]]\n"
        
        note += f"""
## ğŸ·ï¸ æ¨™ç±¤
{', '.join(tags)}
"""
        
        return note
    
    def _save_note(self, note):
        """å„²å­˜ç­†è¨˜"""
        # å¾ç­†è¨˜å…§å®¹ä¸­æå–æ¨™é¡Œ
        title = note.split('\n')[3].split(': ')[1]
        
        # å®‰å…¨çš„æª”æ¡ˆå
        safe_title = ''.join(c for c in title if c.isalnum() or c in (' ', '-', '_'))
        filename = f"{safe_title}.md"
        
        # å„²å­˜åˆ°çŸ¥è­˜åº«
        filepath = self.obsidian_path / filename
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(note)
        
        print(f"ç­†è¨˜å·²å„²å­˜ï¼š{filepath}")
        return filepath
    
    def _select_model(self, task_type, complexity):
        """é¸æ“‡AIæ¨¡å‹"""
        models = {
            "outline_generation": {
                "low": "llama2:7b",
                "medium": "mistral:7b",
                "high": "gpt-oss:120b-cloud"
            },
            "content_generation": {
                "low": "llama2:7b",
                "medium": "mistral:7b",
                "high": "gpt-oss:120b-cloud"
            }
        }
        
        return models[task_type][complexity]
    
    def _call_ai(self, model, prompt):
        """èª¿ç”¨AIæ¨¡å‹"""
        if model.startswith("llama") or model.startswith("mistral") or model.startswith("codellama"):
            return self._call_ollama(model, prompt)
        else:
            return self._call_gpt_oss(model, prompt)
    
    def _call_ollama(self, model, prompt):
        """èª¿ç”¨Ollamaæœ¬åœ°æ¨¡å‹"""
        response = requests.post(
            "http://localhost:11434/api/generate",
            json={
                "model": model,
                "prompt": prompt,
                "stream": False
            }
        )
        return response.json()["response"]
    
    def _call_gpt_oss(self, model, prompt):
        """èª¿ç”¨GPT-ossé›²ç«¯æ¨¡å‹"""
        headers = {
            "Authorization": f"Bearer {self.ai_config['gpt_oss_api_key']}",
            "Content-Type": "application/json"
        }
        
        response = requests.post(
            f"{self.ai_config['gpt_oss_base_url']}/chat/completions",
            headers=headers,
            json={
                "model": model,
                "messages": [
# ç®¡ç†
                    {"role": "user", "content": prompt}
                ],
                "max_tokens": 2000,
                "temperature": 0.7
            }
        )
        
        return response.json()["choices"][0]["message"]["content"]
    
    def _search_similar_notes(self, topic, content):
        """æœå°‹ç›¸ä¼¼ç­†è¨˜"""
        # ç°¡å–®å¯¦ç¾ï¼šæœå°‹æœ¬åœ°Obsidianç­†è¨˜
        similar_notes = []
        
        for note_file in self.obsidian_path.glob("*.md"):
            try:
                with open(note_file, 'r', encoding='utf-8') as f:
                    note_content = f.read()
                
                # ç°¡å–®çš„ç›¸ä¼¼åº¦è¨ˆç®—
                similarity = self._calculate_similarity(topic, content, note_content)
                
                if similarity > 0.5:
                    similar_notes.append({
                        "title": note_file.stem,
                        "path": str(note_file.relative_to(self.obsidian_path)),
                        "similarity": similarity
                    })
            except Exception as e:
                print(f"è®€å–ç­†è¨˜æ™‚å‡ºéŒ¯ï¼š{e}")
        
        return similar_notes
    
    def _calculate_similarity(self, topic, content, note_content):
        """è¨ˆç®—ç›¸ä¼¼åº¦ï¼ˆç°¡å–®å¯¦ç¾ï¼‰"""
        # å¯¦éš›æ‡‰ç”¨ä¸­æ‡‰è©²ä½¿ç”¨æ›´è¤‡é›œçš„ç®—æ³•
        topic_words = set(topic.lower().split())
        content_words = set(content.lower().split()[:100])
        note_words = set(note_content.lower().split())
        
        intersection = len(topic_words & note_words) + len(content_words & note_words)
        union = len(topic_words | content_words | note_words)
        
        return intersection / union if union > 0 else 0
    
    def _evaluate_link_relevance(self, topic, content, note):
        """è©•ä¼°éˆæ¥ç›¸é—œåº¦"""
        model = "mistral:7b"
        
        prompt = f"""
è«‹è©•ä¼°ä»¥ä¸‹å…©å€‹ç­†è¨˜çš„ç›¸é—œç¨‹åº¦ï¼ˆ0-1ä¹‹é–“çš„å°æ•¸ï¼‰ï¼š

ç­†è¨˜1ï¼š
ä¸»é¡Œï¼š{topic}
å…§å®¹æ‘˜è¦ï¼š{content[:200]}...

ç­†è¨˜2ï¼š
ä¸»é¡Œï¼š{note['title']}
è·¯å¾‘ï¼š{note['path']}

è«‹åªè¼¸å‡ºç›¸é—œåº¦åˆ†æ•¸ï¼Œä¸è¦å…¶ä»–å…§å®¹ã€‚
"""
        
        try:
            response = self._call_ai(model, prompt)
            return float(response.strip())
        except:
            return note['similarity']

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    workflow = KnowledgeWorkflow(
        obsidian_path="D:/Knowledge/AI-value",
        ai_config={
            "gpt_oss_api_key": "your-api-key",
            "gpt_oss_base_url": "https://api.gpt-oss.com/v1"
        }
    )
    
    # å‰µå»ºæ™ºèƒ½ç­†è¨˜
    note = workflow.create_smart_note(
        topic="æ©Ÿå™¨å­¸ç¿’åŸºç¤",
        complexity="medium"
    )
    
    print("ç­†è¨˜å‰µå»ºå®Œæˆï¼")
```

### ğŸ” æ™ºèƒ½æœå°‹èˆ‡æª¢ç´¢

#### èªç¾©æœå°‹æµç¨‹
```yaml
semantic_search:
  # ç´¢å¼•æ§‹å»º
  index_building:
    trigger: "scheduled"
    frequency: "daily"
    steps:
      - name: "æ–‡æœ¬åµŒå…¥"
        ai_model: "text-embedding-3-small"
        chunk_size: 500
        overlap: 50
      
      - name: "å‘é‡å­˜å„²"
        database: "chroma"
        collection: "obsidian_notes"
      
      - name: "ç´¢å¼•å„ªåŒ–"
        optimization: "hnsw"
        parameters:
          m: 16
          ef_construction: 200
  
  # æœå°‹åŸ·è¡Œ
  search_execution:
    trigger: "user_query"
    steps:
      - name: "æŸ¥è©¢åµŒå…¥"
        ai_model: "text-embedding-3-small"
      
      - name: "ç›¸ä¼¼åº¦æª¢ç´¢"
        method: "cosine_similarity"
        top_k: 10
      
# æ’åº
        ranking_factors:
          - semantic_similarity: 0.5
          - text_match: 0.3
          - recency: 0.2
      
      - name: "ä¸Šä¸‹æ–‡å¢å¼·"
        context_window: 3
        highlight_matches: true
```

# å·¥ä½œæµ

### ğŸ¤– æ™ºèƒ½ä»£ç¢¼ç”Ÿæˆ

#### é–‹ç™¼æµç¨‹è¨­è¨ˆ
```yaml
code_development:
# åˆ†æ
  requirement_analysis:
    ai_assistant: true
    model: "gpt-oss-120b-cloud"
    steps:
      - name: "éœ€æ±‚ç†è§£"
# åˆ†æ
      
      - name: "åŠŸèƒ½æ‹†åˆ†"
        prompt_template: "å°‡éœ€æ±‚æ‹†åˆ†ç‚ºå…·é«”çš„åŠŸèƒ½æ¨¡å¡Šï¼š{requirements}"
      
      - name: "æŠ€è¡“é¸å‹"
        prompt_template: "ç‚ºä»¥ä¸‹åŠŸèƒ½æ¨è–¦åˆé©çš„æŠ€è¡“æ–¹æ¡ˆï¼š{functions}"
  
  # ä»£ç¢¼ç”Ÿæˆ
  code_generation:
    ai_assistant: true
    model_routing:
      simple_tasks: "codellama:7b"
      complex_tasks: "gpt-oss:120b-cloud"
    
    steps:
      - name: "æ¶æ§‹è¨­è¨ˆ"
        model: "gpt-oss:120b-cloud"
        output_format: "markdown"
      
      - name: "ä»£ç¢¼å¯¦ç¾"
        model: "codellama:7b"
        language: "python"
        style: "pep8"
      
      - name: "å–®å…ƒæ¸¬è©¦"
        model: "codellama:7b"
        framework: "pytest"
        coverage_target: 80
  
  # ä»£ç¢¼å¯©æŸ¥
  code_review:
    ai_assistant: true
    models:
      syntax_check: "codellama:7b"
      security_scan: "gpt-oss:120b-cloud"
      performance_analysis: "mistral:7b"
    
    steps:
      - name: "èªæ³•æª¢æŸ¥"
        linters: ["flake8", "mypy", "black"]
      
      - name: "å®‰å…¨å¯©æŸ¥"
        scan_types:
          - "vulnerability"
          - "dependency"
          - "credential"
      
# åˆ†æ
        metrics:
          - "time_complexity"
          - "space_complexity"
          - "bottleneck_identification"
```

#### å¯¦ç¾è…³æœ¬
```python
# code_development_workflow.py
import requests
import json
from pathlib import Path
from typing import Dict, List

class CodeDevelopmentWorkflow:
    def __init__(self, project_path, obsidian_path, ai_config):
        self.project_path = Path(project_path)
        self.obsidian_path = Path(obsidian_path)
        self.ai_config = ai_config
    
    def develop_feature(self, requirement: str, complexity="medium"):
        """é–‹ç™¼æ–°åŠŸèƒ½"""
# åˆ†æ
        analysis = self._analyze_requirement(requirement)
        
        # 2. è¨­è¨ˆæ¶æ§‹
        architecture = self._design_architecture(analysis, complexity)
        
        # 3. ç”Ÿæˆä»£ç¢¼
        code = self._generate_code(analysis, architecture, complexity)
        
        # 4. ç”Ÿæˆæ¸¬è©¦
        tests = self._generate_tests(code, architecture)
        
        # 5. ä»£ç¢¼å¯©æŸ¥
        review = self._review_code(code, tests)
        
        # 6. ç”Ÿæˆæ–‡æª”
        documentation = self._generate_documentation(architecture, code)
        
        # 7. å„²å­˜åˆ°é …ç›®
        self._save_to_project(analysis, architecture, code, tests, documentation)
        
        # 8. å„²å­˜çŸ¥è­˜ç­†è¨˜
        self._create_knowledge_note(requirement, analysis, architecture, code)
        
        return {
            "analysis": analysis,
            "architecture": architecture,
            "code": code,
            "tests": tests,
            "review": review,
            "documentation": documentation
        }
    
    def _analyze_requirement(self, requirement: str) -> Dict:
# åˆ†æ
        model = "gpt-oss:120b-cloud"
        
        prompt = f"""
# åˆ†æ

éœ€æ±‚ï¼š{requirement}

è«‹æä¾›ä»¥ä¸‹å…§å®¹ï¼š
1. éœ€æ±‚ç†è§£ï¼ˆç”¨ç°¡æ½”çš„èªè¨€é‡æ–°æè¿°éœ€æ±‚ï¼‰
2. åŠŸèƒ½æ‹†åˆ†ï¼ˆå°‡éœ€æ±‚æ‹†åˆ†ç‚ºå…·é«”çš„åŠŸèƒ½æ¨¡å¡Šï¼‰
3. æŠ€è¡“é›£é»ï¼ˆåˆ—å‡ºå¯¦ç¾éç¨‹ä¸­å¯èƒ½é‡åˆ°çš„æŠ€è¡“é›£é»ï¼‰
4. é¢¨éšªè©•ä¼°ï¼ˆè©•ä¼°æ½›åœ¨çš„é¢¨éšªå’Œè§£æ±ºæ–¹æ¡ˆï¼‰
5. é©—æ”¶æ¨™æº–ï¼ˆæ˜ç¢ºåŠŸèƒ½çš„é©—æ”¶æ¨™æº–ï¼‰

è«‹ä»¥çµæ§‹åŒ–çš„æ ¼å¼è¼¸å‡ºã€‚
"""
        
        response = self._call_ai(model, prompt)
        return self._parse_analysis_response(response)
    
    def _design_architecture(self, analysis: Dict, complexity: str) -> Dict:
        """è¨­è¨ˆæ¶æ§‹"""
        model = "gpt-oss:120b-cloud"
        
        prompt = f"""
# åˆ†æ

# åˆ†æ
{json.dumps(analysis, ensure_ascii=False, indent=2)}

è«‹è¨­è¨ˆï¼š
1. ç³»çµ±æ¶æ§‹ï¼ˆåŒ…æ‹¬çµ„ä»¶ã€æ¨¡å¡Šã€æ•¸æ“šæµï¼‰
2. æŠ€è¡“é¸å‹ï¼ˆé¸æ“‡åˆé©çš„æŠ€è¡“æ£§ï¼‰
3. æ¥å£è¨­è¨ˆï¼ˆå®šç¾©ä¸»è¦çš„APIæ¥å£ï¼‰
4. æ•¸æ“šåº«è¨­è¨ˆï¼ˆå¦‚æœéœ€è¦ï¼‰
# éƒ¨ç½²

è«‹ä»¥çµæ§‹åŒ–çš„æ ¼å¼è¼¸å‡ºï¼Œå¯ä»¥åŒ…å«ä»£ç¢¼ç¤ºä¾‹ã€‚
"""
        
        response = self._call_ai(model, prompt)
        return self._parse_architecture_response(response)
    
    def _generate_code(self, analysis: Dict, architecture: Dict, complexity: str) -> str:
        """ç”Ÿæˆä»£ç¢¼"""
        model = self._select_code_model(complexity)
        
        prompt = f"""
åŸºæ–¼ä»¥ä¸‹éœ€æ±‚å’Œæ¶æ§‹è¨­è¨ˆï¼Œç”ŸæˆPythonä»£ç¢¼ï¼š

éœ€æ±‚ï¼š
{json.dumps(analysis, ensure_ascii=False, indent=2)}

æ¶æ§‹ï¼š
{json.dumps(architecture, ensure_ascii=False, indent=2)}

è¦æ±‚ï¼š
1. ä»£ç¢¼æ‡‰è©²éµå¾ªPEP8è¦ç¯„
2. åŒ…å«å®Œæ•´çš„æ–‡æª”å­—ç¬¦ä¸²
3. åŒ…å«é©ç•¶çš„éŒ¯èª¤è™•ç†
4. ä»£ç¢¼æ‡‰è©²æ¨¡çµ„åŒ–ã€å¯é‡ç”¨
5. åŒ…å«å¿…è¦çš„é¡å‹æç¤º

è«‹ç”Ÿæˆå®Œæ•´ã€å¯é‹è¡Œçš„ä»£ç¢¼ã€‚
"""
        
        response = self._call_ai(model, prompt)
        return response
    
    def _generate_tests(self, code: str, architecture: Dict) -> str:
        """ç”Ÿæˆæ¸¬è©¦"""
        model = "codellama:7b"
        
        prompt = f"""
ç‚ºä»¥ä¸‹ä»£ç¢¼ç”Ÿæˆpytestå–®å…ƒæ¸¬è©¦ï¼š

ä»£ç¢¼ï¼š
{code[:1000]}...

è¦æ±‚ï¼š
1. æ¸¬è©¦æ‡‰è©²è¦†è“‹ä¸»è¦åŠŸèƒ½è·¯å¾‘
2. åŒ…å«æ­£å¸¸æƒ…æ³å’Œç•°å¸¸æƒ…æ³çš„æ¸¬è©¦
3. ä½¿ç”¨pytestæ¡†æ¶
4. åŒ…å«å¿…è¦çš„mockå’Œfixture
5. ä»£ç¢¼æ‡‰è©²æ¸…æ™°ã€æ˜“æ–¼ç†è§£
"""
        
        response = self._call_ai(model, prompt)
        return response
    
    def _review_code(self, code: str, tests: str) -> Dict:
        """ä»£ç¢¼å¯©æŸ¥"""
        models = {
            "syntax": "codellama:7b",
            "security": "gpt-oss:120b-cloud",
            "performance": "mistral:7b"
        }
        
        review_results = {}
        
        # èªæ³•æª¢æŸ¥
        syntax_review = self._syntax_check(code)
        review_results["syntax"] = syntax_review
        
        # å®‰å…¨å¯©æŸ¥
        security_review = self._security_check(code)
        review_results["security"] = security_review
        
# åˆ†æ
        performance_review = self._performance_analysis(code)
        review_results["performance"] = performance_review
        
        return review_results
    
    def _generate_documentation(self, architecture: Dict, code: str) -> str:
        """ç”Ÿæˆæ–‡æª”"""
        model = "gpt-oss:120b-cloud"
        
        prompt = f"""
ç‚ºä»¥ä¸‹ä»£ç¢¼ç”ŸæˆæŠ€è¡“æ–‡æª”ï¼š

æ¶æ§‹è¨­è¨ˆï¼š
{json.dumps(architecture, ensure_ascii=False, indent=2)}

ä»£ç¢¼ï¼š
{code[:800]}...

æ–‡æª”æ‡‰è©²åŒ…æ‹¬ï¼š
1. åŠŸèƒ½æè¿°
2. APIæ¥å£èªªæ˜
3. ä½¿ç”¨ç¤ºä¾‹
# é…ç½®
5. æ•…éšœæ’é™¤

è«‹ä»¥Markdownæ ¼å¼è¼¸å‡ºå®Œæ•´çš„æŠ€è¡“æ–‡æª”ã€‚
"""
        
        response = self._call_ai(model, prompt)
        return response
    
    def _save_to_project(self, analysis, architecture, code, tests, documentation):
        """å„²å­˜åˆ°é …ç›®"""
        # å‰µå»ºé …ç›®ç›®éŒ„çµæ§‹
        project_dir = self.project_path / "src"
        project_dir.mkdir(parents=True, exist_ok=True)
        
        # å„²å­˜ä¸»ä»£ç¢¼
        main_file = project_dir / "main.py"
        with open(main_file, 'w', encoding='utf-8') as f:
            f.write(code)
        
        # å„²å­˜æ¸¬è©¦
        tests_file = self.project_path / "tests" / "test_main.py"
        tests_file.parent.mkdir(exist_ok=True)
        with open(tests_file, 'w', encoding='utf-8') as f:
            f.write(tests)
        
        # å„²å­˜æ–‡æª”
        docs_dir = self.project_path / "docs"
        docs_dir.mkdir(exist_ok=True)
        readme_file = docs_dir / "README.md"
        with open(readme_file, 'w', encoding='utf-8') as f:
            f.write(documentation)
        
        print(f"é …ç›®æª”æ¡ˆå·²å„²å­˜åˆ°ï¼š{self.project_path}")
    
    def _create_knowledge_note(self, requirement, analysis, architecture, code):
        """å‰µå»ºçŸ¥è­˜ç­†è¨˜"""
        model = "gpt-oss:120b-cloud"
        
        prompt = f"""
å‰µå»ºä¸€å€‹çŸ¥è­˜åº«ç­†è¨˜ï¼Œè¨˜éŒ„ä»¥ä¸‹é–‹ç™¼ç¶“é©—ï¼š

éœ€æ±‚ï¼š{requirement}

# åˆ†æ
{json.dumps(analysis, ensure_ascii=False, indent=2)}

æ¶æ§‹è¨­è¨ˆï¼š
{json.dumps(architecture, ensure_ascii=False, indent=2)[:500]}...

è«‹ç”Ÿæˆä¸€å€‹åŒ…å«ä»¥ä¸‹å…§å®¹çš„ç­†è¨˜ï¼š
1. é …ç›®èƒŒæ™¯
2. æŠ€è¡“æ–¹æ¡ˆ
3. å¯¦ç¾ç´°ç¯€
4. ç¶“é©—ç¸½çµ
5. æœ€ä½³å¯¦è¸

è«‹ä»¥Obsidian Markdownæ ¼å¼è¼¸å‡ºã€‚
"""
        
        note_content = self._call_ai(model, prompt)
        
        # å„²å­˜ç­†è¨˜åˆ°Obsidian
        safe_title = ''.join(c for c in requirement if c.isalnum() or c in (' ', '-', '_'))
        note_file = self.obsidian_path / f"{safe_title}.md"
        
        with open(note_file, 'w', encoding='utf-8') as f:
            f.write(note_content)
        
        print(f"çŸ¥è­˜ç­†è¨˜å·²å„²å­˜åˆ°ï¼š{note_file}")
    
    def _select_code_model(self, complexity: str) -> str:
        """é¸æ“‡ä»£ç¢¼ç”Ÿæˆæ¨¡å‹"""
        models = {
            "low": "codellama:7b",
            "medium": "codellama:13b",
            "high": "gpt-oss:120b-cloud"
        }
        return models[complexity]
    
    def _call_ai(self, model: str, prompt: str) -> str:
        """èª¿ç”¨AIæ¨¡å‹"""
        if model.startswith("llama") or model.startswith("mistral") or model.startswith("codellama"):
            return self._call_ollama(model, prompt)
        else:
            return self._call_gpt_oss(model, prompt)
    
    def _call_ollama(self, model: str, prompt: str) -> str:
        """èª¿ç”¨Ollama"""
        response = requests.post(
            "http://localhost:11434/api/generate",
            json={
                "model": model,
                "prompt": prompt,
                "stream": False
            }
        )
        return response.json()["response"]
    
    def _call_gpt_oss(self, model: str, prompt: str) -> str:
        """èª¿ç”¨GPT-oss"""
        headers = {
            "Authorization": f"Bearer {self.ai_config['gpt_oss_api_key']}",
            "Content-Type": "application/json"
        }
        
        response = requests.post(
            f"{self.ai_config['gpt_oss_base_url']}/chat/completions",
            headers=headers,
            json={
                "model": model,
                "messages": [
                    {"role": "system", "content": "ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„è»Ÿä»¶é–‹ç™¼æ¶æ§‹å¸«å’Œå·¥ç¨‹å¸«ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                "max_tokens": 3000,
                "temperature": 0.3
            }
        )
        
        return response.json()["choices"][0]["message"]["content"]
    
    def _parse_analysis_response(self, response: str) -> Dict:
# åˆ†æ
        # ç°¡å–®å¯¦ç¾ï¼šå¯¦éš›æ‡‰ç”¨ä¸­éœ€è¦æ›´å¾©é›œçš„è§£æ
        return {
            "requirement": response[:200],
            "analysis": response
        }
    
    def _parse_architecture_response(self, response: str) -> Dict:
        """è§£ææ¶æ§‹éŸ¿æ‡‰"""
        return {
            "architecture": response
        }
    
    def _syntax_check(self, code: str) -> Dict:
        """èªæ³•æª¢æŸ¥"""
        # èª¿ç”¨AIé€²è¡Œèªæ³•æª¢æŸ¥
        model = "codellama:7b"
        prompt = f"æª¢æŸ¥ä»¥ä¸‹Pythonä»£ç¢¼çš„èªæ³•å•é¡Œï¼š\n\n{code[:500]}..."
        
        response = self._call_ai(model, prompt)
        return {"suggestions": response}
    
    def _security_check(self, code: str) -> Dict:
        """å®‰å…¨æª¢æŸ¥"""
        model = "gpt-oss:120b-cloud"
# åˆ†æ
        
        response = self._call_ai(model, prompt)
        return {"security_issues": response}
    
    def _performance_analysis(self, code: str) -> Dict:
# åˆ†æ
        model = "mistral:7b"
# åˆ†æ
        
        response = self._call_ai(model, prompt)
        return {"performance_notes": response}

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    workflow = CodeDevelopmentWorkflow(
        project_path="D:/Projects/my-project",
        obsidian_path="D:/Knowledge/AI-value",
        ai_config={
            "gpt_oss_api_key": "your-api-key",
            "gpt_oss_base_url": "https://api.gpt-oss.com/v1"
        }
    )
    
    # é–‹ç™¼æ–°åŠŸèƒ½
    result = workflow.develop_feature(
        requirement="é–‹ç™¼ä¸€å€‹ç”¨æˆ¶èªè­‰ç³»çµ±ï¼Œæ”¯æŒè¨»å†Šã€ç™»éŒ„ã€å¯†ç¢¼é‡ç½®åŠŸèƒ½",
        complexity="medium"
    )
    
    print("åŠŸèƒ½é–‹ç™¼å®Œæˆï¼")
    print(f"ä»£ç¢¼å¯©æŸ¥çµæœï¼š{result['review']}")
```

# å·¥ä½œæµ

### ğŸ‘¥ åœ˜éšŠå”ä½œæµç¨‹

# å·¥ä½œæµ
```yaml
team_collaboration:
  # çŸ¥è­˜å…±äº«
  knowledge_sharing:
    steps:
      - name: "å…±äº«ç­†è¨˜"
        mechanism: "git_sync"
        conflict_resolution: "manual"
        notification: "team_members"
      
      - name: "çŸ¥è­˜åŒæ­¥"
        frequency: "real_time"
        sync_method: "obsidian_sync"
        backup_strategy: "version_control"
  
  # ä»£ç¢¼å”ä½œ
  code_collaboration:
    version_control: "git"
    branch_strategy: "feature_branches"
    code_review: "ai_assisted"
    
    steps:
      - name: "åŠŸèƒ½é–‹ç™¼"
        workflow: "code_development"
        
      - name: "ä»£ç¢¼å¯©æŸ¥"
        ai_assistant: true
        reviewers: ["human", "ai"]
        checklist:
          - "code_style"
          - "security"
          - "performance"
          - "test_coverage"
        
      - name: "åˆä½µè«‹æ±‚"
        ai_assistant: true
        model: "gpt-oss:120b-cloud"
        auto_merge_threshold: "high_confidence"
  
  # æ–‡æª”å”ä½œ
  documentation_collaboration:
    platform: "obsidian"
    review_process: "ai_assisted"
    
    steps:
      - name: "æ–‡æª”ç·¨å¯«"
        ai_assistant: true
        model: "gpt-oss-120b-cloud"
        template: "tech_documentation"
      
      - name: "å…§å®¹å¯©æŸ¥"
        ai_assistant: true
        model: "mistral:7b"
        checklist:
          - "accuracy"
          - "clarity"
          - "completeness"
          - "consistency"
      
      - name: "çŸ¥è­˜æå–"
        ai_assistant: true
        model: "llama2:7b"
        extraction_types:
          - "key_concepts"
          - "best_practices"
          - "lessons_learned"
```

### ğŸ”„ CI/CDæ•´åˆ

#### è‡ªå‹•åŒ–æµç¨‹
```yaml
# .github/workflows/ai_integration.yml
name: AI Integrated Development

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main]

jobs:
  ai_code_review:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v3
      
      - name: AI Code Review
        run: |
          python scripts/ai_code_review.py
        env:
          GPT_OSS_API_KEY: ${{ secrets.GPT_OSS_API_KEY }}
  
  ai_documentation:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v3
      
      - name: Generate Documentation
        run: |
          python scripts/generate_docs.py
        env:
          GPT_OSS_API_KEY: ${{ secrets.GPT_OSS_API_KEY }}
      
      - name: Update Knowledge Base
        run: |
          python scripts/update_obsidian.py
        env:
          OBSIDIAN_VAULT: ${{ secrets.OBSIDIAN_VAULT }}
```

## ğŸ“Š æ•ˆèƒ½å„ªåŒ–

### âš¡ æ•ˆèƒ½ç›£æ§

# å·¥ä½œæµ
```python
# workflow_monitor.py
import time
import psutil
import requests
from datetime import datetime
from typing import Dict, List

class WorkflowMonitor:
    def __init__(self):
        self.metrics = []
    
    def monitor_workflow(self, workflow_name: str, workflow_func, *args, **kwargs):
# å·¥ä½œæµ
        # è¨˜éŒ„é–‹å§‹æ™‚é–“
        start_time = time.time()
        start_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB
        
        # è¨˜éŒ„CPUä½¿ç”¨ç‡
        start_cpu = psutil.cpu_percent()
        
        try:
# å·¥ä½œæµ
            result = workflow_func(*args, **kwargs)
            
            # è¨˜éŒ„çµæŸæ™‚é–“
            end_time = time.time()
            end_memory = psutil.Process().memory_info().rss / 1024 / 1024
            end_cpu = psutil.cpu_percent()
            
            # è¨ˆç®—æŒ‡æ¨™
            duration = end_time - start_time
            memory_delta = end_memory - start_memory
            cpu_delta = end_cpu - start_cpu
            
            # è¨˜éŒ„æŒ‡æ¨™
            metric = {
                "workflow_name": workflow_name,
                "start_time": datetime.fromtimestamp(start_time).isoformat(),
                "end_time": datetime.fromtimestamp(end_time).isoformat(),
                "duration": duration,
                "start_memory": start_memory,
                "end_memory": end_memory,
                "memory_delta": memory_delta,
                "start_cpu": start_cpu,
                "end_cpu": end_cpu,
                "cpu_delta": cpu_delta,
                "status": "success"
            }
            
            self.metrics.append(metric)
            
            return {
                "result": result,
                "metric": metric
            }
        
        except Exception as e:
            # è¨˜éŒ„éŒ¯èª¤
            end_time = time.time()
            
            metric = {
                "workflow_name": workflow_name,
                "start_time": datetime.fromtimestamp(start_time).isoformat(),
                "end_time": datetime.fromtimestamp(end_time).isoformat(),
                "duration": end_time - start_time,
                "error": str(e),
                "status": "failed"
            }
            
            self.metrics.append(metric)
            
            return {
                "error": e,
                "metric": metric
            }
    
    def get_performance_report(self) -> Dict:
        """ç²å–æ•ˆèƒ½å ±å‘Š"""
        if not self.metrics:
            return {"message": "No metrics available"}
        
        successful_metrics = [m for m in self.metrics if m["status"] == "success"]
        failed_metrics = [m for m in self.metrics if m["status"] == "failed"]
        
        # è¨ˆç®—çµ±è¨ˆæ•¸æ“š
        total_duration = sum(m["duration"] for m in successful_metrics)
        avg_duration = total_duration / len(successful_metrics) if successful_metrics else 0
        
        max_duration = max(m["duration"] for m in successful_metrics) if successful_metrics else 0
        min_duration = min(m["duration"] for m in successful_metrics) if successful_metrics else 0
        
        total_memory_delta = sum(m["memory_delta"] for m in successful_metrics)
        avg_memory_delta = total_memory_delta / len(successful_metrics) if successful_metrics else 0
        
        success_rate = len(successful_metrics) / len(self.metrics) * 100 if self.metrics else 0
        
        return {
            "total_executions": len(self.metrics),
            "successful_executions": len(successful_metrics),
            "failed_executions": len(failed_metrics),
            "success_rate": success_rate,
            "duration_stats": {
                "total": total_duration,
                "average": avg_duration,
                "max": max_duration,
                "min": min_duration
            },
            "memory_stats": {
                "average_delta": avg_memory_delta
            }
        }
    
    def export_metrics(self, filename: str):
        """å°å‡ºæŒ‡æ¨™åˆ°æª”æ¡ˆ"""
        import json
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(self.metrics, f, indent=2, ensure_ascii=False)
        
        print(f"æŒ‡æ¨™å·²å°å‡ºåˆ°ï¼š{filename}")
```

# å·¥ä½œæµ

# é…ç½®
- [ ] æ‰€æœ‰çµ„ä»¶å®‰è£ä¸¦é©—è­‰
# é…ç½®
# å·¥ä½œæµ
- [ ] æ•´åˆåŠŸèƒ½é©—è­‰æ­£å¸¸

# å·¥ä½œæµ
# å·¥ä½œæµ
# å·¥ä½œæµ
# å·¥ä½œæµ
# é…ç½®

# é…ç½®
# éƒ¨ç½²
# ç®¡ç†
# é…ç½®
- [ ] æ•…éšœæ’é™¤æº–å‚™å°±ç·’

---

## ğŸ‰ ä¸‹ä¸€æ­¥

# å·¥ä½œæµ

# ç®¡ç†
2. **ğŸ’» é–‹ç™¼ç¬¬ä¸€å€‹AIè¼”åŠ©é …ç›®**ï¼šäº«å—æ™ºèƒ½ä»£ç¢¼ç”Ÿæˆ
3. **ğŸ¤ è¨­ç½®åœ˜éšŠå”ä½œ**ï¼šèˆ‡åœ˜éšŠæˆå“¡é«˜æ•ˆå”ä½œ
# å·¥ä½œæµ

# å·¥ä½œæµ

---

# æ›´æ–°