# 指南

# 部署

## 📖 概述

# 指南

## 🛠️ 系統要求

### 💻 硬體要求

# 配置
# 配置
|------|----------|------|
| **CPU** | 4核心 | x86_64架構 |
| **記憶體** | 8GB | 用于運行7B模型 |
| **儲存** | 20GB | 模型和數據儲存 |
| **網路** | 1Mbps | 下載模型需要 |

# 配置
# 配置
|------|----------|------|
| **CPU** | 8核心+ | 更好的推理效能 |
| **記憶體** | 16GB+ | 運行更大模型 |
| **儲存** | 50GB+ | 多模型儲存 |
| **GPU** | RTX 3060+ | 显著加速推理 |

### 🌐 軟體要求

# 版本
|------|----------|------|
| **操作系統** | Windows 10+/macOS 10.15+/Linux | 主運行環境 |
# 部署
| **Python** | 3.8+ | 部分工具需要 |
# 管理

## 📦 安裝方式

### 🐳 方式一：Docker安裝（推荐）

#### 1. 安裝Docker
```bash
# Ubuntu/Debian
curl -fsSL https://get.docker.com -o get-docker.sh
sudo sh get-docker.sh

# CentOS/RHEL
sudo yum install -y yum-utils
sudo yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo
sudo yum install docker-ce docker-ce-cli containerd.io

# macOS
brew install --cask docker

# Windows
# 下載Docker Desktop for Windows
```

#### 2. 拉取Ollama镜像
```bash
# 拉取官方镜像
docker pull ollama/ollama:latest

# 驗證镜像
docker images | grep ollama
```

#### 3. 啟動Ollama容器
```bash
# 基礎啟動
docker run -d \
  --name ollama \
  -p 11434:11434 \
  -v ollama:/root/.ollama \
  ollama/ollama:latest

# GPU加速啟動（NVIDIA）
docker run -d \
  --name ollama-gpu \
  --gpus all \
  -p 11434:11434 \
  -v ollama:/root/.ollama \
  ollama/ollama:latest
```

#### 4. 驗證安裝
```bash
# 檢查容器狀態
docker ps | grep ollama

# 測試API
curl http://localhost:11434/api/version
```

### 💻 方式二：直接安裝

#### Linux安裝
```bash
# 下載安裝腳本
curl -fsSL https://ollama.ai/install.sh | sh

# 驗證安裝
ollama --version
```

#### macOS安裝
```bash
# 使用Homebrew安裝
brew install ollama
```

#### Windows安裝
```powershell
# 使用Chocolatey安裝
choco install ollama
```

# 管理

### 📦 常用模型下載

#### 通用对话模型
```bash
# 下載Llama 2系列
ollama pull llama2:7b
ollama pull llama2:13b

# 下載Mistral系列
ollama pull mistral:7b
ollama pull mixtral:8x7b

# 下載Code Llama
ollama pull codellama:7b
```

#### 中文優化模型
```bash
# Qwen系列
ollama pull qwen:7b
ollama pull qwen:14b

# Baichuan系列
ollama pull baichuan:7b
```

# 管理
```bash
# 列出已下載模型
ollama list

# 查看
ollama show llama2:7b

# 刪除模型
ollama rm llama2:7b

# 創建模型副本
ollama cp llama2:7b my-llama2:7b
```

# 配置

# 配置

# 配置
```bash
# 配置
mkdir -p ~/.ollama

# 配置
cat > ~/.ollama/config.yaml << EOF
version: "1.0"
host: "0.0.0.0"
port: 11434

models:
  default: "llama2:7b"
  auto_download: false
  cache_size: "10GB"

server:
  max_connections: 100
  timeout: 30
  keep_alive: "5m"

logging:
  level: "info"
  file: "~/.ollama/logs/ollama.log"
EOF
```

### 🚀 效能優化

# 配置
```yaml
# 配置
gpu:
  enabled: true
  device: "all"
  memory_fraction: 0.8
  precision: "fp16"

memory:
  max_batch_size: 8
  use_cache: true
```

#### 推理優化
```bash
# 設置環境变量優化效能
export OLLAMA_NUM_PARALLEL=2
export OLLAMA_MAX_LOADED_MODELS=3
export OLLAMA_GPU_MEMORY_FRACTION=0.8
```

## 🔗 系統整合

### 💻 OpenCode整合

# 配置
```yaml
# 配置
ai_models:
  ollama_local:
    provider: "ollama"
    base_url: "http://localhost:11434"
    models:
      - "llama2:7b"
      - "codellama:7b"
      - "mistral:7b"
    
    # 模型使用策略
    usage_strategy:
      code_generation: "codellama:7b"
      general_chat: "llama2:7b"
      complex_tasks: "mistral:7b"
```

#### API連接測試
```bash
# 測試Ollama API
curl -X POST http://localhost:11434/api/generate \
  -H "Content-Type: application/json" \
  -d '{
    "model": "llama2:7b",
    "prompt": "Hello, how are you?",
    "stream": false
  }'
```

### 📝 Obsidian整合

# 配置
```javascript
# 配置
{
  "api_url": "http://localhost:11434/api/generate",
  "model": "llama2:7b",
  "parameters": {
    "temperature": 0.7,
    "top_p": 0.9,
    "max_tokens": 2000
  }
}
```

#### 自動化腳本
```python
# Python腳本用于Obsidian整合
import requests
import json

def generate_content(prompt, model="llama2:7b"):
    url = "http://localhost:11434/api/generate"
    payload = {
        "model": model,
        "prompt": prompt,
        "stream": False
    }
    
    response = requests.post(url, json=payload)
    return response.json()['response']

# 使用示例
content = generate_content("写一个关于机器學習的簡介")
print(content)
```

# 管理

### 📈 效能監控

#### 系統監控腳本
```bash
#!/bin/bash
# monitor_ollama.sh

# 檢查服务狀態
check_service() {
    if curl -s http://localhost:11434/api/version > /dev/null; then
        echo "✅ Ollama服务運行正常"
    else
        echo "❌ Ollama服务异常"
    fi
}

# 檢查資源使用
check_resources() {
    echo "📊 系統資源使用情况："
    echo "CPU使用率: $(top -bn1 | grep "Cpu(s)" | awk '{print $2}')"
    echo "記憶體使用: $(free -h | grep Mem | awk '{print $3"/"$2}')"
    echo "磁碟使用: $(df -h / | tail -1 | awk '{print $3"/"$2}')"
}

# 檢查模型狀態
check_models() {
    echo "🤖 已下載模型："
    ollama list
}

# 主函数
main() {
    check_service
    check_resources
    check_models
}

main
```

#### 效能測試腳本
```python
# performance_test.py
import time
import requests
import statistics

def test_model_performance(model, prompt, iterations=5):
    url = "http://localhost:11434/api/generate"
    payload = {
        "model": model,
        "prompt": prompt,
        "stream": False
    }
    
    response_times = []
    
    for i in range(iterations):
        start_time = time.time()
        response = requests.post(url, json=payload)
        end_time = time.time()
        
        response_times.append(end_time - start_time)
        print(f"第{i+1}次測試: {response_times[-1]:.2f}秒")
    
    avg_time = statistics.mean(response_times)
    print(f"\n平均響應時間: {avg_time:.2f}秒")
    print(f"最快響應時間: {min(response_times):.2f}秒")
    print(f"最慢響應時間: {max(response_times):.2f}秒")

# 測試示例
test_model_performance("llama2:7b", "解釋什么是机器學習？")
```

### 🔧 維護腳本

#### 自動清理腳本
```bash
#!/bin/bash
# cleanup_ollama.sh

# 清理未使用的模型
cleanup_unused_models() {
    echo "🧹 清理未使用的模型..."
    # 这里可以新增逻辑来判斷哪些模型是未使用的
    # ollama rm unused-model
}

# 清理日志檔案
cleanup_logs() {
    echo "🗑️ 清理旧日志檔案..."
    find ~/.ollama/logs -name "*.log" -mtime +30 -delete
}

# 压缩模型檔案
compress_models() {
    echo "📦 压缩不常用的模型..."
    # 可以新增逻辑来压缩长時間未使用的模型
}

# 主函数
main() {
    cleanup_unused_models
    cleanup_logs
    compress_models
    echo "✅ 清理完成"
}

main
```

## 🛠️ 故障排除

### 🔧 常见問題解決

#### 服务啟動失败
```bash
# 檢查端口占用
netstat -tulpn | grep 11434

# 强制停止占用端口的进程
sudo fuser -k 11434/tcp

# 重新啟動服务
docker restart ollama
```

#### 記憶體不足
```bash
# 檢查記憶體使用
free -h

# 调整模型大小限制
export OLLAMA_MAX_LOADED_MODELS=1

# 使用更小的模型
# 版本
```

#### GPU不可用
```bash
# 檢查NVIDIA驅動程式
nvidia-smi

# 檢查Docker GPU支持
docker run --rm --gpus all nvidia/cuda:11.0-base nvidia-smi

# 重新安裝NVIDIA Container Toolkit
sudo apt-get install nvidia-container-toolkit
sudo systemctl restart docker
```

### 📋 诊断腳本

#### 全面诊断
```bash
#!/bin/bash
# diagnose_ollama.sh

echo "🔍 Ollama系統诊断開始..."

# 檢查Docker狀態
echo "📦 Docker狀態檢查："
systemctl is-active docker
docker --version

# 檢查Ollama容器
echo "🐳 Ollama容器狀態："
docker ps -a | grep ollama

# 檢查網路連接
echo "🌐 網路連接測試："
curl -I http://localhost:11434

# 檢查磁碟空間
echo "💾 磁碟空間檢查："
df -h

# 檢查記憶體使用
echo "🧠 記憶體使用檢查："
free -h

echo "🔍 诊断完成"
```

## 📚 擴展功能

### 🔄 模型切換

#### 智能模型選擇
```python
# smart_model_selector.py
import requests
import json

def get_optimal_model(task_type, complexity, available_models):
    """根据任務类型和复杂度選擇最优模型"""
    
    model_recommendations = {
        "code_generation": {
            "low": "codellama:7b",
            "medium": "codellama:13b", 
            "high": "mixtral:8x7b"
        },
        "text_generation": {
            "low": "llama2:7b",
            "medium": "mistral:7b",
            "high": "mixtral:8x7b"
        },
        "translation": {
            "low": "llama2:7b",
            "medium": "qwen:14b",
            "high": "mixtral:8x7b"
        }
    }
    
    recommended = model_recommendations.get(task_type, {}).get(complexity, "llama2:7b")
    
    if recommended in available_models:
        return recommended
    else:
        return available_models[0]  # 返回第一个可用模型

# 使用示例
available_models = ["llama2:7b", "codellama:7b", "mistral:7b"]
optimal = get_optimal_model("code_generation", "medium", available_models)
print(f"推荐模型: {optimal}")
```

### 📝 批量處理

#### 批量文本生成
```python
# batch_generator.py
import asyncio
import aiohttp
import json

async def batch_generate(prompts, model="llama2:7b"):
    """批量生成文本"""
    
    url = "http://localhost:11434/api/generate"
    
    async with aiohttp.ClientSession() as session:
        tasks = []
        for prompt in prompts:
            task = generate_single(session, url, model, prompt)
            tasks.append(task)
        
        results = await asyncio.gather(*tasks)
        return results

async def generate_single(session, url, model, prompt):
    """单个文本生成"""
    payload = {
        "model": model,
        "prompt": prompt,
        "stream": False
    }
    
    async with session.post(url, json=payload) as response:
        result = await response.json()
        return result['response']

# 使用示例
prompts = [
    "解釋什么是人工智能",
    "編寫一个Python函数",
    "推荐几本机器學習书籍"
]

async def main():
    results = await batch_generate(prompts)
    for i, result in enumerate(results):
        print(f"結果{i+1}: {result}")

asyncio.run(main())
```

# 部署

# 部署
- [ ] Ollama服务安裝完成
- [ ] 至少下載一个模型
- [ ] 服务啟動并驗證
- [ ] 基礎API測試通過

# 配置
# 配置
# 配置
- [ ] 模型切換策略設置
# 部署

# 配置
# 配置
# 部署
- [ ] 自動清理腳本設置
- [ ] 故障排除腳本准备

# 管理
- [ ] 使用統計監控
- [ ] 效能指标收集
# 配置
# 管理

---

## 🎉 下一步

# 部署

1. **🤖 測試各种模型**：體驗不同模型的特點
2. **💻 整合開發工具**：在OpenCode中使用本地模型
# 管理
4. **📊 監控系統狀態**：使用監控腳本跟踪效能

> 💡 **提示**：建議从较小的模型開始測試，逐步尝试更大、更强大的模型。

---

# 更新