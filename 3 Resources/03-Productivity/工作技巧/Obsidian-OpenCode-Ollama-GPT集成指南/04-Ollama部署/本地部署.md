# 🦙 Ollama本地部署指南

> 🎯 **章节目标**：在本地部署Ollama并与OpenCode、Obsidian集成 | ⏱️ **预计时间**：90分钟 | 📊 **难度等级**：⭐⭐⭐

## 📖 概述

Ollama是一个强大的本地大语言模型运行平台，支持多种开源模型。通过本地部署Ollama，可以实现数据隐私保护、降低API成本和获得更快的响应速度。本指南将详细介绍Ollama的安装、配置和集成。

## 🛠️ 系统要求

### 💻 硬件要求

#### 最低配置
| 组件 | 最低配置 | 说明 |
|------|----------|------|
| **CPU** | 4核心 | x86_64架构 |
| **内存** | 8GB | 用于运行7B模型 |
| **存储** | 20GB | 模型和数据存储 |
| **网络** | 1Mbps | 下载模型需要 |

#### 推荐配置
| 组件 | 推荐配置 | 说明 |
|------|----------|------|
| **CPU** | 8核心+ | 更好的推理性能 |
| **内存** | 16GB+ | 运行更大模型 |
| **存储** | 50GB+ | 多模型存储 |
| **GPU** | RTX 3060+ | 显著加速推理 |

### 🌐 软件要求

| 软件 | 版本要求 | 用途 |
|------|----------|------|
| **操作系统** | Windows 10+/macOS 10.15+/Linux | 主运行环境 |
| **Docker** | 20.10+ | 容器化部署（推荐） |
| **Python** | 3.8+ | 部分工具需要 |
| **Git** | 2.25+ | 代码管理 |

## 📦 安装方式

### 🐳 方式一：Docker安装（推荐）

#### 1. 安装Docker
```bash
# Ubuntu/Debian
curl -fsSL https://get.docker.com -o get-docker.sh
sudo sh get-docker.sh

# CentOS/RHEL
sudo yum install -y yum-utils
sudo yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo
sudo yum install docker-ce docker-ce-cli containerd.io

# macOS
brew install --cask docker

# Windows
# 下载Docker Desktop for Windows
```

#### 2. 拉取Ollama镜像
```bash
# 拉取官方镜像
docker pull ollama/ollama:latest

# 验证镜像
docker images | grep ollama
```

#### 3. 启动Ollama容器
```bash
# 基础启动
docker run -d \
  --name ollama \
  -p 11434:11434 \
  -v ollama:/root/.ollama \
  ollama/ollama:latest

# GPU加速启动（NVIDIA）
docker run -d \
  --name ollama-gpu \
  --gpus all \
  -p 11434:11434 \
  -v ollama:/root/.ollama \
  ollama/ollama:latest
```

#### 4. 验证安装
```bash
# 检查容器状态
docker ps | grep ollama

# 测试API
curl http://localhost:11434/api/version
```

### 💻 方式二：直接安装

#### Linux安装
```bash
# 下载安装脚本
curl -fsSL https://ollama.ai/install.sh | sh

# 验证安装
ollama --version
```

#### macOS安装
```bash
# 使用Homebrew安装
brew install ollama
```

#### Windows安装
```powershell
# 使用Chocolatey安装
choco install ollama
```

## 🤖 模型管理

### 📦 常用模型下载

#### 通用对话模型
```bash
# 下载Llama 2系列
ollama pull llama2:7b
ollama pull llama2:13b

# 下载Mistral系列
ollama pull mistral:7b
ollama pull mixtral:8x7b

# 下载Code Llama
ollama pull codellama:7b
```

#### 中文优化模型
```bash
# Qwen系列
ollama pull qwen:7b
ollama pull qwen:14b

# Baichuan系列
ollama pull baichuan:7b
```

### 🎛️ 模型管理命令
```bash
# 列出已下载模型
ollama list

# 查看模型信息
ollama show llama2:7b

# 删除模型
ollama rm llama2:7b

# 创建模型副本
ollama cp llama2:7b my-llama2:7b
```

## 🎛️ 高级配置

### 📋 配置文件设置

#### 创建配置文件
```bash
# 创建配置目录
mkdir -p ~/.ollama

# 创建配置文件
cat > ~/.ollama/config.yaml << EOF
version: "1.0"
host: "0.0.0.0"
port: 11434

models:
  default: "llama2:7b"
  auto_download: false
  cache_size: "10GB"

server:
  max_connections: 100
  timeout: 30
  keep_alive: "5m"

logging:
  level: "info"
  file: "~/.ollama/logs/ollama.log"
EOF
```

### 🚀 性能优化

#### GPU配置
```yaml
# GPU优化配置
gpu:
  enabled: true
  device: "all"
  memory_fraction: 0.8
  precision: "fp16"

memory:
  max_batch_size: 8
  use_cache: true
```

#### 推理优化
```bash
# 设置环境变量优化性能
export OLLAMA_NUM_PARALLEL=2
export OLLAMA_MAX_LOADED_MODELS=3
export OLLAMA_GPU_MEMORY_FRACTION=0.8
```

## 🔗 系统集成

### 💻 OpenCode集成

#### 配置OpenCode连接
```yaml
# OpenCode配置文件
ai_models:
  ollama_local:
    provider: "ollama"
    base_url: "http://localhost:11434"
    models:
      - "llama2:7b"
      - "codellama:7b"
      - "mistral:7b"
    
    # 模型使用策略
    usage_strategy:
      code_generation: "codellama:7b"
      general_chat: "llama2:7b"
      complex_tasks: "mistral:7b"
```

#### API连接测试
```bash
# 测试Ollama API
curl -X POST http://localhost:11434/api/generate \
  -H "Content-Type: application/json" \
  -d '{
    "model": "llama2:7b",
    "prompt": "Hello, how are you?",
    "stream": false
  }'
```

### 📝 Obsidian集成

#### 插件配置
```javascript
// Obsidian Text Generator插件配置
{
  "api_url": "http://localhost:11434/api/generate",
  "model": "llama2:7b",
  "parameters": {
    "temperature": 0.7,
    "top_p": 0.9,
    "max_tokens": 2000
  }
}
```

#### 自动化脚本
```python
# Python脚本用于Obsidian集成
import requests
import json

def generate_content(prompt, model="llama2:7b"):
    url = "http://localhost:11434/api/generate"
    payload = {
        "model": model,
        "prompt": prompt,
        "stream": False
    }
    
    response = requests.post(url, json=payload)
    return response.json()['response']

# 使用示例
content = generate_content("写一个关于机器学习的简介")
print(content)
```

## 📊 监控与管理

### 📈 性能监控

#### 系统监控脚本
```bash
#!/bin/bash
# monitor_ollama.sh

# 检查服务状态
check_service() {
    if curl -s http://localhost:11434/api/version > /dev/null; then
        echo "✅ Ollama服务运行正常"
    else
        echo "❌ Ollama服务异常"
    fi
}

# 检查资源使用
check_resources() {
    echo "📊 系统资源使用情况："
    echo "CPU使用率: $(top -bn1 | grep "Cpu(s)" | awk '{print $2}')"
    echo "内存使用: $(free -h | grep Mem | awk '{print $3"/"$2}')"
    echo "磁盘使用: $(df -h / | tail -1 | awk '{print $3"/"$2}')"
}

# 检查模型状态
check_models() {
    echo "🤖 已下载模型："
    ollama list
}

# 主函数
main() {
    check_service
    check_resources
    check_models
}

main
```

#### 性能测试脚本
```python
# performance_test.py
import time
import requests
import statistics

def test_model_performance(model, prompt, iterations=5):
    url = "http://localhost:11434/api/generate"
    payload = {
        "model": model,
        "prompt": prompt,
        "stream": False
    }
    
    response_times = []
    
    for i in range(iterations):
        start_time = time.time()
        response = requests.post(url, json=payload)
        end_time = time.time()
        
        response_times.append(end_time - start_time)
        print(f"第{i+1}次测试: {response_times[-1]:.2f}秒")
    
    avg_time = statistics.mean(response_times)
    print(f"\n平均响应时间: {avg_time:.2f}秒")
    print(f"最快响应时间: {min(response_times):.2f}秒")
    print(f"最慢响应时间: {max(response_times):.2f}秒")

# 测试示例
test_model_performance("llama2:7b", "解释什么是机器学习？")
```

### 🔧 维护脚本

#### 自动清理脚本
```bash
#!/bin/bash
# cleanup_ollama.sh

# 清理未使用的模型
cleanup_unused_models() {
    echo "🧹 清理未使用的模型..."
    # 这里可以添加逻辑来判断哪些模型是未使用的
    # ollama rm unused-model
}

# 清理日志文件
cleanup_logs() {
    echo "🗑️ 清理旧日志文件..."
    find ~/.ollama/logs -name "*.log" -mtime +30 -delete
}

# 压缩模型文件
compress_models() {
    echo "📦 压缩不常用的模型..."
    # 可以添加逻辑来压缩长时间未使用的模型
}

# 主函数
main() {
    cleanup_unused_models
    cleanup_logs
    compress_models
    echo "✅ 清理完成"
}

main
```

## 🛠️ 故障排除

### 🔧 常见问题解决

#### 服务启动失败
```bash
# 检查端口占用
netstat -tulpn | grep 11434

# 强制停止占用端口的进程
sudo fuser -k 11434/tcp

# 重新启动服务
docker restart ollama
```

#### 内存不足
```bash
# 检查内存使用
free -h

# 调整模型大小限制
export OLLAMA_MAX_LOADED_MODELS=1

# 使用更小的模型
ollama pull llama2:3b  # 3B参数版本
```

#### GPU不可用
```bash
# 检查NVIDIA驱动
nvidia-smi

# 检查Docker GPU支持
docker run --rm --gpus all nvidia/cuda:11.0-base nvidia-smi

# 重新安装NVIDIA Container Toolkit
sudo apt-get install nvidia-container-toolkit
sudo systemctl restart docker
```

### 📋 诊断脚本

#### 全面诊断
```bash
#!/bin/bash
# diagnose_ollama.sh

echo "🔍 Ollama系统诊断开始..."

# 检查Docker状态
echo "📦 Docker状态检查："
systemctl is-active docker
docker --version

# 检查Ollama容器
echo "🐳 Ollama容器状态："
docker ps -a | grep ollama

# 检查网络连接
echo "🌐 网络连接测试："
curl -I http://localhost:11434

# 检查磁盘空间
echo "💾 磁盘空间检查："
df -h

# 检查内存使用
echo "🧠 内存使用检查："
free -h

echo "🔍 诊断完成"
```

## 📚 扩展功能

### 🔄 模型切换

#### 智能模型选择
```python
# smart_model_selector.py
import requests
import json

def get_optimal_model(task_type, complexity, available_models):
    """根据任务类型和复杂度选择最优模型"""
    
    model_recommendations = {
        "code_generation": {
            "low": "codellama:7b",
            "medium": "codellama:13b", 
            "high": "mixtral:8x7b"
        },
        "text_generation": {
            "low": "llama2:7b",
            "medium": "mistral:7b",
            "high": "mixtral:8x7b"
        },
        "translation": {
            "low": "llama2:7b",
            "medium": "qwen:14b",
            "high": "mixtral:8x7b"
        }
    }
    
    recommended = model_recommendations.get(task_type, {}).get(complexity, "llama2:7b")
    
    if recommended in available_models:
        return recommended
    else:
        return available_models[0]  # 返回第一个可用模型

# 使用示例
available_models = ["llama2:7b", "codellama:7b", "mistral:7b"]
optimal = get_optimal_model("code_generation", "medium", available_models)
print(f"推荐模型: {optimal}")
```

### 📝 批量处理

#### 批量文本生成
```python
# batch_generator.py
import asyncio
import aiohttp
import json

async def batch_generate(prompts, model="llama2:7b"):
    """批量生成文本"""
    
    url = "http://localhost:11434/api/generate"
    
    async with aiohttp.ClientSession() as session:
        tasks = []
        for prompt in prompts:
            task = generate_single(session, url, model, prompt)
            tasks.append(task)
        
        results = await asyncio.gather(*tasks)
        return results

async def generate_single(session, url, model, prompt):
    """单个文本生成"""
    payload = {
        "model": model,
        "prompt": prompt,
        "stream": False
    }
    
    async with session.post(url, json=payload) as response:
        result = await response.json()
        return result['response']

# 使用示例
prompts = [
    "解释什么是人工智能",
    "编写一个Python函数",
    "推荐几本机器学习书籍"
]

async def main():
    results = await batch_generate(prompts)
    for i, result in enumerate(results):
        print(f"结果{i+1}: {result}")

asyncio.run(main())
```

## ✅ 部署检查清单

### 🔧 基础部署
- [ ] Ollama服务安装完成
- [ ] 至少下载一个模型
- [ ] 服务启动并验证
- [ ] 基础API测试通过

### 🎯 集成配置
- [ ] OpenCode连接配置
- [ ] Obsidian插件配置
- [ ] 模型切换策略设置
- [ ] 自动化脚本部署

### 🚀 优化配置
- [ ] GPU加速配置（如适用）
- [ ] 性能监控脚本部署
- [ ] 自动清理脚本设置
- [ ] 故障排除脚本准备

### 📊 管理维护
- [ ] 使用统计监控
- [ ] 性能指标收集
- [ ] 定期备份配置
- [ ] 日志管理策略

---

## 🎉 下一步

部署完成后，您可以：

1. **🤖 测试各种模型**：体验不同模型的特点
2. **💻 集成开发工具**：在OpenCode中使用本地模型
3. **📝 增强知识管理**：在Obsidian中集成AI功能
4. **📊 监控系统状态**：使用监控脚本跟踪性能

> 💡 **提示**：建议从较小的模型开始测试，逐步尝试更大、更强大的模型。

---

**📝 创建时间**：2026-01-21 | **🔄 最后更新**：2026-01-21 | **👥 维护者**：Ollama Deployment Team