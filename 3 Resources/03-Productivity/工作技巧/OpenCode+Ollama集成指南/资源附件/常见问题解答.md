# OpenCode + Ollama 常见問題解答
# OpenCode + Ollama Frequently Asked Questions

## 🚀 安裝与設置 / Installation & Setup

### Q1: 系統要求是什么？

**A**: 基本要求：
- **操作系統**: Linux, macOS, Windows
- **記憶體**: 最少8GB，推荐16GB+
- **GPU**: 可选但推荐，8GB+ VRAM
- **儲存**: 至少20GB可用空間
- **軟體**: Node.js v18+, Python 3.8+ (可选)

### Q2: 如何在没有GPU的机器上使用？

**A**: 可以使用CPU模式，但效能较慢：
```bash
# CPU模式啟動
export OLLAMA_NUM_PARALLEL=1  # 减少并行
export OLLAMA_MAX_LOADED_MODELS=1  # 减少加载模型数
ollama serve
```

**推荐模型**:
- `qwen2.5:3b` - 轻量级，CPU友好
- `qwen2.5:1.5b` - 最轻量選擇

### Q3: 安裝過程中权限被拒絕？

**A**: 解決方案：
```bash
# Linux/macOS
chmod +x install-opencode-ollama.sh
sudo ./install-opencode-ollama.sh

# 或者使用使用者目錄安裝
npm config set prefix ~/.npm-global
export PATH=~/.npm-global/bin:$PATH
npm install -g @opencode-ai/cli
```

### Q4: 如何移除和重新安裝？

**A**: 完全移除步骤：
```bash
# 停止服务
pkill ollama
pkill opencode

# 移除OpenCode
npm uninstall -g @opencode-ai/cli

# 刪除Ollama (Linux/macOS)
sudo rm -rf /usr/local/bin/ollama
sudo rm -rf ~/.ollama

# 配置
rm -rf ~/.config/opencode

# 清理環境变量
編輯 ~/.bashrc 或 ~/.zshrc，刪除相關行
```

---

# 配置

### Q5: OpenCode找不到本地模型？

**A**: 檢查清單：
1. **Ollama服务運行**:
   ```bash
   curl -s http://localhost:11434/api/tags
   ```

2. **模型已下載**:
   ```bash
   ollama list
   ```

# 配置
   ```bash
   cat ~/.config/opencode/opencode.json | jq .
   ```

4. **重载OpenCode**:
   ```bash
   opencode --reload-config
   ```

# 配置

**A**: 常见错误和修復：
```json
// ❌ 错误：缺少逗号
{
  "model": "ollama/qwen2.5-coder:7b"
  "provider": {  // 这里缺少逗号
    "ollama": { ... }
  }
}

// ✅ 正确
{
  "model": "ollama/qwen2.5-coder:7b",
  "provider": {
    "ollama": { ... }
  }
}
```

**驗證工具**:
```bash
# 使用jq驗證JSON
cat ~/.config/opencode/opencode.json | jq .

# 或使用Python驗證
python3 -m json.tool ~/.config/opencode/opencode.json
```

### Q7: 如何切換不同的模型？

# 方法
```json
{
  "model": "ollama/qwen2.5-coder:14b"  // 更改这里
}
```

# 方法
```bash
opencode --model ollama/qwen2.5:7b
```

# 方法
```
/models  # 在OpenCode中执行此命令
```

---

## 🤖 模型相關問題 / Model Related Issues

### Q8: 模型下載失败或中断？

**A**: 解決方案：
```bash
# 方法
ollama pull qwen2.5-coder:7b

# 方法
export https_proxy=http://proxy:port
ollama pull qwen2.5-coder:7b

# 方法
wget https://ollama.com/models/qwen2.5-coder:7b.gguf
mv qwen2.5-coder:7b.gguf ~/.ollama/models/
```

### Q9: 模型占用記憶體过大？

**A**: 優化策略：
```bash
# 1. 設置最大記憶體使用
export OLLAMA_MAX_LOADED_MODELS=1

# 2. 使用更小的模型
ollama pull qwen2.5:3b

# 3. 启用模型压缩
ollama run qwen2.5-coder:7b
/set parameter num_ctx 4096  # 减少上下文
```

# 更新

# 更新
```bash
# 查看
ollama show qwen2.5-coder:7b

# 版本
ollama rm qwen2.5-coder:7b

# 版本
ollama pull qwen2.5-coder:7b:latest

# 更新
ollama show qwen2.5-coder:7b
```

---

## ⚡ 效能問題 / Performance Issues

### Q11: 響應速度很慢？

**A**: 效能優化：
```bash
# 1. GPU優化
export OLLAMA_GPU_MEMORY_FRACTION=0.9
export OLLAMA_NUM_PARALLEL=4

# 2. 减少上下文大小
ollama run qwen2.5-coder:7b
/set parameter num_ctx 2048

# 3. 使用量化模型
ollama pull qwen2.5-coder:7b-q4_0

# 4. 预加载模型
export OLLAMA_KEEP_ALIVE=24h
```

### Q12: 記憶體不足错误？

# 管理
```bash
# 1. 監控記憶體使用
watch -n 1 'ps aux | grep ollama'

# 2. 限制并发
export OLLAMA_NUM_PARALLEL=1

# 3. 使用交换檔案
sudo swapon --show
sudo fallocate -l 4G /swapfile
sudo chmod 600 /swapfile
sudo mkswap /swapfile
sudo swapon /swapfile

# 4. 重启Ollama服务
pkill ollama && ollama serve
```

### Q13: GPU未使用？

**A**: GPU启用檢查：
```bash
# 1. 檢查GPU驅動程式
nvidia-smi

# 2. 檢查CUDA
nvidia-cuda-version-control

# 3. 强制使用GPU
export OLLAMA_GPU=1

# 4. 重新安裝（如果需要）
# 移除
sudo rm -rf /usr/local/bin/ollama
# 重新安裝（会自動检测GPU）
curl -fsSL https://ollama.ai/install.sh | sh
```

---

## 🔧 功能問題 / Functional Issues

### Q14: 工具調用不工作？

**A**: 檢查和修復：
```bash
# 1. 確認模型支持工具調用
ollama show qwen2.5-coder:7b | grep "tool"

# 2. 使用支持工具的模型
# 推荐模型：
# - qwen2.5-coder:7b
# - qwen2.5-coder:14b  
# - deepseek-coder:6.7b

# 配置
grep -A 10 '"tools"' ~/.config/opencode/opencode.json
```

### Q15: 檔案操作权限错误？

**A**: 权限設置：
```bash
# 1. 檢查檔案权限
ls -la /path/to/your/files

# 2. 設置正确权限
chmod -R 755 /path/to/your/files
chown -R $USER:$USER /path/to/your/files

# 配置
# 配置
{
  "permissions": {
    "fileSystem": {
      "allowedPaths": ["./", "/absolute/path"],
      "umask": "022"
    }
  }
}
```

### Q16: 上下文視窗不足？

**A**: 擴展上下文：
```bash
# 1. 創建大上下文模型变体
ollama run qwen2.5-coder:7b
/set parameter num_ctx 16384
/save qwen2.5-coder:7b-16k

# 配置
{
  "model": "ollama/qwen2.5-coder:7b-16k"
}

# 3. 分块處理大檔案
# 将大檔案分成小块分别處理
```

---

## 🌐 網路連接問題 / Network Connection Issues

### Q17: 無法連接Ollama API？

**A**: 網路诊断：
```bash
# 1. 檢查服务運行
curl -v http://localhost:11434/api/tags

# 2. 檢查端口占用
netstat -tlnp | grep 11434
lsof -i :11434

# 3. 檢查防火墙
sudo ufw status  # Ubuntu
sudo firewall-cmd --list-all  # CentOS

# 4. 更改端口（如果需要）
export OLLAMA_HOST=0.0.0.0:11435
ollama serve
```

### Q18: 代理設置問題？

# 配置
```bash
# 1. 設置HTTP代理
export http_proxy=http://proxy.server:port
export https_proxy=http://proxy.server:port

# 2. Git代理
git config --global http.proxy http://proxy.server:port
git config --global https.proxy http://proxy.server:port

# 3. Ollama代理
export OLLAMA_PROXY=http://proxy.server:port

# 4. Node.js代理
npm config set proxy http://proxy.server:port
npm config set https-proxy http://proxy.server:port
```

---

## 🐛 错误代碼详解 / Error Code Explanation

### 错误代碼 1001: 連接超时
**原因**: OpenCode無法連接到Ollama服务
**解決**: 
1. 確認Ollama正在運行
2. 檢查端口11434是否被占用
3. 檢查防火墙設置

### 错误代碼 1002: 模型未找到
**原因**: 指定的模型不存在或未下載
**解決**:
```bash
# 查看
ollama pull model_name  # 下載模型
```

### 错误代碼 1003: 記憶體不足
**原因**: 系統記憶體不足以運行模型
**解決**:
1. 使用更小的模型
2. 增加虚拟記憶體
3. 關閉其他程式释放記憶體

# 配置
**原因**: opencode.json格式或內容错误
**解決**:
1. 使用JSON驗證工具檢查格式
# 配置
# 配置

---

## 🔧 高级問題 / Advanced Issues

### Q19: 如何設置多使用者使用？

# 配置
```bash
# 1. 系統级安裝Ollama
sudo curl -fsSL https://ollama.ai/install.sh | sh
sudo usermod -a -G ollama $USER

# 2. 共享模型目錄
sudo mkdir -p /opt/ollama/models
sudo chown -R ollama:ollama /opt/ollama
sudo chmod -R 755 /opt/ollama

# 配置
export OLLAMA_MODELS=/opt/ollama/models
```

# 配置

# 配置
```dockerfile
# Dockerfile
FROM nvidia/cuda:12.1-devel-ubuntu22.04

RUN apt-get update && apt-get install -y curl git
RUN curl -fsSL https://ollama.ai/install.sh | sh
RUN ollama serve &

# 構建
docker build -t opencode-ollama .

# 運行
docker run --gpus all -p 11434:11434 opencode-ollama
```

### Q21: 如何監控系統效能？

**A**: 監控腳本：
```bash
#!/bin/bash
# monitor.sh
while true; do
    echo "$(date): GPU=$(nvidia-smi --query-gpu=utilization.gpu --format=csv,noheader,nounits)% RAM=$(free | grep Mem | awk '{printf "%.1f%%", $3/$2 * 100.0}')"
    sleep 30
done
```

---

## 📚 學習資源 / Learning Resources

### 官方文檔
- **OpenCode文檔**: https://opencode.ai/docs
- **Ollama文檔**: https://github.com/ollama/ollama/blob/main/README.md
- **模型库**: https://ollama.ai/library

### 社區資源
- **Discord社區**: https://opencode.ai/discord
- **GitHub讨论**: https://github.com/ollama/ollama/discussions
- **Reddit**: https://reddit.com/r/LocalLLaMA

# 教程
# 教程
- **博客文章**: Medium、Dev.to上的相關文章
# 配置

---

## 📞 获取更多帮助 / Getting More Help

### 線上支持
- **GitHub Issues**: https://github.com/anomalyco/opencode/issues
- **官方論壇**: https://opencode.ai/community
- **Stack Overflow**: 標籤 `opencode` `ollama`

### 報告問題时请包含
# 配置
# 版本
3. **错误資訊**: 完整的错误日志
4. **重现步骤**: 詳細的操作步骤
# 配置

---

> [!tip] 💡 预防措施 / Preventive Measures
# 配置
> - 儲存重要日志資訊
# 更新
> - 参与社區讨论获取最新資訊

---

# 更新