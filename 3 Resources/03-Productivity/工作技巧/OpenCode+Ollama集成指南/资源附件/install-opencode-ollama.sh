#!/bin/bash
# OpenCode + Ollama ä¸€é”®å®‰è£…è„šæœ¬
# OpenCode + Ollama One-Click Installation Script
# 
# æ­¤è„šæœ¬å°†è‡ªåŠ¨å®‰è£…å’Œé…ç½®å®Œæ•´çš„æœ¬åœ°AIç¼–ç¨‹ç¯å¢ƒ
# This script automatically installs and configures a complete local AI programming environment
#
# ä½œè€…: OpenCodeæœ¬åœ°æ¨¡å‹é›†æˆé¡¹ç›®
# ç‰ˆæœ¬: 1.0.0
# æ—¥æœŸ: 2026-01-15

set -e  # é‡åˆ°é”™è¯¯æ—¶é€€å‡º

# ======== é¢œè‰²å’Œæ ·å¼å®šä¹‰ / Color and Style Definitions ========
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
PURPLE='\033[0;35m'
CYAN='\033[0;36m'
NC='\033[0m' # No Color

BOLD='\033[1m'
UNDERLINE='\033[4m'

# ======== æ‰“å°å‡½æ•° / Print Functions ========
print_header() {
    echo -e "${BLUE}${BOLD}ğŸ¦™ OpenCode + Ollama ä¸€é”®å®‰è£…è„šæœ¬${NC}"
    echo -e "${BLUE}======================================${NC}"
    echo ""
}

print_step() {
    echo -e "${CYAN}ğŸ“ æ­¥éª¤ $1: $2${NC}"
}

print_success() {
    echo -e "${GREEN}âœ… $1${NC}"
}

print_warning() {
    echo -e "${YELLOW}âš ï¸  $1${NC}"
}

print_error() {
    echo -e "${RED}âŒ $1${NC}"
}

print_info() {
    echo -e "${PURPLE}â„¹ï¸  $1${NC}"
}

# ======== ç³»ç»Ÿæ£€æµ‹ / System Detection ========
detect_system() {
    print_step 1 "ç³»ç»Ÿæ£€æµ‹" "Detecting System Information"
    
    # æ“ä½œç³»ç»Ÿæ£€æµ‹
    if [[ "$OSTYPE" == "linux-gnu"* ]]; then
        OS="linux"
        DISTRO=$(lsb_release -si 2>/dev/null || echo "Unknown")
    elif [[ "$OSTYPE" == "darwin"* ]]; then
        OS="macos"
        DISTRO=$(sw_vers -productVersion)
    elif [[ "$OSTYPE" == "msys" ]] || [[ "$OSTYPE" == "cygwin" ]]; then
        OS="windows"
        DISTRO="Windows"
    else
        print_error "ä¸æ”¯æŒçš„æ“ä½œç³»ç»Ÿ: $OSTYPE"
        exit 1
    fi
    
    # æ¶æ„æ£€æµ‹
    ARCH=$(uname -m)
    
    # å†…å­˜æ£€æµ‹
    if [[ "$OS" == "linux" ]]; then
        TOTAL_MEM=$(free -g | awk '/^Mem:/{print $2}')
    elif [[ "$OS" == "macos" ]]; then
        TOTAL_MEM=$(sysctl -n hw.memsize | awk '{print int($1/1024/1024/1024)}')
    else
        TOTAL_MEM=16  # é»˜è®¤å€¼
    fi
    
    # GPUæ£€æµ‹
    if command -v nvidia-smi &> /dev/null; then
        GPU_MEMORY=$(nvidia-smi --query-gpu=memory.total --format=csv,noheader,nounits 2>/dev/null | head -1)
        HAS_GPU=true
    else
        GPU_MEMORY=0
        HAS_GPU=false
    fi
    
    # æ˜¾ç¤ºç³»ç»Ÿä¿¡æ¯
    echo "æ“ä½œç³»ç»Ÿ: $OS ($DISTRO)"
    echo "æ¶æ„: $ARCH"
    echo "æ€»å†…å­˜: ${TOTAL_MEM}GB"
    echo "GPUå†…å­˜: ${GPU_MEMORY}GB"
    echo "GPUå¯ç”¨: $HAS_GPU"
    echo ""
    
    # ç¡¬ä»¶å…¼å®¹æ€§è¯„ä¼°
    if [[ $TOTAL_MEM -lt 8 ]]; then
        print_warning "ç³»ç»Ÿå†…å­˜ä¸è¶³8GBï¼Œå¯èƒ½å½±å“æ€§èƒ½"
    fi
    
    if [[ $HAS_GPU == false ]]; then
        print_warning "æœªæ£€æµ‹åˆ°GPUï¼Œå°†ä½¿ç”¨CPUæ¨¡å¼ï¼ˆé€Ÿåº¦è¾ƒæ…¢ï¼‰"
    fi
    
    print_success "ç³»ç»Ÿæ£€æµ‹å®Œæˆ"
}

# ======== æ£€æŸ¥ä¾èµ– / Check Dependencies ========
check_dependencies() {
    print_step 2 "ä¾èµ–æ£€æŸ¥" "Checking Dependencies"
    
    local missing_deps=()
    
    # æ£€æŸ¥åŸºç¡€å·¥å…·
    for cmd in curl git wget; do
        if ! command -v $cmd &> /dev/null; then
            missing_deps+=($cmd)
        fi
    done
    
    # æ£€æŸ¥Node.js
    if ! command -v node &> /dev/null; then
        missing_deps+=("node")
    else
        NODE_VERSION=$(node --version | cut -d'v' -f2)
        REQUIRED_NODE="18.0.0"
        if ! node -e "process.exit(require('semver').gte('$NODE_VERSION', '$REQUIRED_NODE') ? 0 : 1)" 2>/dev/null; then
            print_warning "Node.jsç‰ˆæœ¬è¿‡ä½ ($NODE_VERSION)ï¼Œå»ºè®®å‡çº§åˆ°v18+"
            missing_deps+=("node-upgrade")
        fi
    fi
    
    # æ£€æŸ¥Pythonï¼ˆæŸäº›æ¨¡å‹éœ€è¦ï¼‰
    if ! command -v python3 &> /dev/null; then
        print_info "æœªå®‰è£…Python3ï¼ŒæŸäº›æ¨¡å‹å¯èƒ½éœ€è¦"
    fi
    
    # æŠ¥å‘Šç¼ºå¤±ä¾èµ–
    if [[ ${#missing_deps[@]} -gt 0 ]]; then
        print_error "ç¼ºå°‘ä»¥ä¸‹ä¾èµ–: ${missing_deps[*]}"
        print_info "è¯·å…ˆå®‰è£…ç¼ºå¤±çš„ä¾èµ–ï¼Œç„¶åé‡æ–°è¿è¡Œæ­¤è„šæœ¬"
        exit 1
    fi
    
    print_success "æ‰€æœ‰ä¾èµ–å·²æ»¡è¶³"
}

# ======== å®‰è£…OpenCode / Install OpenCode ========
install_opencode() {
    print_step 3 "å®‰è£…OpenCode" "Installing OpenCode"
    
    if command -v opencode &> /dev/null; then
        OPENCODE_VERSION=$(opencode --version 2>/dev/null | head -1)
        print_success "OpenCodeå·²å®‰è£… (ç‰ˆæœ¬: $OPENCODE_VERSION)"
        read -p "æ˜¯å¦è¦æ›´æ–°åˆ°æœ€æ–°ç‰ˆæœ¬? (y/N): " -n 1 -r
        echo
        if [[ ! $REPLY =~ ^[Yy]$ ]]; then
            return
        fi
    fi
    
    print_info "æ­£åœ¨å®‰è£…OpenCode..."
    
    # æ–¹æ³•1: å®˜æ–¹å®‰è£…è„šæœ¬ï¼ˆæ¨èï¼‰
    if curl -fsSL https://opencode.ai/install | bash; then
        print_success "OpenCodeå®‰è£…æˆåŠŸ"
    else
        print_warning "å®˜æ–¹å®‰è£…å¤±è´¥ï¼Œå°è¯•npmå®‰è£…..."
        
        # æ–¹æ³•2: npmå®‰è£…
        if npm install -g @opencode-ai/cli; then
            print_success "OpenCodeé€šè¿‡npmå®‰è£…æˆåŠŸ"
        else
            print_error "OpenCodeå®‰è£…å¤±è´¥"
            exit 1
        fi
    fi
    
    # éªŒè¯å®‰è£…
    if command -v opencode &> /dev/null; then
        OPENCODE_VERSION=$(opencode --version 2>/dev/null | head -1)
        print_success "OpenCodeéªŒè¯æˆåŠŸ (ç‰ˆæœ¬: $OPENCODE_VERSION)"
    else
        print_error "OpenCodeå®‰è£…éªŒè¯å¤±è´¥"
        exit 1
    fi
}

# ======== å®‰è£…Ollama / Install Ollama ========
install_ollama() {
    print_step 4 "å®‰è£…Ollama" "Installing Ollama"
    
    if command -v ollama &> /dev/null; then
        OLLAMA_VERSION=$(ollama --version 2>/dev/null)
        print_success "Ollamaå·²å®‰è£… (ç‰ˆæœ¬: $OLLAMA_VERSION)"
        read -p "æ˜¯å¦è¦æ›´æ–°åˆ°æœ€æ–°ç‰ˆæœ¬? (y/N): " -n 1 -r
        echo
        if [[ ! $REPLY =~ ^[Yy]$ ]]; then
            return
        fi
    fi
    
    print_info "æ­£åœ¨å®‰è£…Ollama..."
    
    # æ ¹æ®æ“ä½œç³»ç»Ÿé€‰æ‹©å®‰è£…æ–¹å¼
    if [[ "$OS" == "linux" ]] || [[ "$OS" == "macos" ]]; then
        if curl -fsSL https://ollama.ai/install.sh | sh; then
            print_success "Ollamaå®‰è£…æˆåŠŸ"
        else
            print_error "Ollamaå®‰è£…å¤±è´¥"
            exit 1
        fi
    elif [[ "$OS" == "windows" ]]; then
        print_info "Windowsç”¨æˆ·è¯·æ‰‹åŠ¨å®‰è£…Ollama:"
        print_info "1. è®¿é—® https://ollama.ai/download"
        print_info "2. ä¸‹è½½Windowsç‰ˆæœ¬"
        print_info "3. è¿è¡Œå®‰è£…ç¨‹åº"
        
        read -p "å®‰è£…å®ŒæˆåæŒ‰å›è½¦ç»§ç»­..." -r
        echo
        
        if ! command -v ollama &> /dev/null; then
            print_error "Ollamaæœªæ­£ç¡®å®‰è£…"
            exit 1
        fi
    fi
    
    # éªŒè¯å®‰è£…
    OLLAMA_VERSION=$(ollama --version 2>/dev/null)
    print_success "OllamaéªŒè¯æˆåŠŸ (ç‰ˆæœ¬: $OLLAMA_VERSION)"
}

# ======== é…ç½®Ollama / Configure Ollama ========
configure_ollama() {
    print_step 5 "é…ç½®Ollama" "Configuring Ollama"
    
    # åˆ›å»ºé…ç½®ç›®å½•
    OLLAMA_CONFIG_DIR="$HOME/.ollama"
    mkdir -p "$OLLAMA_CONFIG_DIR"
    
    # è®¾ç½®ç¯å¢ƒå˜é‡
    print_info "é…ç½®Ollamaç¯å¢ƒå˜é‡..."
    
    cat >> "$HOME/.bashrc" << EOF

# OpenCode + Ollama ç¯å¢ƒå˜é‡
export OLLAMA_HOST=0.0.0.0:11434
export OLLAMA_ORIGINS=*
export OLLAMA_KEEP_ALIVE=24h
export OLLAMA_MAX_LOADED_MODELS=2

# æ€§èƒ½ä¼˜åŒ–
if [ -n "\$GPU_MEMORY" ]; then
    export OLLAMA_GPU_MEMORY_FRACTION=0.8
fi
export OLLAMA_NUM_PARALLEL=4
EOF
    
    # æ ¹æ®shellç±»å‹æ›´æ–°é…ç½®æ–‡ä»¶
    if [[ "$SHELL" == *"zsh"* ]]; then
        sed 's/\.bashrc/\.zshrc/g' "$HOME/.bashrc" > "$HOME/.zshrc.tmp"
        mv "$HOME/.zshrc.tmp" "$HOME/.zshrc"
    fi
    
    print_success "Ollamaç¯å¢ƒå˜é‡é…ç½®å®Œæˆ"
}

# ======== é€‰æ‹©å’Œä¸‹è½½æ¨¡å‹ / Select and Download Models ========
download_models() {
    print_step 6 "æ¨¡å‹é€‰æ‹©ä¸ä¸‹è½½" "Model Selection and Download"
    
    # æ¨èæ¨¡å‹åˆ—è¡¨
    local models=(
        "qwen2.5-coder:7b:Qwen2.5-Coder 7B (æ¨è):ç¼–ç¨‹ä¸“ç”¨:8GB:å¹³è¡¡"
        "qwen2.5-coder:14b:Qwen2.5-Coder 14B (é«˜æ€§èƒ½):ç¼–ç¨‹ä¸“ç”¨:16GB:é«˜æ€§èƒ½"
        "qwen2.5:7b:Qwen2.5 7B (é€šç”¨):é€šç”¨å¯¹è¯:8GB:å¹³è¡¡"
        "qwen2.5:3b:Qwen2.5 3B (è½»é‡):åŸºç¡€ä»»åŠ¡:4GB:å¿«é€Ÿ"
        "mistral-nemo:12b:Mistral-Nemo 12B (æ¨ç†):å¼ºæ¨ç†èƒ½åŠ›:12GB:åˆ†æ"
    )
    
    # æ ¹æ®ç¡¬ä»¶è¿‡æ»¤æ¨¡å‹
    local available_models=()
    for model_info in "${models[@]}"; do
        IFS=':' read -r model_id model_name model_desc gpu_req perf <<< "$model_info"
        
        if [[ $gpu_req -le $GPU_MEMORY ]] || [[ $gpu_req -eq 0 ]]; then
            available_models+=("$model_id:$model_name:$model_desc:$gpu_req:$perf")
        fi
    done
    
    # æ˜¾ç¤ºå¯é€‰æ¨¡å‹
    echo "å¯ç”¨æ¨¡å‹ï¼ˆåŸºäºæ‚¨çš„ç¡¬ä»¶é…ç½®ï¼‰:"
    echo ""
    
    local i=1
    for model_info in "${available_models[@]}"; do
        IFS=':' read -r model_id model_name model_desc gpu_req perf <<< "$model_info"
        echo "$i) $model_name"
        echo "   æè¿°: $model_desc"
        echo "   GPUéœ€æ±‚: ${gpu_req}GB"
        echo "   æ€§èƒ½: $perf"
        echo ""
        ((i++))
    done
    
    # ç”¨æˆ·é€‰æ‹©
    read -p "è¯·é€‰æ‹©è¦ä¸‹è½½çš„æ¨¡å‹ (1-${i}): " -n 1 -r
    echo
    
    if [[ $REPLY -ge 1 ]] && [[ $REPLY -lt $i ]]; then
        selected_model_info="${available_models[$((REPLY-1))]}"
        IFS=':' read -r selected_model_id selected_model_name model_desc gpu_req perf <<< "$selected_model_info"
        
        print_info "é€‰æ‹©æ¨¡å‹: $selected_model_name"
        print_info "å¼€å§‹ä¸‹è½½..."
        
        # ä¸‹è½½æ¨¡å‹
        if ollama pull "$selected_model_id"; then
            print_success "æ¨¡å‹ä¸‹è½½æˆåŠŸ"
            SELECTED_MODEL="$selected_model_id"
            SELECTED_MODEL_NAME="$selected_model_name"
        else
            print_error "æ¨¡å‹ä¸‹è½½å¤±è´¥"
            exit 1
        fi
    else
        print_error "æ— æ•ˆé€‰æ‹©"
        exit 1
    fi
}

# ======== é…ç½®OpenCode / Configure OpenCode ========
configure_opencode() {
    print_step 7 "é…ç½®OpenCode" "Configuring OpenCode"
    
    # åˆ›å»ºé…ç½®ç›®å½•
    OPENCODE_CONFIG_DIR="$HOME/.config/opencode"
    mkdir -p "$OPENCODE_CONFIG_DIR"
    
    # ç”Ÿæˆé…ç½®æ–‡ä»¶
    local config_file="$OPENCODE_CONFIG_DIR/opencode.json"
    
    cat > "$config_file" << EOF
{
  "\$schema": "https://opencode.ai/config.json",
  "model": "ollama/$SELECTED_MODEL",
  "provider": {
    "ollama": {
      "npm": "@ai-sdk/openai-compatible",
      "name": "Ollama (Local)",
      "options": {
        "baseURL": "http://localhost:11434/v1",
        "timeout": 120000,
        "maxRetries": 3,
        "headers": {
          "Connection": "keep-alive"
        }
      },
      "models": {
        "$SELECTED_MODEL": {
          "name": "$SELECTED_MODEL_NAME (Local)",
          "options": {
            "temperature": $([ "$perf" == "é«˜æ€§èƒ½" ] && echo "0.05" || echo "0.1"),
            "top_p": 0.9,
            "extraBody": {
              "num_ctx": 8192,
              "num_batch": 512,
              "repeat_penalty": 1.1
            }
          },
          "limit": {
            "context": 8192,
            "output": 4096
          }
        }
      }
    }
  },
  "tools": {
    "timeout": 60000,
    "maxParallel": 3
  }
}
EOF
    
    print_success "OpenCodeé…ç½®æ–‡ä»¶å·²ç”Ÿæˆ: $config_file"
}

# ======== å¯åŠ¨æœåŠ¡ / Start Services ========
start_services() {
    print_step 8 "å¯åŠ¨æœåŠ¡" "Starting Services"
    
    # å¯åŠ¨OllamaæœåŠ¡
    print_info "å¯åŠ¨OllamaæœåŠ¡..."
    ollama serve &
    OLLAMA_PID=$!
    
    # ç­‰å¾…æœåŠ¡å¯åŠ¨
    print_info "ç­‰å¾…OllamaæœåŠ¡å¯åŠ¨..."
    for i in {1..30}; do
        if curl -s http://localhost:11434/api/tags > /dev/null 2>&1; then
            print_success "OllamaæœåŠ¡å¯åŠ¨æˆåŠŸ"
            break
        fi
        sleep 1
    done
    
    if ! curl -s http://localhost:11434/api/tags > /dev/null 2>&1; then
        print_error "OllamaæœåŠ¡å¯åŠ¨å¤±è´¥"
        kill $OLLAMA_PID 2>/dev/null
        exit 1
    fi
    
    # éªŒè¯æ¨¡å‹
    print_info "éªŒè¯æ¨¡å‹å¯ç”¨æ€§..."
    if ollama list | grep -q "$SELECTED_MODEL"; then
        print_success "æ¨¡å‹éªŒè¯æˆåŠŸ"
    else
        print_error "æ¨¡å‹ä¸å¯ç”¨"
        kill $OLLAMA_PID 2>/dev/null
        exit 1
    fi
}

# ======== æµ‹è¯•é›†æˆ / Test Integration ========
test_integration() {
    print_step 9 "æµ‹è¯•é›†æˆ" "Testing Integration"
    
    print_info "æ‰§è¡ŒOpenCodeæµ‹è¯•..."
    
    # åˆ›å»ºæµ‹è¯•è„šæœ¬
    cat > /tmp/opencode_test.sh << 'EOF'
#!/bin/bash
echo "ğŸ§ª OpenCodeé›†æˆæµ‹è¯•"
echo "====================="

# æµ‹è¯•å‘½ä»¤
test_command="åˆ›å»ºä¸€ä¸ªPythonå‡½æ•°ï¼Œè®¡ç®—æ–æ³¢é‚£å¥‘æ•°åˆ—"
echo "æµ‹è¯•å‘½ä»¤: $test_command"
echo ""

# æ‰§è¡Œæµ‹è¯•
if timeout 60 opencode run "$test_command" --model ollama/$SELECTED_MODEL > /tmp/test_output.txt 2>&1; then
    echo "âœ… æµ‹è¯•æˆåŠŸ"
    echo ""
    echo "è¾“å‡ºå†…å®¹:"
    cat /tmp/test_output.txt
else
    echo "âŒ æµ‹è¯•å¤±è´¥æˆ–è¶…æ—¶"
    if [ -f /tmp/test_output.txt ]; then
        echo "é”™è¯¯ä¿¡æ¯:"
        cat /tmp/test_output.txt
    fi
fi
EOF
    
    chmod +x /tmp/opencode_test.sh
    /tmp/opencode_test.sh
    
    # æ¸…ç†
    rm -f /tmp/opencode_test.sh /tmp/test_output.txt
}

# ======== å®Œæˆæ€»ç»“ / Completion Summary ========
completion_summary() {
    print_step 10 "å®‰è£…å®Œæˆ" "Installation Complete"
    
    echo ""
    echo -e "${GREEN}${BOLD}ğŸ‰ OpenCode + Ollama å®‰è£…æˆåŠŸï¼${NC}"
    echo ""
    echo -e "${BLUE}å®‰è£…ä¿¡æ¯æ€»ç»“:${NC}"
    echo "=================="
    echo "å®‰è£…æ¨¡å‹: $SELECTED_MODEL_NAME"
    echo "æ¨¡å‹ID: $SELECTED_MODEL"
    echo "é…ç½®æ–‡ä»¶: $HOME/.config/opencode/opencode.json"
    echo "OllamaæœåŠ¡: http://localhost:11434"
    echo ""
    
    echo -e "${CYAN}åç»­ä½¿ç”¨æ­¥éª¤:${NC}"
    echo "==============="
    echo "1. é‡æ–°åŠ è½½shellé…ç½®:"
    echo "   source ~/.bashrc  # æˆ– source ~/.zshrc"
    echo ""
    echo "2. å¯åŠ¨OllamaæœåŠ¡ï¼ˆå¦‚æœæœªå¯åŠ¨ï¼‰:"
    echo "   ollama serve"
    echo ""
    echo "3. å¯åŠ¨OpenCode:"
    echo "   opencode"
    echo ""
    echo "4. é€‰æ‹©æœ¬åœ°æ¨¡å‹:"
    echo "   /models"
    echo ""
    echo "5. å¼€å§‹ä½¿ç”¨:"
    echo "   è¾“å…¥æ‚¨çš„ç¼–ç¨‹éœ€æ±‚"
    echo ""
    
    echo -e "${YELLOW}é‡è¦æé†’:${NC}"
    echo "============="
    echo "- OllamaæœåŠ¡å·²åå°è¿è¡Œ (PID: $OLLAMA_PID)"
    echo "- é…ç½®æ–‡ä»¶ä½ç½®: $HOME/.config/opencode/opencode.json"
    echo "- å¦‚éœ€åœæ­¢Ollama: kill $OLLAMA_PID"
    echo "- å¦‚éœ€é‡å¯Ollama: ollama serve"
    echo ""
    
    echo -e "${PURPLE}è·å–å¸®åŠ©:${NC}"
    echo "==========="
    echo "- å®˜æ–¹æ–‡æ¡£: https://opencode.ai/docs"
    echo "- ç¤¾åŒºæ”¯æŒ: https://opencode.ai/discord"
    echo "- æ•…éšœæ’é™¤: æŸ¥çœ‹é…ç½®æ–‡ä»¶ä¸­çš„æ—¥å¿—"
    echo ""
}

# ======== ä¸»ç¨‹åº / Main Program ========
main() {
    print_header
    
    # æ£€æŸ¥æ˜¯å¦ä»¥rootè¿è¡Œ
    if [[ $EUID -eq 0 ]]; then
        print_warning "ä¸å»ºè®®ä»¥rootç”¨æˆ·è¿è¡Œæ­¤è„šæœ¬"
        read -p "æ˜¯å¦ç»§ç»­? (y/N): " -n 1 -r
        echo
        if [[ ! $REPLY =~ ^[Yy]$ ]]; then
            exit 1
        fi
    fi
    
    # æ‰§è¡Œå®‰è£…æ­¥éª¤
    detect_system
    check_dependencies
    install_opencode
    install_ollama
    configure_ollama
    download_models
    configure_opencode
    start_services
    test_integration
    completion_summary
    
    print_success "å®‰è£…è„šæœ¬æ‰§è¡Œå®Œæˆï¼"
}

# ======== é”™è¯¯å¤„ç† / Error Handling ========
trap 'print_error "è„šæœ¬æ‰§è¡Œä¸­æ–­ï¼Œæ­£åœ¨æ¸…ç†..."; kill $OLLAMA_PID 2>/dev/null; exit 1' INT TERM

# è¿è¡Œä¸»ç¨‹åº
main "$@"