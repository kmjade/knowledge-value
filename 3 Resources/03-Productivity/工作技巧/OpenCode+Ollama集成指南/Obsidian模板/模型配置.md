---
tags:
  - opencode
  - ollama
  - model-config
  - setup
  - guide
created: <% tp.file.creation_date() %>
---

# 🔧 模型配置向导 / Model Configuration Wizard

## 🎯 配置目标 / Configuration Goals

> [!info] 💡 配置向导说明 / Configuration Wizard Info
> 这个向导将帮助您：
> 1. **评估硬件配置** - 确定最优模型选择
> 2. **配置OpenCode** - 设置本地模型集成
> 3. **验证配置** - 测试集成是否正常工作
> 4. **优化性能** - 根据硬件调优参数

---

## 🖥️ 硬件评估 / Hardware Assessment

### 自动硬件检测

<%*
// 检测系统硬件信息
const hardwareInfo = {
    platform: process.platform,
    arch: process.arch,
    
    // GPU检测（简化版）
    hasGPU: false, // 实际环境中可以通过命令检测
    gpuMemory: 0,
    
    // 内存检测（模拟）
    totalMemory: 16, // 默认值，实际环境中可以获取真实值
    availableMemory: 8,
    
    // 存储检测
    availableStorage: 100 // GB
};

// 尝试获取更准确的硬件信息
try {
    // 在实际环境中，这里会执行系统命令
    // const gpuInfo = require('child_process').execSync('nvidia-smi --query-gpu=memory.total --format=csv,noheader,nounits').toString();
    // hardwareInfo.gpuMemory = parseInt(gpuInfo);
    // hardwareInfo.hasGPU = hardwareInfo.gpuMemory > 0;
} catch (error) {
    console.log('GPU检测失败，使用默认值');
}
-%>

### 当前系统信息

| 硬件组件 / Hardware Component | 检测结果 / Detected Result | 状态 / Status |
|--------------------------|-------------------------|-------------|
| **操作系统 / OS** | `<% hardwareInfo.platform %>` | ✅ 已检测 |
| **架构 / Architecture** | `<% hardwareInfo.arch %>` | ✅ 已检测 |
| **GPU支持 / GPU Support** | `<% hardwareInfo.hasGPU ? '✅ 可用' : '❌ 未检测到' %>` | `<% hardwareInfo.hasGPU ? '✅ 优秀' : '⚠️ 需CPU模式' %>` |
| **GPU内存 / GPU Memory** | `<% hardwareInfo.gpuMemory %>GB` | `<% hardwareInfo.gpuMemory >= 8 ? '✅ 充足' : hardwareInfo.gpuMemory > 0 ? '⚠️ 可能不足' : '❌ 无GPU' %>` |
| **系统内存 / System RAM** | `<% hardwareInfo.totalMemory %>GB` | `<% hardwareInfo.totalMemory >= 16 ? '✅ 充足' : '⚠️ 建议升级' %>` |
| **可用存储 / Available Storage** | `<% hardwareInfo.availableStorage %>GB` | ✅ 充足 |

### 硬件兼容性评估

<%*
// 根据硬件评估兼容性
let hardwareScore = 0;
let recommendations = [];

if (hardwareInfo.hasGPU) {
    hardwareScore += 30;
    if (hardwareInfo.gpuMemory >= 8) hardwareScore += 20;
    if (hardwareInfo.gpuMemory >= 16) hardwareScore += 20;
} else {
    recommendations.push('考虑添加GPU以提升性能');
}

if (hardwareInfo.totalMemory >= 16) {
    hardwareScore += 15;
} else if (hardwareInfo.totalMemory >= 8) {
    hardwareScore += 10;
} else {
    recommendations.push('建议升级系统内存到至少16GB');
}

if (hardwareInfo.availableStorage >= 50) {
    hardwareScore += 10;
}

// 评估等级
let performanceLevel = '';
if (hardwareScore >= 70) {
    performanceLevel = '🏆 高性能 - 适合所有本地模型';
} else if (hardwareScore >= 50) {
    performanceLevel = '⭐ 中等性能 - 适合主流模型';
} else if (hardwareScore >= 30) {
    performanceLevel = '📱 基础性能 - 适合轻量模型';
} else {
    performanceLevel = '⚠️ 性能不足 - 建议硬件升级';
}
-%>

| 评估项 / Assessment | 得分 / Score | 评级 / Grade |
|------------------|-------------|-------------|
| **硬件总分 / Hardware Score** | `<% hardwareScore %>/100` | `<% performanceLevel %>` |

### 🎯 优化建议

<%*
if (recommendations.length > 0) {
-%>
<details>
<summary>📋 硬件优化建议 / Hardware Optimization Recommendations</summary>

<% recommendations.forEach(rec => { -%>
- 🔧 <% rec %>
<% }); -%>

</details>

<%
} else {
-%>
> [!success] 🎉 硬件配置优秀 / Excellent Hardware Configuration
> 您的硬件配置完全满足本地模型运行需求！

<%
}
-%>

---

## 🤖 模型推荐 / Model Recommendations

### 智能模型选择

<%*
// 根据硬件推荐模型
const recommendedModels = [];
const modelDetails = {
    'qwen2.5-coder:3b': {
        name: 'Qwen2.5-Coder 3B',
        memory: 6, gpu: 4, ram: 8, performance: '基础',
        useCase: '轻量级编程，快速响应',
        pros: ['启动快', '内存占用低', '适合CPU模式'],
        cons: ['复杂任务能力有限']
    },
    'qwen2.5-coder:7b': {
        name: 'Qwen2.5-Coder 7B',
        memory: 14, gpu: 8, ram: 16, performance: '平衡',
        useCase: '通用编程，推荐选择',
        pros: ['性能与资源平衡', '工具支持', '代码质量高'],
        cons: ['需要8GB+ GPU']
    },
    'qwen2.5-coder:14b': {
        name: 'Qwen2.5-Coder 14B',
        memory: 28, gpu: 16, ram: 32, performance: '高性能',
        useCase: '复杂编程任务，专业开发',
        pros: ['强大的推理能力', '优秀的代码质量', '支持复杂项目'],
        cons: ['需要强大硬件', '响应速度较慢']
    },
    'qwen2.5:7b': {
        name: 'Qwen2.5 7B',
        memory: 14, gpu: 8, ram: 16, performance: '通用',
        useCase: '通用对话和基础编程',
        pros: ['平衡的性能', '多语言支持', '上下文大'],
        cons: ['编程能力不如专用模型']
    },
    'mistral-nemo:12b': {
        name: 'Mistral-Nemo 12B',
        memory: 24, gpu: 12, ram: 24, performance: '推理强',
        useCase: '复杂推理，代码审查',
        pros: ['优秀的推理能力', '128K上下文', '分析能力强'],
        cons: ['编程专用能力较弱', '资源需求高']
    }
};

// 筛选合适的模型
Object.entries(modelDetails).forEach(([modelId, details]) => {
    if (details.gpu <= (hardwareInfo.gpuMemory || 8) && 
        details.ram <= (hardwareInfo.totalMemory || 16)) {
        recommendedModels.push({ id: modelId, ...details });
    }
});

// 如果没有合适的模型，添加最基础的选项
if (recommendedModels.length === 0) {
    recommendedModels.push({
        id: 'qwen2.5:3b',
        name: 'Qwen2.5 3B',
        memory: 6,
        gpu: 4,
        ram: 8,
        performance: '基础',
        useCase: '最低配置要求',
        pros: ['可在CPU运行', '内存需求低'],
        cons: ['功能有限']
    });
}
-%>

### 📊 推荐模型列表

| 模型 / Model | GPU内存 / GPU VRAM | 系统内存 / RAM | 性能等级 / Level | 适用场景 / Use Case |
|-------------|-------------------|----------------|-------------------|-------------------|
<%*
recommendedModels.forEach(model => {
-%>
| **<% model.name %>** | `<% model.gpu %>GB+` | `<% model.ram %>GB+` | `<% model.performance %>` | `<% model.useCase %>` |
<%
});
-%>

### 🎯 详细模型对比

<%*
// 选择模型进行详细对比
const modelOptions = recommendedModels.map(m => m.name);
const selectedModelName = await tp.system.suggester(
    modelOptions,
    recommendedModels.map(m => m.id),
    false,
    '选择要详细了解的模型 / Select model to details:'
);

const selectedModel = recommendedModels.find(m => m.id === selectedModelName) || recommendedModels[0];
-%>

#### 🏆 选中模型: <% selectedModel.name %>

> [!info] 模型详细信息 / Model Details
> 
> **基本信息 / Basic Info**:
> - **模型ID**: `<% selectedModel.id %>`
> - **性能等级**: `<% selectedModel.performance %>`
> - **推荐用途**: `<% selectedModel.useCase %>`
> 
> **硬件需求 / Hardware Requirements**:
> - **GPU内存**: `<% selectedModel.gpu %>GB`
> - **系统内存**: `<% selectedModel.ram %>GB`
> - **存储空间**: `<% Math.ceil(selectedModel.memory / 2) %>GB` (模型文件)
> 
> **优势 / Pros**:
<% selectedModel.pros.forEach(pro => { -%>
> - ✅ <% pro %>
<% }); -%>
> 
> **限制 / Cons**:
<% selectedModel.cons.forEach(con => { -%>
> - ❌ <% con %>
<% }); -%>

---

## ⚙️ 配置文件生成 / Configuration File Generation

### OpenCode配置

<%*
// 生成配置文件
const configContent = {
    "$schema": "https://opencode.ai/config.json",
    "model": `ollama/${selectedModel.id}`,
    "provider": {
        "ollama": {
            "npm": "@ai-sdk/openai-compatible",
            "name": "Ollama (Local)",
            "options": {
                "baseURL": "http://localhost:11434/v1",
                "timeout": 120000,
                "maxRetries": 3
            },
            "models": {}
        }
    }
};

// 添加模型配置
configContent.provider.ollama.models[selectedModel.id] = {
    "name": `${selectedModel.name} (Local)`,
    "description": selectedModel.useCase,
    "options": {
        "temperature": selectedModel.performance === '高性能' ? 0.05 : 0.1,
        "top_p": 0.9,
        "extraBody": {
            "num_ctx": selectedModel.performance === '高性能' ? 8192 : 4096,
            "num_batch": 512,
            "repeat_penalty": 1.1
        }
    },
    "limit": {
        "context": selectedModel.performance === '高性能' ? 8192 : 4096,
        "output": 4096
    }
};
-%>

#### 📄 生成的配置文件

<details>
<summary>🔧 OpenCode配置 / OpenCode Configuration</summary>

**文件位置**: `~/.config/opencode/opencode.json`

```json
<% JSON.stringify(configContent, null, 2) %>
```

</details>

### 安装命令 / Installation Commands

<%*
// 生成安装脚本
const installCommands = [];
if (hardwareInfo.platform === 'win32') {
    installCommands.push(
        '# Windows安装命令',
        '# 1. 创建配置目录',
        'if not exist "%USERPROFILE%\\.config\\opencode" mkdir "%USERPROFILE%\\.config\\opencode"',
        '',
        '# 2. 创建配置文件',
        'echo {"$schema": "https://opencode.ai/config.json"} > "%USERPROFILE%\\.config\\opencode\\opencode.json"',
        '',
        '# 3. 下载模型',
        'ollama pull ' + selectedModel.id,
        '',
        '# 4. 启动Ollama',
        'ollama serve'
    );
} else {
    installCommands.push(
        '# macOS/Linux安装命令',
        '# 1. 创建配置目录',
        'mkdir -p ~/.config/opencode',
        '',
        '# 2. 创建配置文件',
        'cat > ~/.config/opencode/opencode.json << EOF',
        JSON.stringify(configContent, null, 2),
        'EOF',
        '',
        '# 3. 下载模型',
        'ollama pull ' + selectedModel.id,
        '',
        '# 4. 启动Ollama',
        'ollama serve'
    );
}
-%>

#### 🚀 一键安装脚本

<details>
<summary>📋 安装步骤 / Installation Steps</summary>

```bash
<% installCommands.join('\n') %>
```

**手动步骤**:
1. **配置目录**: 确保配置目录存在
2. **配置文件**: 将上面的JSON内容保存到配置文件
3. **下载模型**: 执行模型下载命令
4. **启动服务**: 启动Ollama服务
5. **测试集成**: 使用OpenCode测试

</details>

---

## ✅ 配置验证 / Configuration Validation

### 验证清单

<details>
<summary>🔍 配置检查清单 / Configuration Checklist</summary>

| 检查项 / Check Item | 验证方法 / Verification Method | 状态 / Status |
|-------------------|----------------------------|-------------|
| **Ollama服务运行** | `curl http://localhost:11434/api/tags` | ⏳ 待验证 |
| **模型已下载** | `ollama list \| grep <% selectedModel.id.split(':')[0] %>` | ⏳ 待验证 |
| **OpenCode配置正确** | `cat ~/.config/opencode/opencode.json` | ⏳ 待验证 |
| **OpenCode版本兼容** | `opencode --version` | ⏳ 待验证 |
| **基础功能测试** | `opencode run "测试" --model ollama/<% selectedModel.id %>` | ⏳ 待验证 |

</details>

### 自动验证脚本

<%*
const validationScript = `#!/bin/bash
# OpenCode + Ollama 配置验证脚本
# Generated by Configuration Wizard

echo "🔍 开始配置验证..."
echo ""

# 检查Ollama服务
echo "1. 检查Ollama服务状态"
if curl -s http://localhost:11434/api/tags > /dev/null 2>&1; then
    echo "✅ Ollama服务运行正常"
    ollama_status=0
else
    echo "❌ Ollama服务未运行"
    echo "   启动命令: ollama serve"
    ollama_status=1
fi
echo ""

# 检查模型
echo "2. 检查模型 ${selectedModel.name}"
if ollama list | grep -q "${selectedModel.id}"; then
    echo "✅ 模型已下载"
    model_status=0
else
    echo "❌ 模型未下载"
    echo "   下载命令: ollama pull ${selectedModel.id}"
    model_status=1
fi
echo ""

# 检查配置文件
echo "3. 检查OpenCode配置"
config_file="$HOME/.config/opencode/opencode.json"
if [ -f "$config_file" ]; then
    echo "✅ 配置文件存在"
    if grep -q "${selectedModel.id}" "$config_file"; then
        echo "✅ 模型配置正确"
        config_status=0
    else
        echo "❌ 模型配置错误"
        config_status=1
    fi
else
    echo "❌ 配置文件不存在"
    config_status=1
fi
echo ""

# 检查OpenCode
echo "4. 检查OpenCode安装"
if command -v opencode &> /dev/null; then
    echo "✅ OpenCode已安装"
    opencode_version=$(opencode --version 2>/dev/null | head -1)
    echo "   版本: $opencode_version"
    opencode_status=0
else
    echo "❌ OpenCode未安装"
    echo "   安装命令: npm install -g @opencode-ai/cli"
    opencode_status=1
fi
echo ""

# 测试集成
echo "5. 测试集成功能"
if [ $ollama_status -eq 0 ] && [ $model_status -eq 0 ] && [ $config_status -eq 0 ] && [ $opencode_status -eq 0 ]; then
    echo "⏳ 执行功能测试..."
    if timeout 30 opencode run "简单测试" --model ollama/${selectedModel.id} > /dev/null 2>&1; then
        echo "✅ 集成测试通过"
        test_status=0
    else
        echo "❌ 集成测试失败"
        test_status=1
    fi
else
    echo "⏭️ 跳过集成测试（前置检查失败）"
    test_status=1
fi
echo ""

# 总结
echo "📊 验证结果总结"
echo "Ollama服务: $([ $ollama_status -eq 0 ] && echo '✅ 通过' || echo '❌ 失败')"
echo "模型下载: $([ $model_status -eq 0 ] && echo '✅ 通过' || echo '❌ 失败')"
echo "配置文件: $([ $config_status -eq 0 ] && echo '✅ 通过' || echo '❌ 失败')"
echo "OpenCode: $([ $opencode_status -eq 0 ] && echo '✅ 通过' || echo '❌ 失败')"
echo "集成测试: $([ $test_status -eq 0 ] && echo '✅ 通过' || echo '❌ 失败')"

if [ $ollama_status -eq 0 ] && [ $model_status -eq 0 ] && [ $config_status -eq 0 ] && [ $opencode_status -eq 0 ] && [ $test_status -eq 0 ]; then
    echo ""
    echo "🎉 所有验证通过！配置成功！"
    exit 0
else
    echo ""
    echo "⚠️ 部分验证失败，请检查上述错误"
    exit 1
fi
`;
-%>

<details>
<summary>🔧 验证脚本 / Validation Script</summary>

**保存为**: `validate-config.sh`

```bash
<% validationScript %>
```

**执行**:
```bash
chmod +x validate-config.sh
./validate-config.sh
```

</details>

---

## 🚀 快速开始 / Quick Start

### 首次使用步骤

<%*
const quickStartSteps = [
    {
        step: 1,
        title: '启动服务',
        description: '启动Ollama服务',
        command: 'ollama serve'
    },
    {
        step: 2,
        title: '验证模型',
        description: '确认模型可用',
        command: `ollama list | grep ${selectedModel.id.split(':')[0]}`
    },
    {
        step: 3,
        title: '启动OpenCode',
        description: '启动OpenCode客户端',
        command: 'opencode'
    },
    {
        step: 4,
        title: '选择模型',
        description: '在OpenCode中选择本地模型',
        command: '/models'
    },
    {
        step: 5,
        title: '首次测试',
        description: '执行简单测试任务',
        command: '创建一个Hello World程序'
    }
];
-%>

| 步骤 / Step | 操作 / Action | 命令 / Command | 预期结果 / Expected Result |
|-------------|---------------|----------------|-------------------------|
<% quickStartSteps.forEach(step => { -%>
| **<% step.step %>. <% step.title %>** | <% step.description %> | `<% step.command %>` | ✅ 完成无错误 |
<% }); -%>

### 示例测试任务

<details>
<summary>💡 推荐测试任务 / Recommended Test Tasks</summary>

#### 基础测试
```bash
# 简单代码生成
创建一个Python函数，计算两个数字的和

# 文件分析
分析当前目录中的README.md文件

# 工具使用测试
在当前目录创建一个test.py文件，包含简单的测试代码
```

#### 进阶测试
```bash
# 复杂任务
创建一个Flask Web应用，包含基本的CRUD操作

# 多文件处理
分析这个Python项目的结构，并生成项目文档

# 代码重构
重构这个函数，提高性能和可读性
```

</details>

---

## 📊 性能调优 / Performance Tuning

### 环境变量优化

<details>
<summary>⚙️ 高级环境变量 / Advanced Environment Variables</summary>

```bash
# GPU优化
export OLLAMA_GPU_MEMORY_FRACTION=0.8     # GPU内存使用比例
export OLLAMA_NUM_PARALLEL=4               # 并行处理数量
export OLLAMA_MAX_LOADED_MODELS=2           # 最大预加载模型数

# 内存优化
export OLLAMA_CONTEXT_SIZE=8192            # 默认上下文大小
export OLLAMA_BATCH_SIZE=512                # 批处理大小

# 性能优化
export OLLAMA_KEEP_ALIVE=24h               # 模型保持活跃时间
export OLLAMA_HOST=0.0.0.0:11434          # 服务绑定地址
```

</details>

### 模型参数调优

<%*
// 根据模型和使用场景生成调优建议
const tuningSuggestions = [];
const performanceLevel = selectedModel.performance;

if (performanceLevel === '高性能') {
    tuningSuggestions.push({
        parameter: 'temperature',
        recommended: 0.05,
        reason: '高精度任务需要更低温度'
    });
    tuningSuggestions.push({
        parameter: 'top_p',
        recommended: 0.8,
        reason: '减少随机性，提高输出稳定性'
    });
} else if (performanceLevel === '平衡') {
    tuningSuggestions.push({
        parameter: 'temperature',
        recommended: 0.1,
        reason: '平衡创造性和准确性'
    });
    tuningSuggestions.push({
        parameter: 'top_p',
        recommended: 0.9,
        reason: '适度多样性，保持相关性'
    });
} else {
    tuningSuggestions.push({
        parameter: 'temperature',
        recommended: 0.2,
        reason: '增加创造性，适用于基础任务'
    });
    tuningSuggestions.push({
        parameter: 'top_p',
        recommended: 0.95,
        reason: '提高响应多样性'
    });
}
-%>

#### 🔧 推荐参数设置

| 参数 / Parameter | 推荐值 / Recommended | 原因 / Reason |
|----------------|----------------------|-------------|
<% tuningSuggestions.forEach(tune => { -%>
| **<% tune.parameter %>** | `<% tune.recommended %>` | `<% tune.reason %>` |
<% }); -%>

#### 📄 更新配置

<details>
<summary>🔧 参数调优配置 / Parameter Tuning Configuration</summary>

```json
{
  "provider": {
    "ollama": {
      "models": {
        "<% selectedModel.id %>": {
          "options": {
            "temperature": <% tuningSuggestions.find(t => t.parameter === 'temperature').recommended %>,
            "top_p": <% tuningSuggestions.find(t => t.parameter === 'top_p').recommended %>,
            "extraBody": {
              "num_ctx": 8192,
              "repeat_penalty": 1.1
            }
          }
        }
      }
    }
  }
}
```

</details>

---

## 📚 参考资源 / Reference Resources

### 📖 文档链接
- **[OpenCode配置文档](https://opencode.ai/docs/config)** - 详细配置说明
- **[Ollama模型库](https://ollama.ai/library)** - 可用模型列表
- **[硬件要求指南](https://github.com/ollama/ollama/blob/main/docs/faq.md)** - 硬件兼容性

### 🛠️ 实用工具
- **[nvidia-smi](https://developer.nvidia.com/nvidia-smi)** - GPU监控工具
- **[htop](https://htop.dev/)** - 系统资源监控
- **[Model Inspector](https://huggingface.co/spaces/Xenova/model-inspector)** - 模型检查工具

---

## 📝 配置记录 / Configuration Record

> [!note] 📋 个人配置记录 / Personal Configuration Log
> 
> **配置日期**: `<% tp.date.now('YYYY-MM-DD HH:mm') %>`
> **选择模型**: `<% selectedModel.name %>`
> **硬件配置**: GPU: `<% hardwareInfo.gpuMemory %>GB`, RAM: `<% hardwareInfo.totalMemory %>GB`
> **性能等级**: `<% selectedModel.performance %>`
> **配置状态**: ✅ 已完成 / ⏳ 进行中 / ❌ 需要修复

---

> [!success] 🎉 配置向导完成！/ Configuration Wizard Complete!
> 
> 您的OpenCode + Ollama环境配置已完成！下一步：
> 1. **保存配置文件** - 将生成的配置保存到正确位置
> 2. **执行安装命令** - 下载并启动必要服务
> 3. **运行验证脚本** - 确保一切正常工作
> 4. **开始使用** - 享受本地AI编程助手！

*此配置向导基于最佳实践设计，如有问题请参考故障排除指南 / This wizard is designed based on best practices*