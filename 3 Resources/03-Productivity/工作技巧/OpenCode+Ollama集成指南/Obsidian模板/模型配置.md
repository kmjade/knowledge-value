---
tags:
  - opencode
  - ollama
  - model-config
  - setup
  - guide
created: <% tp.file.creation_date() %>
---

# 配置

# 配置

# 配置
> 這個向导将帮助您：
# 配置
# 配置
# 配置
> 4. **優化效能** - 根据硬體调优参数

---

## 🖥️ 硬體評估 / Hardware Assessment

### 自動硬體检测

<%*
// 检测系統硬體資訊
const hardwareInfo = {
    platform: process.platform,
    arch: process.arch,
    
    // GPU检测（簡化版）
    hasGPU: false, // 实际環境中可以通過命令检测
    gpuMemory: 0,
    
    // 記憶體检测（模拟）
    totalMemory: 16, // 默认值，实际環境中可以获取真实值
    availableMemory: 8,
    
    // 儲存检测
    availableStorage: 100 // GB
};

// 尝试获取更准确的硬體資訊
try {
    // 在实际環境中，这里会执行系統命令
    // const gpuInfo = require('child_process').execSync('nvidia-smi --query-gpu=memory.total --format=csv,noheader,nounits').toString();
    // hardwareInfo.gpuMemory = parseInt(gpuInfo);
    // hardwareInfo.hasGPU = hardwareInfo.gpuMemory > 0;
} catch (error) {
    console.log('GPU检测失败，使用默认值');
}
-%>

### 当前系統資訊

| 硬體組件 / Hardware Component | 检测結果 / Detected Result | 狀態 / Status |
|--------------------------|-------------------------|-------------|
| **操作系統 / OS** | `<% hardwareInfo.platform %>` | ✅ 已检测 |
| **架構 / Architecture** | `<% hardwareInfo.arch %>` | ✅ 已检测 |
| **GPU支持 / GPU Support** | `<% hardwareInfo.hasGPU ? '✅ 可用' : '❌ 未检测到' %>` | `<% hardwareInfo.hasGPU ? '✅ 優秀' : '⚠️ 需CPU模式' %>` |
| **GPU記憶體 / GPU Memory** | `<% hardwareInfo.gpuMemory %>GB` | `<% hardwareInfo.gpuMemory >= 8 ? '✅ 充足' : hardwareInfo.gpuMemory > 0 ? '⚠️ 可能不足' : '❌ 無GPU' %>` |
| **系統記憶體 / System RAM** | `<% hardwareInfo.totalMemory %>GB` | `<% hardwareInfo.totalMemory >= 16 ? '✅ 充足' : '⚠️ 建議升級' %>` |
| **可用儲存 / Available Storage** | `<% hardwareInfo.availableStorage %>GB` | ✅ 充足 |

### 硬體相容性評估

<%*
// 根据硬體評估相容性
let hardwareScore = 0;
let recommendations = [];

if (hardwareInfo.hasGPU) {
    hardwareScore += 30;
    if (hardwareInfo.gpuMemory >= 8) hardwareScore += 20;
    if (hardwareInfo.gpuMemory >= 16) hardwareScore += 20;
} else {
    recommendations.push('考虑新增GPU以提升效能');
}

if (hardwareInfo.totalMemory >= 16) {
    hardwareScore += 15;
} else if (hardwareInfo.totalMemory >= 8) {
    hardwareScore += 10;
} else {
    recommendations.push('建議升級系統記憶體到至少16GB');
}

if (hardwareInfo.availableStorage >= 50) {
    hardwareScore += 10;
}

// 評估等级
let performanceLevel = '';
if (hardwareScore >= 70) {
    performanceLevel = '🏆 高效能 - 适合所有本地模型';
} else if (hardwareScore >= 50) {
    performanceLevel = '⭐ 中等效能 - 适合主流模型';
} else if (hardwareScore >= 30) {
    performanceLevel = '📱 基礎效能 - 适合轻量模型';
} else {
    performanceLevel = '⚠️ 效能不足 - 建議硬體升級';
}
-%>

| 評估项 / Assessment | 得分 / Score | 评级 / Grade |
|------------------|-------------|-------------|
| **硬體总分 / Hardware Score** | `<% hardwareScore %>/100` | `<% performanceLevel %>` |

### 🎯 優化建議

<%*
if (recommendations.length > 0) {
-%>
<details>
<summary>📋 硬體優化建議 / Hardware Optimization Recommendations</summary>

<% recommendations.forEach(rec => { -%>
- 🔧 <% rec %>
<% }); -%>

</details>

<%
} else {
-%>
# 配置
# 配置

<%
}
-%>

---

## 🤖 模型推荐 / Model Recommendations

### 智能模型選擇

<%*
// 根据硬體推荐模型
const recommendedModels = [];
const modelDetails = {
    'qwen2.5-coder:3b': {
        name: 'Qwen2.5-Coder 3B',
        memory: 6, gpu: 4, ram: 8, performance: '基礎',
        useCase: '轻量级編程，快速響應',
        pros: ['啟動快', '記憶體占用低', '适合CPU模式'],
        cons: ['复杂任務能力有限']
    },
    'qwen2.5-coder:7b': {
        name: 'Qwen2.5-Coder 7B',
        memory: 14, gpu: 8, ram: 16, performance: '平衡',
        useCase: '通用編程，推荐選擇',
        pros: ['效能与資源平衡', '工具支持', '代碼品質高'],
        cons: ['需要8GB+ GPU']
    },
    'qwen2.5-coder:14b': {
        name: 'Qwen2.5-Coder 14B',
        memory: 28, gpu: 16, ram: 32, performance: '高效能',
        useCase: '复杂編程任務，专业開發',
        pros: ['强大的推理能力', '優秀的代碼品質', '支持复杂專案'],
        cons: ['需要强大硬體', '響應速度较慢']
    },
    'qwen2.5:7b': {
        name: 'Qwen2.5 7B',
        memory: 14, gpu: 8, ram: 16, performance: '通用',
        useCase: '通用对话和基礎編程',
        pros: ['平衡的效能', '多语言支持', '上下文大'],
        cons: ['編程能力不如专用模型']
    },
    'mistral-nemo:12b': {
        name: 'Mistral-Nemo 12B',
        memory: 24, gpu: 12, ram: 24, performance: '推理强',
        useCase: '复杂推理，代碼審查',
# 分析
        cons: ['編程专用能力较弱', '資源需求高']
    }
};

// 篩選合适的模型
Object.entries(modelDetails).forEach(([modelId, details]) => {
    if (details.gpu <= (hardwareInfo.gpuMemory || 8) && 
        details.ram <= (hardwareInfo.totalMemory || 16)) {
        recommendedModels.push({ id: modelId, ...details });
    }
});

// 如果没有合适的模型，新增最基礎的选项
if (recommendedModels.length === 0) {
    recommendedModels.push({
        id: 'qwen2.5:3b',
        name: 'Qwen2.5 3B',
        memory: 6,
        gpu: 4,
        ram: 8,
        performance: '基礎',
# 配置
        pros: ['可在CPU運行', '記憶體需求低'],
        cons: ['功能有限']
    });
}
-%>

### 📊 推荐模型列表

| 模型 / Model | GPU記憶體 / GPU VRAM | 系統記憶體 / RAM | 效能等级 / Level | 适用場景 / Use Case |
|-------------|-------------------|----------------|-------------------|-------------------|
<%*
recommendedModels.forEach(model => {
-%>
| **<% model.name %>** | `<% model.gpu %>GB+` | `<% model.ram %>GB+` | `<% model.performance %>` | `<% model.useCase %>` |
<%
});
-%>

### 🎯 詳細模型對比

<%*
// 選擇模型進行詳細對比
const modelOptions = recommendedModels.map(m => m.name);
const selectedModelName = await tp.system.suggester(
    modelOptions,
    recommendedModels.map(m => m.id),
    false,
    '選擇要詳細了解的模型 / Select model to details:'
);

const selectedModel = recommendedModels.find(m => m.id === selectedModelName) || recommendedModels[0];
-%>

#### 🏆 选中模型: <% selectedModel.name %>

> [!info] 模型詳細資訊 / Model Details
> 
> **基本資訊 / Basic Info**:
> - **模型ID**: `<% selectedModel.id %>`
> - **效能等级**: `<% selectedModel.performance %>`
> - **推荐用途**: `<% selectedModel.useCase %>`
> 
> **硬體需求 / Hardware Requirements**:
> - **GPU記憶體**: `<% selectedModel.gpu %>GB`
> - **系統記憶體**: `<% selectedModel.ram %>GB`
> - **儲存空間**: `<% Math.ceil(selectedModel.memory / 2) %>GB` (模型檔案)
> 
> **優勢 / Pros**:
<% selectedModel.pros.forEach(pro => { -%>
> - ✅ <% pro %>
<% }); -%>
> 
> **限制 / Cons**:
<% selectedModel.cons.forEach(con => { -%>
> - ❌ <% con %>
<% }); -%>

---

# 配置

# 配置

<%*
# 配置
const configContent = {
    "$schema": "https://opencode.ai/config.json",
    "model": `ollama/${selectedModel.id}`,
    "provider": {
        "ollama": {
            "npm": "@ai-sdk/openai-compatible",
            "name": "Ollama (Local)",
            "options": {
                "baseURL": "http://localhost:11434/v1",
                "timeout": 120000,
                "maxRetries": 3
            },
            "models": {}
        }
    }
};

# 配置
configContent.provider.ollama.models[selectedModel.id] = {
    "name": `${selectedModel.name} (Local)`,
    "description": selectedModel.useCase,
    "options": {
        "temperature": selectedModel.performance === '高效能' ? 0.05 : 0.1,
        "top_p": 0.9,
        "extraBody": {
            "num_ctx": selectedModel.performance === '高效能' ? 8192 : 4096,
            "num_batch": 512,
            "repeat_penalty": 1.1
        }
    },
    "limit": {
        "context": selectedModel.performance === '高效能' ? 8192 : 4096,
        "output": 4096
    }
};
-%>

# 配置

<details>
# 配置

**檔案位置**: `~/.config/opencode/opencode.json`

```json
<% JSON.stringify(configContent, null, 2) %>
```

</details>

### 安裝命令 / Installation Commands

<%*
// 生成安裝腳本
const installCommands = [];
if (hardwareInfo.platform === 'win32') {
    installCommands.push(
        '# Windows安裝命令',
# 配置
        'if not exist "%USERPROFILE%\\.config\\opencode" mkdir "%USERPROFILE%\\.config\\opencode"',
        '',
# 配置
        'echo {"$schema": "https://opencode.ai/config.json"} > "%USERPROFILE%\\.config\\opencode\\opencode.json"',
        '',
        '# 3. 下載模型',
        'ollama pull ' + selectedModel.id,
        '',
        '# 4. 啟動Ollama',
        'ollama serve'
    );
} else {
    installCommands.push(
        '# macOS/Linux安裝命令',
# 配置
        'mkdir -p ~/.config/opencode',
        '',
# 配置
        'cat > ~/.config/opencode/opencode.json << EOF',
        JSON.stringify(configContent, null, 2),
        'EOF',
        '',
        '# 3. 下載模型',
        'ollama pull ' + selectedModel.id,
        '',
        '# 4. 啟動Ollama',
        'ollama serve'
    );
}
-%>

#### 🚀 一键安裝腳本

<details>
<summary>📋 安裝步骤 / Installation Steps</summary>

```bash
<% installCommands.join('\n') %>
```

**手動步骤**:
# 配置
# 配置
3. **下載模型**: 执行模型下載命令
4. **啟動服务**: 啟動Ollama服务
5. **測試整合**: 使用OpenCode測試

</details>

---

# 配置

### 驗證清單

<details>
# 配置

# 方法
|-------------------|----------------------------|-------------|
| **Ollama服务運行** | `curl http://localhost:11434/api/tags` | ⏳ 待驗證 |
| **模型已下載** | `ollama list \| grep <% selectedModel.id.split(':')[0] %>` | ⏳ 待驗證 |
# 配置
# 版本
| **基礎功能測試** | `opencode run "測試" --model ollama/<% selectedModel.id %>` | ⏳ 待驗證 |

</details>

### 自動驗證腳本

<%*
const validationScript = `#!/bin/bash
# 配置
# Generated by Configuration Wizard

# 配置
echo ""

# 檢查Ollama服务
echo "1. 檢查Ollama服务狀態"
if curl -s http://localhost:11434/api/tags > /dev/null 2>&1; then
    echo "✅ Ollama服务運行正常"
    ollama_status=0
else
    echo "❌ Ollama服务未運行"
    echo "   啟動命令: ollama serve"
    ollama_status=1
fi
echo ""

# 檢查模型
echo "2. 檢查模型 ${selectedModel.name}"
if ollama list | grep -q "${selectedModel.id}"; then
    echo "✅ 模型已下載"
    model_status=0
else
    echo "❌ 模型未下載"
    echo "   下載命令: ollama pull ${selectedModel.id}"
    model_status=1
fi
echo ""

# 配置
# 配置
config_file="$HOME/.config/opencode/opencode.json"
if [ -f "$config_file" ]; then
# 配置
    if grep -q "${selectedModel.id}" "$config_file"; then
# 配置
        config_status=0
    else
# 配置
        config_status=1
    fi
else
# 配置
    config_status=1
fi
echo ""

# 檢查OpenCode
echo "4. 檢查OpenCode安裝"
if command -v opencode &> /dev/null; then
    echo "✅ OpenCode已安裝"
    opencode_version=$(opencode --version 2>/dev/null | head -1)
# 版本
    opencode_status=0
else
    echo "❌ OpenCode未安裝"
    echo "   安裝命令: npm install -g @opencode-ai/cli"
    opencode_status=1
fi
echo ""

# 測試整合
echo "5. 測試整合功能"
if [ $ollama_status -eq 0 ] && [ $model_status -eq 0 ] && [ $config_status -eq 0 ] && [ $opencode_status -eq 0 ]; then
    echo "⏳ 执行功能測試..."
    if timeout 30 opencode run "简单測試" --model ollama/${selectedModel.id} > /dev/null 2>&1; then
        echo "✅ 整合測試通過"
        test_status=0
    else
        echo "❌ 整合測試失败"
        test_status=1
    fi
else
    echo "⏭️ 跳过整合測試（前置檢查失败）"
    test_status=1
fi
echo ""

# 總結
echo "📊 驗證結果總結"
echo "Ollama服务: $([ $ollama_status -eq 0 ] && echo '✅ 通過' || echo '❌ 失败')"
echo "模型下載: $([ $model_status -eq 0 ] && echo '✅ 通過' || echo '❌ 失败')"
# 配置
echo "OpenCode: $([ $opencode_status -eq 0 ] && echo '✅ 通過' || echo '❌ 失败')"
echo "整合測試: $([ $test_status -eq 0 ] && echo '✅ 通過' || echo '❌ 失败')"

if [ $ollama_status -eq 0 ] && [ $model_status -eq 0 ] && [ $config_status -eq 0 ] && [ $opencode_status -eq 0 ] && [ $test_status -eq 0 ]; then
    echo ""
# 配置
    exit 0
else
    echo ""
    echo "⚠️ 部分驗證失败，请檢查上述错误"
    exit 1
fi
`;
-%>

<details>
<summary>🔧 驗證腳本 / Validation Script</summary>

**儲存为**: `validate-config.sh`

```bash
<% validationScript %>
```

**执行**:
```bash
chmod +x validate-config.sh
./validate-config.sh
```

</details>

---

## 🚀 快速開始 / Quick Start

### 首次使用步骤

<%*
const quickStartSteps = [
    {
        step: 1,
        title: '啟動服务',
        description: '啟動Ollama服务',
        command: 'ollama serve'
    },
    {
        step: 2,
        title: '驗證模型',
        description: '確認模型可用',
        command: `ollama list | grep ${selectedModel.id.split(':')[0]}`
    },
    {
        step: 3,
        title: '啟動OpenCode',
        description: '啟動OpenCode客户端',
        command: 'opencode'
    },
    {
        step: 4,
        title: '選擇模型',
        description: '在OpenCode中選擇本地模型',
        command: '/models'
    },
    {
        step: 5,
        title: '首次測試',
        description: '执行简单測試任務',
        command: '創建一个Hello World程式'
    }
];
-%>

| 步骤 / Step | 操作 / Action | 命令 / Command | 预期結果 / Expected Result |
|-------------|---------------|----------------|-------------------------|
<% quickStartSteps.forEach(step => { -%>
| **<% step.step %>. <% step.title %>** | <% step.description %> | `<% step.command %>` | ✅ 完成無错误 |
<% }); -%>

### 示例測試任務

<details>
<summary>💡 推荐測試任務 / Recommended Test Tasks</summary>

#### 基礎測試
```bash
# 简单代碼生成
創建一个Python函数，計算两个数字的和

# 分析
# 分析

# 工具使用測試
在当前目錄創建一个test.py檔案，包含简单的測試代碼
```

#### 进阶測試
```bash
# 复杂任務
創建一个Flask Web應用程式，包含基本的CRUD操作

# 多檔案處理
# 分析

# 代碼重构
重构這個函数，提高效能和可读性
```

</details>

---

## 📊 效能调优 / Performance Tuning

### 環境变量優化

<details>
<summary>⚙️ 高级環境变量 / Advanced Environment Variables</summary>

```bash
# GPU優化
export OLLAMA_GPU_MEMORY_FRACTION=0.8     # GPU記憶體使用比例
export OLLAMA_NUM_PARALLEL=4               # 并行處理数量
export OLLAMA_MAX_LOADED_MODELS=2           # 最大预加载模型数

# 記憶體優化
export OLLAMA_CONTEXT_SIZE=8192            # 默认上下文大小
export OLLAMA_BATCH_SIZE=512                # 批處理大小

# 效能優化
export OLLAMA_KEEP_ALIVE=24h               # 模型保持活跃時間
export OLLAMA_HOST=0.0.0.0:11434          # 服务绑定地址
```

</details>

### 模型参数调优

<%*
// 根据模型和使用場景生成调优建議
const tuningSuggestions = [];
const performanceLevel = selectedModel.performance;

if (performanceLevel === '高效能') {
    tuningSuggestions.push({
        parameter: 'temperature',
        recommended: 0.05,
        reason: '高精度任務需要更低温度'
    });
    tuningSuggestions.push({
        parameter: 'top_p',
        recommended: 0.8,
        reason: '减少随机性，提高輸出稳定性'
    });
} else if (performanceLevel === '平衡') {
    tuningSuggestions.push({
        parameter: 'temperature',
        recommended: 0.1,
        reason: '平衡创造性和准确性'
    });
    tuningSuggestions.push({
        parameter: 'top_p',
        recommended: 0.9,
        reason: '适度多样性，保持相關性'
    });
} else {
    tuningSuggestions.push({
        parameter: 'temperature',
        recommended: 0.2,
        reason: '增加创造性，适用于基礎任務'
    });
    tuningSuggestions.push({
        parameter: 'top_p',
        recommended: 0.95,
        reason: '提高響應多样性'
    });
}
-%>

#### 🔧 推荐参数設置

| 参数 / Parameter | 推荐值 / Recommended | 原因 / Reason |
|----------------|----------------------|-------------|
<% tuningSuggestions.forEach(tune => { -%>
| **<% tune.parameter %>** | `<% tune.recommended %>` | `<% tune.reason %>` |
<% }); -%>

# 更新

<details>
# 配置

```json
{
  "provider": {
    "ollama": {
      "models": {
        "<% selectedModel.id %>": {
          "options": {
            "temperature": <% tuningSuggestions.find(t => t.parameter === 'temperature').recommended %>,
            "top_p": <% tuningSuggestions.find(t => t.parameter === 'top_p').recommended %>,
            "extraBody": {
              "num_ctx": 8192,
              "repeat_penalty": 1.1
            }
          }
        }
      }
    }
  }
}
```

</details>

---

## 📚 參考資源 / Reference Resources

### 📖 文檔連結
# 配置
- **[Ollama模型库](https://ollama.ai/library)** - 可用模型列表
# 指南

### 🛠️ 实用工具
- **[nvidia-smi](https://developer.nvidia.com/nvidia-smi)** - GPU監控工具
- **[htop](https://htop.dev/)** - 系統資源監控
- **[Model Inspector](https://huggingface.co/spaces/Xenova/model-inspector)** - 模型檢查工具

---

# 配置

# 配置
> 
# 配置
> **選擇模型**: `<% selectedModel.name %>`
# 配置
> **效能等级**: `<% selectedModel.performance %>`
# 配置

---

# 配置
> 
# 配置
# 配置
> 2. **执行安裝命令** - 下載并啟動必要服务
> 3. **運行驗證腳本** - 确保一切正常工作
> 4. **開始使用** - 享受本地AI編程助手！

# 指南