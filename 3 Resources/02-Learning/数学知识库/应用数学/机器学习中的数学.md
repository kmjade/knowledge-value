---
title: 机器学习中的数学
aliases:
  - Mathematics in Machine Learning
  - ML数学基础
created: 2026-01-27
tags:
  - 数学/应用
  - 数学/主题
  - 机器学习
  - 人工智能
difficulty: 高级
mastery: 15%
---

# 机器学习中的数学

> "机器学习的本质是数据驱动的数学建模。" - 佚名

## 📖 概述

机器学习是人工智能的核心领域，其理论基础深深植根于多个数学分支：
- **线性代数**：数据表示和变换
- **微积分**：优化和梯度下降
- **概率统计**：模型评估和预测
- **信息论**：熵和损失函数

---

## 🔢 线性代数

### 向量与矩阵

**向量**：表示数据点的特征
```
x = [x₁, x₂, ..., xₙ]ᵀ
```

**矩阵**：表示数据集或变换
```
X = [x₁, x₂, ..., xₙ]  （每一列是一个数据点）
```

### 重要概念

| 概念 | 定义 | ML中的应用 |
|------|------|------------|
| 向量范数 | 向量的大小 | 正则化、距离度量 |
| 矩阵秩 | 矩阵的维度 | 降维 |
| 特征值/向量 | 方程 Ax = λx | PCA、PageRank |
| 奇异值分解 | A = UΣVᵀ | 推荐系统、图像压缩 |
| 矩阵求逆 | A⁻¹A = I | 最小二乘解 |

### 线性回归的矩阵形式

普通最小二乘法：
```
y = Xβ + ε

估计：β̂ = (XᵀX)⁻¹Xᵀy
```

---

## 📐 微积分

### 梯度与梯度下降

**梯度**：函数最快的增长方向
```
∇f(x) = [∂f/∂x₁, ∂f/∂x₂, ..., ∂f/∂xₙ]ᵀ
```

**梯度下降**：
```
θ = θ - α · ∇J(θ)
```

其中：
- θ：模型参数
- α：学习率
- J(θ)：损失函数
- ∇J(θ)：梯度

### 反向传播

链式法则在神经网络中的应用：

```
∂L/∂w = ∂L/∂a · ∂a/∂z · ∂z/∂w
```

**示例**：
```
z = wx + b
a = σ(z)
L = (a - y)²

∂L/∂w = 2(a - y) · σ'(z) · x
```

### 重要导数

| 函数 | 导数 | 应用 |
|------|------|------|
| Sigmoid | σ'(z) = σ(z)(1-σ(z)) | 分类激活 |
| ReLU | ReLU'(z) = max(0, 1) | 隐藏层激活 |
| Tanh | tanh'(z) = 1 - tanh²(z) | 归一化激活 |
| Softmax | ∂softmaxᵢ/∂zⱼ = softmaxᵢ(δᵢⱼ - softmaxⱼ) | 多分类 |

---

## 📊 概率统计

### 概率分布

| 分布 | ML应用 | 公式 |
|------|--------|------|
| 伯努利 | 二分类 | P(X) = pˣ(1-p)^(1-ˣ) |
| 高斯 | 噪声建模 | N(μ, σ²) |
| 泊松 | 计数数据 | P(k) = λᵏe⁻ˡ/k! |
| 狄利克雷 | 多项分布先验 | Dir(α) |

### 贝叶斯学习

**贝叶斯定理**：
```
P(θ|D) = P(D|θ) · P(θ) / P(D)
```

- P(θ)：先验概率
- P(D|θ)：似然
- P(θ|D)：后验概率
- P(D)：证据

**MAP估计**：
```
θ̂MAP = argmax P(θ|D) = argmax P(D|θ)P(θ)
```

### 最大似然估计

**MLE**：
```
θ̂MLE = argmax P(D|θ)
```

取对数后：
```
θ̂MLE = argmax Σ log P(xᵢ|θ)
```

### 常见损失函数

| 损失函数 | 公式 | 应用 |
|----------|------|------|
| 均方误差 | L = ½Σ(y-ŷ)² | 回归 |
| 交叉熵 | L = -Σy·log(ŷ) | 分类 |
| Hinge | L = max(0, 1-y·ŷ) | SVM |
| KL散度 | D_KL(P\|Q) = ΣP·log(P/Q) | 生成模型 |

---

## 🔢 优化理论

### 凸优化

**凸集**：任意两点的连线仍在集合内

**凸函数**：f(λx+(1-λ)y) ≤ λf(x)+(1-λ)f(y)

**重要性质**：
- 凸函数的局部最优即全局最优
- 梯度为0的点即为最小值点

### 正则化

**L1正则化**（Lasso）：
```
L = L₀ + λ·||w||₁
产生稀疏解
```

**L2正则化**（Ridge）：
```
L = L₀ + λ·||w||₂²
平滑权重分布
```

**弹性网络**：
```
L = L₀ + αλ||w||₁ + (1-α)λ||w||₂²
```

---

## 📈 特征工程

### 标准化

**Z-score标准化**：
```
x' = (x - μ) / σ
```

**Min-Max缩放**：
```
x' = (x - x_min) / (x_max - x_min)
```

### 降维

**主成分分析 (PCA)**：
```
1. 中心化数据
2. 计算协方差矩阵 Σ
3. 求特征值和特征向量
4. 选择前k个特征向量
5. 投影数据
```

**t-SNE**：
```
保持高维数据中的局部关系
```

---

## 🎯 分类算法中的数学

### 逻辑回归

**假设函数**：
```
hθ(x) = σ(θᵀx) = 1 / (1 + e^(-θᵀx))
```

**损失函数**：
```
L(θ) = -[y·log hθ(x) + (1-y)·log(1-hθ(x))]
```

### 支持向量机

**硬间隔**：
```
min ½||w||²
s.t. yᵢ(wᵀxᵢ + b) ≥ 1
```

**软间隔**：
```
min ½||w||² + C·Σξᵢ
s.t. yᵢ(wᵀxᵢ + b) ≥ 1 - ξᵢ, ξᵢ ≥ 0
```

### 决策树

**信息增益（ID3）**：
```
IG = H(D) - Σ (|Dᵢ|/|D|)·H(Dᵢ)
```

**基尼系数（CART）**：
```
Gini = 1 - Σpᵢ²
```

---

## 🧠 深度学习中的数学

### 卷积神经网络

**卷积运算**：
```
(S * K)[i,j] = ΣₘΣₙ S[i+m,j+n]·K[m,n]
```

**池化**：
- 最大池化：max(窗口内的值)
- 平均池化：mean(窗口内的值)

### 循环神经网络

**RNN状态更新**：
```
hₜ = σ(W_hh·hₜ₋₁ + W_xh·xₜ + b_h)
yₜ = σ(W_hy·hₜ + b_y)
```

**LSTM**：
```
fₜ = σ(W_f·[hₜ₋₁, xₜ] + b_f)    （遗忘门）
iₜ = σ(W_i·[hₜ₋₁, xₜ] + b_i)    （输入门）
C̃ₜ = tanh(W_C·[hₜ₋₁, xₜ] + b_C) （候选）
Cₜ = fₜ ⊙ Cₜ₋₁ + iₜ ⊙ C̃ₜ       （记忆状态）
oₜ = σ(W_o·[hₜ₋₁, xₜ] + b_o)    （输出门）
hₜ = oₜ ⊙ tanh(Cₜ)
```

### 注意力机制

**注意力权重**：
```
αᵢ = softmax(s(q, kᵢ))
```

**注意力输出**：
```
output = Σ αᵢ·vᵢ
```

---

## 📊 模型评估

### 分类指标

| 指标 | 公式 | 含义 |
|------|------|------|
| 准确率 | (TP+TN)/(TP+TN+FP+FN) | 正确预测的比例 |
| 精确率 | TP/(TP+FP) | 预测为正的准确性 |
| 召回率 | TP/(TP+FN) | 实际为正的召回率 |
| F1分数 | 2·(精确率·召回率)/(精确率+召回率) | 综合指标 |
| AUC-ROC | 曲线下面积 | 区分能力 |

### 回归指标

| 指标 | 公式 | 含义 |
|------|------|------|
| MSE | Σ(y-ŷ)²/n | 均方误差 |
| RMSE | √MSE | 均方根误差 |
| MAE | Σ|y-ŷ|/n | 平均绝对误差 |
| R² | 1 - SSE/SST | 拟合优度 |

---

## 💡 学习路径

### 第一步：数学基础
```
线性代数 → 微积分 → 概率统计
```

### 第二步：机器学习理论
```
监督学习 → 无监督学习 → 深度学习
```

### 第三步：实践
```
实现算法 → 调参优化 → 项目应用
```

---

## 📚 推荐资源

### 课程
- **Coursera**: Mathematics for Machine Learning
- **3Blue1Brown**: 线性代数本质、神经网络
- **Deep Learning Specialization**: Andrew Ng

### 书籍
- **《数学之美》**：吴军
- **《机器学习》**：周志华（西瓜书）
- **《深度学习》**：Goodfellow（花书）

### 工具
- **NumPy/SciPy**: 科学计算
- **Scikit-learn**: 机器学习库
- **TensorFlow/PyTorch**: 深度学习框架

---

## 🔗 相关主题

- [[高等数学/微积分基础]] - 微积分基础
- [[高等数学/线性代数]] - 线性代数
- [[统计学与概率/概率论基础]] - 概率论基础
- [[统计学与概率/统计学]] - 统计学方法
