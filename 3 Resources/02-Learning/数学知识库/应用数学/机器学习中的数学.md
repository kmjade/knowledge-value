---
title: 机器學習中的数学
aliases:
  - Mathematics in Machine Learning
  - ML数学基礎
created: 2026-01-27
tags:
  - 数学/應用程式
  - 数学/主題
  - 机器學習
  - 人工智能
difficulty: 高级
mastery: 15%
---

# 机器學習中的数学

> "机器學習的本质是數據驅動程式的数学建模。" - 佚名

## 📖 概述

机器學習是人工智能的核心领域，其理论基礎深深植根于多個数学分支：
- **线性代数**：數據表示和变换
- **微积分**：優化和梯度下降
- **概率統計**：模型評估和预测
- **資訊论**：熵和损失函数

---

## 🔢 线性代数

### 向量与矩阵

**向量**：表示數據点的特征
```
x = [x₁, x₂, ..., xₙ]ᵀ
```

**矩阵**：表示數據集或变换
```
X = [x₁, x₂, ..., xₙ]  （每一列是一个數據点）
```

### 重要概念

| 概念 | 定义 | ML中的應用程式 |
|------|------|------------|
| 向量范数 | 向量的大小 | 正则化、距离度量 |
| 矩阵秩 | 矩阵的维度 | 降维 |
| 特征值/向量 | 方程 Ax = λx | PCA、PageRank |
| 奇异值分解 | A = UΣVᵀ | 推荐系統、图像压缩 |
| 矩阵求逆 | A⁻¹A = I | 最小二乘解 |

### 线性回归的矩阵形式

普通最小二乘法：
```
y = Xβ + ε

估计：β̂ = (XᵀX)⁻¹Xᵀy
```

---

## 📐 微积分

### 梯度与梯度下降

**梯度**：函数最快的增长方向
```
∇f(x) = [∂f/∂x₁, ∂f/∂x₂, ..., ∂f/∂xₙ]ᵀ
```

**梯度下降**：
```
θ = θ - α · ∇J(θ)
```

其中：
- θ：模型参数
- α：學習率
- J(θ)：损失函数
- ∇J(θ)：梯度

### 反向传播

链式法则在神经網路中的應用程式：

```
∂L/∂w = ∂L/∂a · ∂a/∂z · ∂z/∂w
```

**示例**：
```
z = wx + b
a = σ(z)
L = (a - y)²

∂L/∂w = 2(a - y) · σ'(z) · x
```

### 重要导数

| 函数 | 导数 | 應用程式 |
|------|------|------|
| Sigmoid | σ'(z) = σ(z)(1-σ(z)) | 分類激活 |
| ReLU | ReLU'(z) = max(0, 1) | 隐藏层激活 |
| Tanh | tanh'(z) = 1 - tanh²(z) | 归一化激活 |
| Softmax | ∂softmaxᵢ/∂zⱼ = softmaxᵢ(δᵢⱼ - softmaxⱼ) | 多分類 |

---

## 📊 概率統計

### 概率分布

| 分布 | ML應用程式 | 公式 |
|------|--------|------|
| 伯努利 | 二分類 | P(X) = pˣ(1-p)^(1-ˣ) |
| 高斯 | 噪声建模 | N(μ, σ²) |
| 泊松 | 计数數據 | P(k) = λᵏe⁻ˡ/k! |
| 狄利克雷 | 多项分布先验 | Dir(α) |

### 贝叶斯學習

**贝叶斯定理**：
```
P(θ|D) = P(D|θ) · P(θ) / P(D)
```

- P(θ)：先验概率
- P(D|θ)：似然
- P(θ|D)：后验概率
- P(D)：证据

**MAP估计**：
```
θ̂MAP = argmax P(θ|D) = argmax P(D|θ)P(θ)
```

### 最大似然估计

**MLE**：
```
θ̂MLE = argmax P(D|θ)
```

取对数后：
```
θ̂MLE = argmax Σ log P(xᵢ|θ)
```

### 常见损失函数

| 损失函数 | 公式 | 應用程式 |
|----------|------|------|
| 均方误差 | L = ½Σ(y-ŷ)² | 回归 |
| 交叉熵 | L = -Σy·log(ŷ) | 分類 |
| Hinge | L = max(0, 1-y·ŷ) | SVM |
| KL散度 | D_KL(P\|Q) = ΣP·log(P/Q) | 生成模型 |

---

## 🔢 優化理论

### 凸優化

**凸集**：任意两点的连线仍在集合内

**凸函数**：f(λx+(1-λ)y) ≤ λf(x)+(1-λ)f(y)

**重要性质**：
- 凸函数的局部最优即全局最优
- 梯度为0的点即为最小值点

### 正则化

**L1正则化**（Lasso）：
```
L = L₀ + λ·||w||₁
产生稀疏解
```

**L2正则化**（Ridge）：
```
L = L₀ + λ·||w||₂²
平滑权重分布
```

**弹性網路**：
```
L = L₀ + αλ||w||₁ + (1-α)λ||w||₂²
```

---

## 📈 特征工程

### 標準化

**Z-score標準化**：
```
x' = (x - μ) / σ
```

**Min-Max缩放**：
```
x' = (x - x_min) / (x_max - x_min)
```

### 降维

# 分析
```
1. 中心化數據
2. 計算协方差矩阵 Σ
3. 求特征值和特征向量
4. 選擇前k个特征向量
5. 投影數據
```

**t-SNE**：
```
保持高维數據中的局部关系
```

---

## 🎯 分類算法中的数学

### 逻辑回归

**假设函数**：
```
hθ(x) = σ(θᵀx) = 1 / (1 + e^(-θᵀx))
```

**损失函数**：
```
L(θ) = -[y·log hθ(x) + (1-y)·log(1-hθ(x))]
```

### 支持向量机

**硬间隔**：
```
min ½||w||²
s.t. yᵢ(wᵀxᵢ + b) ≥ 1
```

**软间隔**：
```
min ½||w||² + C·Σξᵢ
s.t. yᵢ(wᵀxᵢ + b) ≥ 1 - ξᵢ, ξᵢ ≥ 0
```

### 決策树

**資訊增益（ID3）**：
```
IG = H(D) - Σ (|Dᵢ|/|D|)·H(Dᵢ)
```

**基尼系数（CART）**：
```
Gini = 1 - Σpᵢ²
```

---

## 🧠 深度學習中的数学

### 卷积神经網路

**卷积運算**：
```
(S * K)[i,j] = ΣₘΣₙ S[i+m,j+n]·K[m,n]
```

**池化**：
- 最大池化：max(視窗内的值)
- 平均池化：mean(視窗内的值)

### 循环神经網路

# 更新
```
hₜ = σ(W_hh·hₜ₋₁ + W_xh·xₜ + b_h)
yₜ = σ(W_hy·hₜ + b_y)
```

**LSTM**：
```
fₜ = σ(W_f·[hₜ₋₁, xₜ] + b_f)    （遗忘门）
iₜ = σ(W_i·[hₜ₋₁, xₜ] + b_i)    （輸入门）
C̃ₜ = tanh(W_C·[hₜ₋₁, xₜ] + b_C) （候选）
Cₜ = fₜ ⊙ Cₜ₋₁ + iₜ ⊙ C̃ₜ       （记忆狀態）
oₜ = σ(W_o·[hₜ₋₁, xₜ] + b_o)    （輸出门）
hₜ = oₜ ⊙ tanh(Cₜ)
```

### 注意力機制

**注意力权重**：
```
αᵢ = softmax(s(q, kᵢ))
```

**注意力輸出**：
```
output = Σ αᵢ·vᵢ
```

---

## 📊 模型評估

### 分類指标

| 指标 | 公式 | 含義 |
|------|------|------|
| 准确率 | (TP+TN)/(TP+TN+FP+FN) | 正确预测的比例 |
| 精确率 | TP/(TP+FP) | 预测为正的准确性 |
| 召回率 | TP/(TP+FN) | 实际为正的召回率 |
| F1分数 | 2·(精确率·召回率)/(精确率+召回率) | 综合指标 |
| AUC-ROC | 曲線下面积 | 区分能力 |

### 回归指标

| 指标 | 公式 | 含義 |
|------|------|------|
| MSE | Σ(y-ŷ)²/n | 均方误差 |
| RMSE | √MSE | 均方根误差 |
| MAE | Σ|y-ŷ|/n | 平均绝对误差 |
| R² | 1 - SSE/SST | 拟合优度 |

---

## 💡 學習路徑

### 第一步：数学基礎
```
线性代数 → 微积分 → 概率統計
```

### 第二步：机器學習理论
```
監督學習 → 無監督學習 → 深度學習
```

### 第三步：實踐
```
實現算法 → 调参優化 → 專案應用程式
```

---

## 📚 推荐資源

### 课程
- **Coursera**: Mathematics for Machine Learning
- **3Blue1Brown**: 线性代数本质、神经網路
- **Deep Learning Specialization**: Andrew Ng

### 书籍
- **《数学之美》**：吴军
- **《机器學習》**：周志华（西瓜书）
- **《深度學習》**：Goodfellow（花书）

### 工具
- **NumPy/SciPy**: 科學計算
- **Scikit-learn**: 机器學習库
- **TensorFlow/PyTorch**: 深度學習框架

---

## 🔗 相關主題

- [[高等数学/微积分基礎]] - 微积分基礎
- [[高等数学/线性代数]] - 线性代数
- [[統計学与概率/概率论基礎]] - 概率论基礎
# 方法
