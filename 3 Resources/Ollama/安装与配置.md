---
title: 安装与配置
tags: [ollama, installation]
---

# 安装与配置

## 1. 下载 Ollama
- 前往官方网站 https://ollama.com 并下载对应平台的安装包。
- 安装完成后打开终端（Windows PowerShell、macOS Terminal、Linux Shell），运行 `ollama --version` 检查是否成功安装。

## 2. 添加模型仓库（可选）
```bash
# 添加模型仓库（示例）
ollama pull llama2
```
- 常用模型可以直接通过 `ollama pull <model>` 下载。
- 若想使用自定义模型，请参考官方文档的 **自定义模型** 部分。

## 3. 配置 Ollama 启动参数
- 默认情况下，Ollama 在本地 11434 端口提供 REST API。
- 如需修改端口或其他参数，可编辑配置文件（`~/.ollama/config.yaml`）或使用环境变量：
```bash
export OLLAMA_HOST=0.0.0.0
export OLLAMA_PORT=12345
ollama serve
```

## 4. 验证安装
打开浏览器访问 `http://localhost:11434`（或自定义端口），应显示类似以下 JSON 响应：
```json
{"status":"ready"}
```

## 5. 与 Obsidian 集成
- 在 Obsidian 中安装 **AI Assistant** 插件（或其他支持 Ollama 的插件）。
- 插件设置中填写 API 地址，例如 `http://localhost:11434/api/generate`，并选择使用的模型（如 `llama2`）。

> 📌 **提示**：首次使用时建议先在终端手动调用一次 API，确认返回正常后再在插件中配置。
