---
title: 系统要求
date: 2026-01-22
tags: [环境准备, 系统要求, 硬件配置, 软件配置]
para: projects
status: in-progress
language: zh-cn
---

# 系统要求

> **章节**: 01-环境准备 | **难度**: ⭐ | **预计时间**: 30分钟

在开始部署 Obsidian + OpenCode + Ollama + GPT 集成系统之前，请确保您的环境满足以下要求。

---

## 💻 硬件要求

### 最低配置

| 组件 | 最低配置 | 说明 |
|-----|----------|------|
| **CPU** | 4核心 | 支持基本推理 |
| **内存** | 8GB | 适合小型模型 |
| **存储** | 50GB SSD | 模型和数据存储 |
| **GPU** | 可选 | CPU 推理可用 |

**适用场景**:
- ✅ 仅使用云端模型
- ✅ 小型本地模型（7B 以下）
- ✅ 基础知识管理
- ❌ 复杂代码生成
- ❌ 大型本地模型推理

### 推荐配置

| 组件 | 推荐配置 | 说明 |
|-----|----------|------|
| **CPU** | 8核心+ | 推理更流畅 |
| **内存** | 16GB+ | 支持中型模型 |
| **存储** | 100GB+ NVMe SSD | 快速读写 |
| **GPU** | RTX 3060 12GB+ | 加速推理 |

**适用场景**:
- ✅ 混合部署（本地+云端）
- ✅ 中型本地模型（7B-13B）
- ✅ 知识管理 + 代码开发
- ✅ 实时 AI 交互

### 高性能配置

| 组件 | 高性能配置 | 说明 |
|-----|-----------|------|
| **CPU** | 12核心+ | 如 i7/i9 或 Ryzen 7/9 |
| **内存** | 32GB+ | 支持大型模型 |
| **存储** | 200GB+ NVMe SSD | 模型库 + 数据 |
| **GPU** | RTX 4090 24GB | 大模型推理 |

**适用场景**:
- ✅ 大型本地模型（13B+）
- ✅ 完全本地化部署
- ✅ 复杂 AI 任务
- ✅ 多模型并发

---

## 🛠️ 软件要求

### 操作系统

| 系统 | 最低版本 | 推荐版本 | 兼容性 |
|-----|---------|---------|--------|
| **Windows** | Windows 10 | Windows 11 | ⭐⭐⭐⭐⭐ |
| **macOS** | macOS 11 | macOS 14 | ⭐⭐⭐⭐ |
| **Linux** | Ubuntu 20.04 | Ubuntu 24.04 | ⭐⭐⭐⭐⭐ |

### 必需软件

| 软件 | 最低版本 | 推荐版本 | 用途 |
|-----|---------|---------|------|
| **Obsidian** | 0.15.0 | 1.0.0+ | 知识管理 |
| **Node.js** | 16.x | 20.x LTS | 某些插件依赖 |
| **Git** | 2.x | 最新版 | 版本控制 |
| **Python** | 3.8 | 3.10+ | Ollama 某些功能 |
| **Docker** | 20.10 | 24.0+ | 容器化部署（可选）|

### 可选软件

| 软件 | 用途 | 必要性 |
|-----|------|-------|
| **VS Code** | 代码编辑 | 推荐 |
| **Docker Desktop** | 容器管理 | 可选 |
| **Postman** | API 测试 | 可选 |
| **Git GUI** | Git 客户端 | 可选 |

---

## 🌐 网络要求

### 带宽要求

| 使用场景 | 最低带宽 | 推荐带宽 |
|---------|---------|---------|
| **仅本地模型** | - | - |
| **云端模型** | 10 Mbps | 50 Mbps+ |
| **大文件下载** | 20 Mbps | 100 Mbps+ |

### 延迟要求

| 交互类型 | 最高延迟 | 推荐延迟 |
|---------|---------|---------|
| **实时对话** | 200ms | <100ms |
| **代码生成** | 500ms | <200ms |
| **批量处理** | 1000ms | <500ms |

### 稳定性要求

- **可用性**: 99%+（云端服务）
- **丢包率**: <0.1%
- **网络类型**: 宽带/Wi-Fi 5G/有线网络

---

## 🎯 配置建议

### 根据使用场景选择

#### 场景 1: 知识管理为主

**配置**: 最低配置 + 云端模型
```
CPU: 4核心
内存: 8GB
存储: 50GB SSD
网络: 10 Mbps+
```

**适用**:
- 以笔记整理为主
- 少量代码辅助
- 预算有限

#### 场景 2: 知识 + 开发均衡

**配置**: 推荐配置 + 混合部署
```
CPU: 8核心+
内存: 16GB+
存储: 100GB+ NVMe SSD
GPU: RTX 3060 12GB+
网络: 50 Mbps+
```

**适用**:
- 知识管理 + 代码开发
- 本地 + 云端混合
- 性价比最优

#### 场景 3: 完全本地化

**配置**: 高性能配置
```
CPU: 12核心+
内存: 32GB+
存储: 200GB+ NVMe SSD
GPU: RTX 4090 24GB
网络: 100 Mbps+（用于下载）
```

**适用**:
- 隐私要求高
- 复杂 AI 任务
- 预算充足

---

## 🔍 环境检查清单

在继续之前，请检查以下项目：

### 硬件检查

- [ ] CPU 核心数 ≥ 4
- [ ] 内存 ≥ 8GB
- [ ] 可用存储空间 ≥ 50GB
- [ ] （可选）GPU 已安装驱动

### 软件检查

- [ ] 操作系统符合要求
- [ ] Obsidian 已安装或可安装
- [ ] Node.js 已安装（`node --version`）
- [ ] Git 已安装（`git --version`）
- [ ] Python 已安装（`python --version`）
- [ ] （可选）Docker 已安装（`docker --version`）

### 网络检查

- [ ] 网络连接正常
- [ ] 带宽符合要求
- [ ] （如使用云端）防火墙允许 AI API 访问

---

## ⚠️ 常见问题

### Q1: 我的电脑配置较低，还能用吗？

**A**: 可以！建议：
- 使用云端模型（GPT-oss:120b-cloud）
- 或使用小型本地模型（如 Llama-7B）
- 适合知识管理场景

### Q2: 没有 GPU，能运行本地模型吗？

**A**: 可以，但：
- 仅使用 CPU 推理，速度较慢
- 建议使用小型模型
- 或直接使用云端模型

### Q3: 存储空间不够怎么办？

**A**: 可以：
- 只安装需要的模型
- 使用云端模型作为主力
- 定期清理旧模型和数据

### Q4: 网络不稳定怎么办？

**A**: 建议：
- 优先使用本地模型
- 本地模型不需要网络
- 仅在下载模型时需要网络

---

## 📊 性能参考

### 模型推理速度参考

| 硬件配置 | 模型大小 | 推理速度 |
|----------|---------|---------|
| CPU 4核 | 7B | 5-10 tokens/s |
| CPU 8核 | 7B | 10-20 tokens/s |
| RTX 3060 | 7B | 50-100 tokens/s |
| RTX 3060 | 13B | 25-50 tokens/s |
| RTX 4090 | 7B | 200+ tokens/s |
| RTX 4090 | 13B | 100+ tokens/s |

### 存储空间需求

| 组件 | 大小 |
|-----|------|
| Obsidian + 插件 | 500MB |
| OpenCode | 200MB |
| Ollama | 100MB |
| Llama-7B 模型 | 4GB |
| Llama-13B 模型 | 8GB |
| 知识库数据 | 1-10GB（取决于内容）|
| **总计（推荐）** | **15-20GB** |

---

## 🚀 下一步

环境确认无误后，继续下一步：

→ [[基础配置]] - 配置 Obsidian

---

## 📚 相关资源

- [[3 Resources/工作技巧/Obsidian-OpenCode-Ollama-GPT集成指南/README]] - 返回主目录
- [[基础配置]] - 下一章
- [[README.md]] - 返回仓库主页

---

**更新时间**: 2026-01-22 | **版本**: v1.0
