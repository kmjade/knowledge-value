---
title: é«˜çº§è‡ªåŠ¨åŒ–é…ç½®
date: 2026-01-22
tags: [é«˜çº§åº”ç”¨, è‡ªåŠ¨åŒ–, CI/CD, å·¥ä½œæµä¼˜åŒ–]
para: projects
status: in-progress
language: zh-cn
---

# é«˜çº§è‡ªåŠ¨åŒ–é…ç½®

> **ç« èŠ‚**: 08-é«˜çº§åº”ç”¨ | **éš¾åº¦**: â­â­â­â­â­ | **é¢„è®¡æ—¶é—´**: 180 åˆ†é’Ÿ

## ğŸ“– æ¦‚è¿°

æœ¬æŒ‡å—å°†æ·±å…¥ä»‹ç»å¦‚ä½•ä¸º Obsidian + OpenCode + Ollama + GPT é›†æˆç³»ç»Ÿé…ç½®é«˜çº§è‡ªåŠ¨åŒ–åŠŸèƒ½ï¼ŒåŒ…æ‹¬ CI/CD æµç¨‹ã€æ™ºèƒ½è§¦å‘å™¨ã€è‡ªåŠ¨åŒ–æµ‹è¯•ç­‰ï¼Œå®ç°çœŸæ­£çš„ AI é©±åŠ¨å·¥ä½œæµã€‚

---

## ğŸ¯ è‡ªåŠ¨åŒ–å±‚çº§æ¶æ„

### å››å±‚è‡ªåŠ¨åŒ–ä½“ç³»

```mermaid
graph TB
    subgraph "è§¦å‘å±‚"
        A[å®šæ—¶ä»»åŠ¡]
        B[æ–‡ä»¶ç›‘æ§]
        C[äº‹ä»¶è§¦å‘]
        D[API è°ƒç”¨]
    end

    subgraph "å¤„ç†å±‚"
        E[AI æ¨ç†å¼•æ“]
        F[å·¥ä½œæµç¼–æ’]
        G[æ•°æ®ç®¡é“]
    end

    subgraph "é›†æˆå±‚"
        H[CI/CD æµç¨‹]
        I[å¤–éƒ¨ç³»ç»Ÿé›†æˆ]
        J[å¤šè®¾å¤‡åŒæ­¥]
    end

    subgraph "é€šçŸ¥å±‚"
        K[é‚®ä»¶é€šçŸ¥]
        L[å³æ—¶é€šè®¯]
        M[ä»ªè¡¨ç›˜ç›‘æ§]
    end

    A --> E
    B --> E
    C --> F
    D --> E

    E --> H
    F --> I
    G --> J

    H --> K
    I --> L
    J --> M
```

---

## ğŸ¤– AI æ¨ç†è‡ªåŠ¨åŒ–

### 1. æ™ºèƒ½ä»»åŠ¡è°ƒåº¦

#### åŸºäºæ—¶é—´çš„è°ƒåº¦

```yaml
# è‡ªåŠ¨åŒ–é…ç½®æ–‡ä»¶
scheduler_config:
  daily_tasks:
    - name: "æ¯æ—¥çŸ¥è¯†æ•´ç†"
      time: "08:00"
      trigger: "cron"
      workflow: "para_organize"

    - name: "æ—¥æŠ¥ç”Ÿæˆ"
      time: "18:00"
      trigger: "cron"
      workflow: "daily_report"

  weekly_tasks:
    - name: "å‘¨åº¦çŸ¥è¯†å›é¡¾"
      time: "å‘¨æ—¥ 09:00"
      trigger: "cron"
      workflow: "weekly_review"

  monthly_tasks:
    - name: "æœˆåº¦å½’æ¡£"
      day: 1
      time: "10:00"
      trigger: "cron"
      workflow: "monthly_archive"
```

#### åŸºäºäº‹ä»¶çš„è§¦å‘

```yaml
event_triggers:
  file_changes:
    - pattern: "0 Personals/ğŸ“¥ 00_InBox/*.md"
      action: "auto_tag"
      debounce: "5min"

    - pattern: "1 Projects/**/*.md"
      action: "update_project_status"
      debounce: "10min"

  git_commits:
    - branch: "main"
      action: "update_docs"
      on: "push"

  api_calls:
    - endpoint: "/webhooks/task_complete"
      action: "update_knowledge_graph"
```

### 2. AI æ¨¡å‹æ™ºèƒ½è·¯ç”±

#### è‡ªé€‚åº”æ¨¡å‹é€‰æ‹©ç®—æ³•

```python
# æ™ºèƒ½è·¯ç”±å™¨
class AITaskRouter:
    def __init__(self):
        self.models = {
            'local_fast': {
                'name': 'llama2:7b',
                'cost': 0,
                'latency': 50,
                'capacity': 'simple'
            },
            'local_balanced': {
                'name': 'mistral:7b',
                'cost': 0,
                'latency': 80,
                'capacity': 'medium'
            },
            'cloud_powerful': {
                'name': 'gpt-oss:120b-cloud',
                'cost': 0.01,
                'latency': 200,
                'capacity': 'complex'
            }
        }

    def route_task(self, task_info):
        """æ™ºèƒ½è·¯ç”±ä»»åŠ¡åˆ°åˆé€‚çš„æ¨¡å‹"""
        # åˆ†æä»»åŠ¡ç‰¹å¾
        complexity = self._analyze_complexity(task_info)
        urgency = self._check_urgency(task_info)
        budget = self._get_budget_status()

        # å†³ç­–é€»è¾‘
        if urgency == 'high' and budget >= 0.01:
            return self.models['cloud_powerful']
        elif complexity == 'high' and budget >= 0.01:
            return self.models['cloud_powerful']
        elif complexity == 'medium':
            return self.models['local_balanced']
        else:
            return self.models['local_fast']

    def _analyze_complexity(self, task_info):
        """åˆ†æä»»åŠ¡å¤æ‚åº¦"""
        # åŸºäºå†…å®¹é•¿åº¦ã€ç±»å‹ã€ä¸Šä¸‹æ–‡ç­‰åˆ¤æ–­
        pass
```

---

## ğŸ”„ CI/CD è‡ªåŠ¨åŒ–æµç¨‹

### 1. GitHub Actions é…ç½®

#### çŸ¥è¯†åº“è‡ªåŠ¨éƒ¨ç½²

```yaml
# .github/workflows/knowledge-deploy.yml
name: Knowledge Base Deployment

on:
  push:
    branches: [main]
  schedule:
    - cron: '0 9 * * *'  # æ¯å¤© 9:00

jobs:
  deploy-knowledge:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout
        uses: actions/checkout@v3

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      - name: Install Ollama
        run: |
          curl -fsSL https://ollama.com/install.sh | sh
          ollama serve &

      - name: Run AI Analysis
        env:
          API_KEY: ${{ secrets.AI_API_KEY }}
        run: |
          python scripts/ai_analysis.py \
            --input vault/ \
            --output reports/ \
            --model mistral:7b

      - name: Generate Knowledge Graph
        run: |
          python scripts/generate_graph.py \
            --vault vault/ \
            --output assets/graph.json

      - name: Deploy
        run: |
          git config user.name "AI Bot"
          git config user.email "bot@example.com"
          git add reports/ assets/
          git commit -m "Auto-update: AI analysis and graphs"
          git push
```

### 2. Docker Compose è‡ªåŠ¨åŒ–

#### å®Œæ•´æœåŠ¡ç¼–æ’

```yaml
# docker-compose.yml
version: '3.8'

services:
  obsidian-sync:
    image: obsidian/sync-server:latest
    volumes:
      - ./vault:/vault
    environment:
      - SYNC_TOKEN=${OBSIDIAN_SYNC_TOKEN}
    restart: unless-stopped

  ollama:
    image: ollama/ollama:latest
    volumes:
      - ./models:/root/.ollama
    ports:
      - "11434:11434"
    environment:
      - OLLAMA_NUM_PARALLEL=4
      - OLLAMA_MAX_LOADED_MODELS=3
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '4'
          memory: 8G

  ai-processor:
    build: ./services/ai-processor
    volumes:
      - ./vault:/vault
      - ./logs:/logs
    environment:
      - API_BASE=http://ollama:11434
      - CLOUD_API_KEY=${CLOUD_API_KEY}
      - OBSIDIAN_VAULT=/vault
    depends_on:
      - ollama
    restart: unless-stopped

  web-dashboard:
    build: ./services/dashboard
    ports:
      - "3000:3000"
    environment:
      - GRAFANA_URL=http://grafana:3000
    restart: unless-stopped
```

---

## ğŸ“Š æ•°æ®ç®¡é“è‡ªåŠ¨åŒ–

### 1. çŸ¥è¯†æå–ä¸å¤„ç†

#### ETL æµç¨‹

```python
# çŸ¥è¯† ETL ç®¡é“
class KnowledgeETL:
    """çŸ¥è¯†æå–ã€è½¬æ¢ã€åŠ è½½ç®¡é“"""

    def __init__(self, vault_path, ai_client):
        self.vault_path = vault_path
        self.ai_client = ai_client

    def extract(self):
        """ä» Obsidian æå–åŸå§‹æ•°æ®"""
        notes = []
        for file_path in Path(self.vault_path).rglob('*.md'):
            with open(file_path) as f:
                content = f.read()
                notes.append({
                    'path': file_path,
                    'content': content,
                    'metadata': self._extract_metadata(content)
                })
        return notes

    def transform(self, notes):
        """ä½¿ç”¨ AI è½¬æ¢å’Œå¢å¼ºæ•°æ®"""
        enhanced_notes = []
        for note in notes:
            # AI å¢å¼ºå¤„ç†
            analysis = self.ai_client.analyze(
                content=note['content'],
                tasks=['summarize', 'extract_tags', 'find_relations']
            )

            enhanced_notes.append({
                **note,
                'summary': analysis['summary'],
                'tags': analysis['tags'],
                'relations': analysis['relations']
            })

        return enhanced_notes

    def load(self, enhanced_notes):
        """åŠ è½½åˆ°çŸ¥è¯†å›¾è°±"""
        # æ„å»ºå›¾è°±
        graph = self._build_graph(enhanced_notes)

        # ä¿å­˜ä¸ºå¯è§†åŒ–æ ¼å¼
        graph_path = Path(self.vault_path) / 'assets' / 'knowledge_graph.json'
        with open(graph_path, 'w') as f:
            json.dump(graph, f, indent=2)

        return graph_path

    def run_pipeline(self):
        """è¿è¡Œå®Œæ•´ ETL æµç¨‹"""
        print("ğŸ”„ å¼€å§‹ ETL æµç¨‹...")

        # 1. æå–
        print("  1. æå–æ•°æ®...")
        notes = self.extract()
        print(f"     âœ“ æå–äº† {len(notes)} æ¡ç¬”è®°")

        # 2. è½¬æ¢
        print("  2. AI è½¬æ¢...")
        enhanced_notes = self.transform(notes)
        print(f"     âœ“ å¢å¼ºäº† {len(enhanced_notes)} æ¡ç¬”è®°")

        # 3. åŠ è½½
        print("  3. æ„å»ºçŸ¥è¯†å›¾è°±...")
        graph_path = self.load(enhanced_notes)
        print(f"     âœ“ çŸ¥è¯†å›¾è°±å·²ä¿å­˜åˆ° {graph_path}")

        print("âœ… ETL æµç¨‹å®Œæˆï¼")
```

### 2. å®æ—¶æ•°æ®æµ

#### Webhook é›†æˆ

```python
# Webhook æœåŠ¡å™¨
from fastapi import FastAPI, BackgroundTasks
from pydantic import BaseModel

app = FastAPI()

class NoteEvent(BaseModel):
    event_type: str
    note_path: str
    timestamp: datetime

@app.post("/webhooks/note")
async def handle_note_event(event: NoteEvent, background_tasks: BackgroundTasks):
    """å¤„ç†ç¬”è®°äº‹ä»¶"""

    # å¼‚æ­¥å¤„ç†
    background_tasks.add_task(
        process_note_event,
        event.event_type,
        event.note_path
    )

async def process_note_event(event_type: str, note_path: str):
    """å¼‚æ­¥å¤„ç†ç¬”è®°äº‹ä»¶"""

    if event_type == 'created':
        # æ–°ç¬”è®°åˆ›å»ºï¼šè‡ªåŠ¨æ‰“æ ‡ç­¾
        await auto_tag_note(note_path)

    elif event_type == 'updated':
        # ç¬”è®°æ›´æ–°ï¼šæ›´æ–°çŸ¥è¯†å›¾è°±
        await update_knowledge_graph(note_path)

    elif event_type == 'linked':
        # ç¬”è®°é“¾æ¥ï¼šå¢å¼ºå…³è”åˆ†æ
        await analyze_links(note_path)

# å¯åŠ¨æœåŠ¡å™¨
if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

---

## ğŸš¨ æ™ºèƒ½ç›‘æ§ä¸å‘Šè­¦

### 1. ç³»ç»Ÿç›‘æ§ä»ªè¡¨ç›˜

#### Grafana é…ç½®

```json
// Grafana Dashboard é…ç½®
{
  "dashboard": {
    "title": "AI System Monitor",
    "panels": [
      {
        "title": "API è°ƒç”¨æ¬¡æ•°",
        "targets": [
          {
            "expr": "sum(rate(api_calls_total[5m]))",
            "legendFormat": "calls/sec"
          }
        ]
      },
      {
        "title": "æ¨ç†å»¶è¿Ÿ",
        "targets": [
          {
            "expr": "avg(inference_latency_ms)",
            "legendFormat": "ms"
          }
        ]
      },
      {
        "title": "æ¨¡å‹ä½¿ç”¨åˆ†å¸ƒ",
        "type": "piechart",
        "targets": [
          {
            "expr": "count by (model) (inference_requests)"
          }
        ]
      },
      {
        "title": "çŸ¥è¯†åº“å¢é•¿",
        "type": "graph",
        "targets": [
          {
            "expr": "knowledge_notes_total",
            "legendFormat": "notes"
          }
        ]
      }
    ]
  }
}
```

### 2. æ™ºèƒ½å‘Šè­¦ç³»ç»Ÿ

#### å‘Šè­¦è§„åˆ™

```yaml
# alerting_rules.yml
alerting:
  rules:
    - name: "high_latency"
      condition: "avg(inference_latency_ms) > 5000"
      duration: "5m"
      severity: "warning"
      notification:
        - email
        - slack
      message: "AI æ¨ç†å»¶è¿Ÿè¶…è¿‡ 5 ç§’ï¼"

    - name: "api_error_rate"
      condition: "rate(api_errors_total[5m]) > 0.1"
      duration: "2m"
      severity: "critical"
      notification:
        - pagerduty
        - slack
      message: "API é”™è¯¯ç‡è¿‡é«˜ï¼"

    - name: "disk_space_low"
      condition: "disk_available_percent < 10"
      duration: "10m"
      severity: "warning"
      notification:
        - email
      message: "ç£ç›˜ç©ºé—´ä¸è¶³ 10%ï¼"

    - name: "model_not_loaded"
      condition: "model_loaded_count < 1"
      duration: "1m"
      severity: "critical"
      notification:
        - slack
      message: "æ²¡æœ‰åŠ è½½ä»»ä½•æ¨¡å‹ï¼"
```

---

## ğŸ§ª è‡ªåŠ¨åŒ–æµ‹è¯•

### 1. é›†æˆæµ‹è¯•å¥—ä»¶

```python
# è‡ªåŠ¨åŒ–æµ‹è¯•
import pytest
from ai_integration import AIClient, KnowledgeBase

@pytest.fixture
def ai_client():
    return AIClient(api_key="test_key")

@pytest.fixture
def knowledge_base():
    return KnowledgeBase(path="./test_vault")

class TestAIIntegration:
    """AI é›†æˆæµ‹è¯•"""

    def test_note_creation(self, ai_client):
        """æµ‹è¯•ç¬”è®°åˆ›å»º"""
        result = ai_client.create_note(
            title="æµ‹è¯•ç¬”è®°",
            content="è¿™æ˜¯æµ‹è¯•å†…å®¹"
        )
        assert result['status'] == 'success'
        assert 'note_id' in result

    def test_smart_search(self, ai_client, knowledge_base):
        """æµ‹è¯•æ™ºèƒ½æœç´¢"""
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        knowledge_base.create_test_notes()

        # æ‰§è¡Œæœç´¢
        results = ai_client.smart_search(query="æœºå™¨å­¦ä¹ ")

        assert len(results) > 0
        assert all('relevance_score' in r for r in results)

    def test_model_routing(self, ai_client):
        """æµ‹è¯•æ¨¡å‹è·¯ç”±"""
        # ç®€å•ä»»åŠ¡åº”è¯¥ç”¨æœ¬åœ°æ¨¡å‹
        result1 = ai_client.complete(
            prompt="ä½ å¥½",
            auto_route=True
        )
        assert result1['model_type'] == 'local'

        # å¤æ‚ä»»åŠ¡åº”è¯¥ç”¨äº‘ç«¯æ¨¡å‹
        result2 = ai_client.complete(
            prompt="è¯¦ç»†åˆ†æè¿™ç¯‡è®ºæ–‡çš„æ ¸å¿ƒè´¡çŒ®",
            auto_route=True
        )
        assert result2['model_type'] == 'cloud'

    def test_automated_organization(self, knowledge_base):
        """æµ‹è¯•è‡ªåŠ¨ç»„ç»‡"""
        # æ·»åŠ æ··ä¹±çš„ç¬”è®°
        knowledge_base.add_messy_notes()

        # è¿è¡Œè‡ªåŠ¨ç»„ç»‡
        knowledge_base.auto_organize()

        # éªŒè¯åˆ†ç±»
        categories = knowledge_base.get_categories()
        assert 'Projects' in categories
        assert 'Areas' in categories
        assert 'Resources' in categories

# è¿è¡Œæµ‹è¯•
if __name__ == "__main__":
    pytest.main([
        "--verbose",
        "--cov=ai_integration",
        "tests/"
    ])
```

### 2. æ€§èƒ½åŸºå‡†æµ‹è¯•

```python
# æ€§èƒ½åŸºå‡†æµ‹è¯•
import time
import statistics

class PerformanceBenchmark:
    """æ€§èƒ½åŸºå‡†æµ‹è¯•"""

    def benchmark_inference_speed(self, model_name, prompts):
        """æµ‹è¯•æ¨ç†é€Ÿåº¦"""
        latencies = []

        for prompt in prompts:
            start = time.time()
            result = self.ai_client.complete(
                model=model_name,
                prompt=prompt
            )
            latency = time.time() - start
            latencies.append(latency)

        return {
            'model': model_name,
            'avg_latency': statistics.mean(latencies),
            'p95_latency': statistics.quantiles(latencies, n=20)[18],
            'p99_latency': statistics.quantiles(latencies, n=20)[19],
            'tokens_per_second': self._calculate_tps(latencies, prompts)
        }

    def benchmark_throughput(self, concurrent_requests=10):
        """æµ‹è¯•ååé‡"""
        import concurrent.futures

        start_time = time.time()

        with concurrent.futures.ThreadPoolExecutor(max_workers=concurrent_requests) as executor:
            futures = [
                executor.submit(self.ai_client.complete, prompt="test")
                for _ in range(concurrent_requests)
            ]
            concurrent.futures.wait(futures)

        total_time = time.time() - start_time

        return {
            'concurrent_requests': concurrent_requests,
            'total_time': total_time,
            'requests_per_second': concurrent_requests / total_time
        }
```

---

## ğŸ¯ æœ€ä½³å®è·µ

### 1. è‡ªåŠ¨åŒ–é…ç½®åŸåˆ™

```yaml
automation_principles:
  æ¸è¿›å¼éƒ¨ç½²:
    - "ä»ç®€å•ä»»åŠ¡å¼€å§‹"
    - "é€æ­¥å¢åŠ å¤æ‚åº¦"
    - "æ¯ä¸ªé˜¶æ®µéªŒè¯"

  å®¹é”™è®¾è®¡:
    - "æ·»åŠ é‡è¯•æœºåˆ¶"
    - "å®ç°é™çº§ç­–ç•¥"
    - "è®°å½•è¯¦ç»†æ—¥å¿—"

  å¯è§‚æµ‹æ€§:
    - "ç›‘æ§æ‰€æœ‰è‡ªåŠ¨åŒ–æµç¨‹"
    - "æ”¶é›†å…³é”®æŒ‡æ ‡"
    - "è®¾ç½®åˆç†å‘Šè­¦"

  æ–‡æ¡£åŒ–:
    - "è®°å½•æ‰€æœ‰è‡ªåŠ¨åŒ–é…ç½®"
    - "ç»´æŠ¤è¿è¡Œæ‰‹å†Œ"
    - "æ›´æ–°æ•…éšœæ’é™¤æŒ‡å—"
```

### 2. èµ„æºä¼˜åŒ–

```yaml
resource_optimization:
  æ¨¡å‹åŠ è½½:
    - "æŒ‰éœ€åŠ è½½æ¨¡å‹"
    - "å¸è½½ä¸å¸¸ç”¨æ¨¡å‹"
    - "ä½¿ç”¨æ¨¡å‹é‡åŒ–"

  å†…å­˜ç®¡ç†:
    - "é™åˆ¶å¹¶å‘è¯·æ±‚æ•°"
    - "å®ç°ç¼“å­˜ç­–ç•¥"
    - "å®šæœŸæ¸…ç†ç¼“å­˜"

  è®¡ç®—èµ„æº:
    - "æ ¹æ®ä»»åŠ¡ç±»å‹åˆ†é…èµ„æº"
    - "å®ç°ä»»åŠ¡é˜Ÿåˆ—"
    - "ä½¿ç”¨æ‰¹å¤„ç†"
```

---

## ğŸš€ å®æ–½è·¯çº¿å›¾

### é˜¶æ®µä¸€ï¼šåŸºç¡€è‡ªåŠ¨åŒ–ï¼ˆ1-2å‘¨ï¼‰

- [ ] é…ç½®å®šæ—¶ä»»åŠ¡
- [ ] è®¾ç½®äº‹ä»¶è§¦å‘å™¨
- [ ] å®ç°åŸºç¡€é€šçŸ¥
- [ ] éƒ¨ç½²ç›‘æ§ä»ªè¡¨ç›˜

### é˜¶æ®µäºŒï¼šé›†æˆè‡ªåŠ¨åŒ–ï¼ˆ2-4å‘¨ï¼‰

- [ ] é…ç½® CI/CD æµç¨‹
- [ ] å®ç° ETL æ•°æ®ç®¡é“
- [ ] é›†æˆå¤–éƒ¨ç³»ç»Ÿ
- [ ] å»ºç«‹è‡ªåŠ¨åŒ–æµ‹è¯•

### é˜¶æ®µä¸‰ï¼šæ™ºèƒ½è‡ªåŠ¨åŒ–ï¼ˆ4-8å‘¨ï¼‰

- [ ] å®ç°æ™ºèƒ½æ¨¡å‹è·¯ç”±
- [ ] éƒ¨ç½²é«˜çº§ç›‘æ§ç³»ç»Ÿ
- [ ] ä¼˜åŒ–èµ„æºè°ƒåº¦
- [ ] å»ºç«‹è‡ªåŠ¨åŒ–å†³ç­–

---

## ğŸ”— ç›¸å…³é“¾æ¥

```
è‡ªåŠ¨åŒ–.md
    â†“
â† 07-æ•…éšœæ’é™¤/æ•…éšœæ’é™¤èˆ‡ç¶­è­·.md
â†’ 09-èµ„æºä¸‹è½½/å·¥å…·åˆ—è¡¨.md
â†” README.md (ä¸»ç›®å½•)
```

---

## ğŸ“š å‚è€ƒèµ„æº

- **è‡ªåŠ¨åŒ–å·¥å…·**: Ansible, Terraform, GitHub Actions
- **ç›‘æ§å·¥å…·**: Grafana, Prometheus, Datadog
- **CI/CD**: Jenkins, GitLab CI, GitHub Actions
- **æµ‹è¯•æ¡†æ¶**: Pytest, Jest, Cypress

---

**æ›´æ–°æ—¶é—´**: 2026-01-22 | **ç‰ˆæœ¬**: v1.0
