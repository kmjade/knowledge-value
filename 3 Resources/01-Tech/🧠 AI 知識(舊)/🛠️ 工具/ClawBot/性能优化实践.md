---
title: æ•ˆèƒ½å„ªåŒ–å¯¦è¸
status: active
priority: high
tags: [clawdbot, performance, optimization, best-practices]
aliases: [æ•ˆèƒ½å„ªåŒ–, Performance Guide]
created: 2026-01-31
modified: 2026-01-31
---

# æ•ˆèƒ½å„ªåŒ–å¯¦è¸

# æŒ‡å—

## ğŸ“‹ ç›®éŒ„

# åˆ†æ
- [å¼‚æ­¥ç·¨ç¨‹å„ªåŒ–](#å¼‚æ­¥ç·¨ç¨‹å„ªåŒ–)
- [è³‡æ–™åº«å„ªåŒ–](#è³‡æ–™åº«å„ªåŒ–)
- [ç¼“å­˜ç­–ç•¥](#ç¼“å­˜ç­–ç•¥)
- [ç¶²è·¯å„ªåŒ–](#ç¶²è·¯å„ªåŒ–)
- [è¨˜æ†¶é«”å„ªåŒ–](#è¨˜æ†¶é«”å„ªåŒ–)
- [å¹¶å‘æ§åˆ¶](#å¹¶å‘æ§åˆ¶)
- [ç›£æ§ä¸è°ƒä¼˜](#ç›£æ§ä¸è°ƒä¼˜)
- [å„ªåŒ–æ¡ˆä¾‹](#å„ªåŒ–æ¡ˆä¾‹)

---

# åˆ†æ

### æ•ˆèƒ½æŒ‡æ ‡

| æŒ‡æ ‡ | ç›®æ¨™å€¼ | èªªæ˜ |
|------|-------|------|
| **éŸ¿æ‡‰æ™‚é–“** | < 100ms | API éŸ¿æ‡‰æ™‚é–“ |
| **ååé‡** | > 1000 req/s | æ¯ç§’è™•ç†è¯·æ±‚æ•° |
| **é”™è¯¯ç‡** | < 0.1% | é”™è¯¯è¯·æ±‚æ¯”ä¾‹ |
| **CPU ä½¿ç”¨ç‡** | < 70% | æ­£å¸¸è´Ÿè½½ä¸‹ |
| **è¨˜æ†¶é«”ä½¿ç”¨** | < 2GB | å•ä¸ªå®ä¾‹ |
| **è³‡æ–™åº«æŸ¥è©¢æ™‚é–“** | < 10ms | å¹³å‡æŸ¥è©¢æ™‚é–“ |

# åˆ†æ

```python
import time
import cProfile
import pstats
from functools import wraps
from typing import Callable, Any

# 1. ç®€å•çš„æ•ˆèƒ½è®¡æ—¶å™¨
def timer(func: Callable) -> Callable:
    """å‡½æ•°æ‰§è¡Œæ™‚é–“è£…é¥°å™¨"""
    @wraps(func)
    async def async_wrapper(*args, **kwargs):
        start = time.perf_counter()
        result = await func(*args, **kwargs)
        end = time.perf_counter()

        duration = (end - start) * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
        print(f"{func.__name__} æ‰§è¡Œæ™‚é–“: {duration:.2f}ms")

        return result

    @wraps(func)
    def sync_wrapper(*args, **kwargs):
        start = time.perf_counter()
        result = func(*args, **kwargs)
        end = time.perf_counter()

        duration = (end - start) * 1000
        print(f"{func.__name__} æ‰§è¡Œæ™‚é–“: {duration:.2f}ms")

        return result

    return async_wrapper if asyncio.iscoroutinefunction(func) else sync_wrapper

# åˆ†æ
import tracemalloc

class MemoryProfiler:
# åˆ†æ

    @staticmethod
    def start():
        """é–‹å§‹è¨˜æ†¶é«”è·Ÿè¸ª"""
        tracemalloc.start()

    @staticmethod
    def stop():
# é¡¯ç¤º
        snapshot = tracemalloc.take_snapshot()
        top_stats = snapshot.statistics('lineno')

        print("è¨˜æ†¶é«”ä½¿ç”¨ Top 10:")
        for stat in top_stats[:10]:
            print(stat)

        tracemalloc.stop()

# åˆ†æ
def profile_cpu(func: Callable) -> Callable:
# åˆ†æ
    @wraps(func)
    def wrapper(*args, **kwargs):
        profiler = cProfile.Profile()
        profiler.enable()

        result = func(*args, **kwargs)

        profiler.disable()
        stats = pstats.Stats(profiler)
        stats.sort_stats('cumulative')
# é¡¯ç¤º

        return result

    return wrapper

# ä½¿ç”¨ç¤ºä¾‹
@timer
@profile_cpu
async def expensive_operation():
    """è€—æ—¶æ“ä½œ"""
    # ... ä¸šåŠ¡é€»è¾‘
    pass

# åˆ†æ
MemoryProfiler.start()
await expensive_operation()
MemoryProfiler.stop()
```

### æ•ˆèƒ½åŸºå‡†æ¸¬è©¦

```python
import timeit
from statistics import mean, stdev

class PerformanceBenchmark:
    """æ•ˆèƒ½åŸºå‡†æ¸¬è©¦"""

    def __init__(self, iterations: int = 100):
        self.iterations = iterations

    def benchmark(self, func: Callable, *args, **kwargs) -> dict:
        """é‹è¡ŒåŸºå‡†æ¸¬è©¦"""

        # é¢„çƒ­
        for _ in range(5):
            func(*args, **kwargs)

        # å®é™…æ¸¬è©¦
        times = timeit.repeat(
            lambda: func(*args, **kwargs),
            number=self.iterations,
            repeat=10
        )

        # è¨ˆç®—çµ±è¨ˆæ•¸æ“š
        avg_time = mean(times) / self.iterations * 1000  # æ¯«ç§’
        std_dev = stdev(times) / self.iterations * 1000
        min_time = min(times) / self.iterations * 1000
        max_time = max(times) / self.iterations * 1000

        return {
            "average_ms": avg_time,
            "std_dev_ms": std_dev,
            "min_ms": min_time,
            "max_ms": max_time,
            "iterations": self.iterations
        }

    def compare(self, func1: Callable, func2: Callable, *args, **kwargs):
        """æ¯”è¾ƒä¸¤ä¸ªå‡½æ•°çš„æ•ˆèƒ½"""

        result1 = self.benchmark(func1, *args, **kwargs)
        result2 = self.benchmark(func2, *args, **kwargs)

        improvement = ((result1["average_ms"] - result2["average_ms"]) /
                      result1["average_ms"] * 100)

        return {
            "func1": result1,
            "func2": result2,
            "improvement_percent": improvement,
            "faster_func": "func2" if improvement > 0 else "func1"
        }

# ä½¿ç”¨ç¤ºä¾‹
benchmark = PerformanceBenchmark(iterations=1000)

# æ¸¬è©¦å‡½æ•°A
async def version_a(items: list):
    return [item * 2 for item in items]

# æ¸¬è©¦å‡½æ•°B
async def version_b(items: list):
    return list(map(lambda x: x * 2, items))

# æ¯”è¾ƒæ•ˆèƒ½
test_data = list(range(1000))
result = benchmark.compare(version_a, version_b, test_data)

print(f"æ•ˆèƒ½æå‡: {result['improvement_percent']:.2f}%")
# ç‰ˆæœ¬
```

---

## å¼‚æ­¥ç·¨ç¨‹å„ªåŒ–

### å¼‚æ­¥æœ€ä½³å¯¦è¸

```python
import asyncio
import aiohttp
from typing import List, Dict

class AsyncOptimizer:
    """å¼‚æ­¥å„ªåŒ–å™¨"""

    # âœ… å¥½çš„å¯¦è¸ï¼šä½¿ç”¨ asyncio.gather å¹¶å‘æ‰§è¡Œ
    async def fetch_multiple_urls_concurrent(self, urls: List[str]) -> List[Dict]:
        """å¹¶å‘è·å–å¤šå€‹URL"""
        async with aiohttp.ClientSession() as session:
            tasks = [self.fetch_url(session, url) for url in urls]
            results = await asyncio.gather(*tasks, return_exceptions=True)

        # è™•ç†çµæœå’Œå¼‚å¸¸
        successful_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                print(f"URL {urls[i]} å¤±è´¥: {result}")
            else:
                successful_results.append(result)

        return successful_results

    # âŒ ä¸å¥½çš„å¯¦è¸ï¼šä¸²è¡Œæ‰§è¡Œ
    async def fetch_multiple_urls_sequential(self, urls: List[str]) -> List[Dict]:
        """ä¸²è¡Œè·å–å¤šå€‹URLï¼ˆæ…¢ï¼‰"""
        results = []
        for url in urls:
            result = await self.fetch_url(session=None, url=url)
            results.append(result)

        return results

    # ä½¿ç”¨ä¿¡å·é‡é™åˆ¶å¹¶å‘
    async def fetch_with_semaphore(self, urls: List[str], max_concurrent: int = 10):
        """ä½¿ç”¨ä¿¡å·é‡é™åˆ¶å¹¶å‘æ•°é‡"""
        semaphore = asyncio.Semaphore(max_concurrent)

        async def fetch_with_limit(session, url):
            async with semaphore:
                return await self.fetch_url(session, url)

        async with aiohttp.ClientSession() as session:
            tasks = [fetch_with_limit(session, url) for url in urls]
            results = await asyncio.gather(*tasks)

        return results

    # ä½¿ç”¨å¼‚æ­¥ç”Ÿæˆå™¨è™•ç†å¤§æ•¸æ“š
    async def process_large_dataset(self, source):
        """ä½¿ç”¨å¼‚æ­¥ç”Ÿæˆå™¨è™•ç†å¤§æ•¸æ“š"""
        async for batch in self.yield_batches(source, batch_size=100):
            # è™•ç†æ‰¹æ¬¡
            processed = await self.process_batch(batch)

            # ç™¼é€è¿›åº¦
            yield processed

    async def yield_batches(self, source, batch_size: int):
        """å¼‚æ­¥ç”Ÿæˆæ‰¹æ¬¡æ•¸æ“š"""
        batch = []
        async for item in source:
            batch.append(item)
            if len(batch) >= batch_size:
                yield batch
                batch = []

        if batch:  # è™•ç†å‰©ä½™çš„
            yield batch
```

### é¿å…é˜»å¡æ“ä½œ

```python
# âŒ é”™è¯¯ï¼šåœ¨å¼‚æ­¥å‡½æ•°ä¸­ä½¿ç”¨é˜»å¡æ“ä½œ
async def bad_blocking_operation():
    # è¿™äº›æ“ä½œä¼šé˜»å¡äº‹ä»¶å¾ªç¯
    time.sleep(1)  # é˜»å¡
    result = requests.get("https://api.example.com")  # é˜»å¡
    data = json.loads(large_json_string)  # å¯èƒ½é˜»å¡

# âœ… æ­£ç¡®ï¼šä½¿ç”¨å¼‚æ­¥æ›¿ä»£æ–¹æ¡ˆ
async def good_async_operation():
    # ä½¿ç”¨ asyncio.sleep æ›¿ä»£ time.sleep
    await asyncio.sleep(1)

    # ä½¿ç”¨ aiohttp æ›¿ä»£ requests
    async with aiohttp.ClientSession() as session:
        async with session.get("https://api.example.com") as response:
            result = await response.json()

    # ä½¿ç”¨ orjson æ›¿ä»£ jsonï¼ˆæ›´å¿«ï¼‰
    import orjson
    data = orjson.loads(large_json_string)

# å¯¹äºCPUå¯†é›†å‹ä»»å‹™ï¼Œä½¿ç”¨çº¿ç¨‹æ± æˆ–è¿›ç¨‹æ± 
async def cpu_intensive_task(data):
    """CPUå¯†é›†å‹ä»»å‹™"""
    loop = asyncio.get_running_loop()

    # ä½¿ç”¨ run_in_executor ç·šä¸Šç¨‹æ± ä¸­æ‰§è¡Œ
    result = await loop.run_in_executor(
        None,  # ä½¿ç”¨é»˜è®¤çº¿ç¨‹æ± 
        heavy_computation,
        data
    )

    return result
```

---

## è³‡æ–™åº«å„ªåŒ–

### æŸ¥è©¢å„ªåŒ–

```python
from sqlalchemy import text
from typing import List

class DatabaseOptimizer:
    """è³‡æ–™åº«å„ªåŒ–å™¨"""

    # âœ… å¥½çš„å¯¦è¸ï¼šä½¿ç”¨ç´¢å¼•
    async def get_user_by_email_optimized(self, email: str):
        """ä½¿ç”¨ç´¢å¼•æŸ¥è©¢ï¼ˆå¿«ï¼‰"""
        # ç¡®ä¿ email å­—æ®µæœ‰ç´¢å¼•
        query = text("SELECT * FROM users WHERE email = :email")
        result = await self.db.execute(query, {"email": email})
        return result.fetchone()

    # âŒ ä¸å¥½çš„å¯¦è¸ï¼šå…¨è¡¨æ‰«æ
    async def get_user_by_email_slow(self, email: str):
        """å…¨è¡¨æ‰«æï¼ˆæ…¢ï¼‰"""
        # LIKE æŸ¥è©¢ç„¡æ³•ä½¿ç”¨ç´¢å¼•
        query = text("SELECT * FROM users WHERE email LIKE :email")
        result = await self.db.execute(query, {"email": f"%{email}%"})
        return result.fetchall()

    # âœ… ä½¿ç”¨æ‰¹é‡æ“ä½œ
    async def insert_users_batch(self, users: List[Dict]):
        """æ‰¹é‡æ’å…¥ï¼ˆå¿«ï¼‰"""
        query = text("""
            INSERT INTO users (name, email, created_at)
            VALUES (:name, :email, NOW())
        """)

        await self.db.execute_many(query, users)

    # âŒ å¾ªç¯æ’å…¥ï¼ˆæ…¢ï¼‰
    async def insert_users_loop(self, users: List[Dict]):
        """å¾ªç¯æ’å…¥ï¼ˆæ…¢ï¼‰"""
        for user in users:
            query = text("""
                INSERT INTO users (name, email, created_at)
                VALUES (:name, :email, NOW())
            """)
            await self.db.execute(query, user)

    # âœ… ä½¿ç”¨ JOIN å„ªåŒ–å…³è”æŸ¥è©¢
    async def get_users_with_orders_optimized(self, user_ids: List[int]):
        """ä½¿ç”¨JOINå„ªåŒ–å…³è”æŸ¥è©¢"""
        query = text("""
            SELECT u.id, u.name, COUNT(o.id) as order_count
            FROM users u
            LEFT JOIN orders o ON u.id = o.user_id
            WHERE u.id = ANY(:user_ids)
            GROUP BY u.id, u.name
        """)

        result = await self.db.execute(query, {"user_ids": user_ids})
        return result.fetchall()

    # âŒ N+1 æŸ¥è©¢å•é¡Œ
    async def get_users_with_orders_slow(self, user_ids: List[int]):
        """N+1æŸ¥è©¢å•é¡Œï¼ˆæ…¢ï¼‰"""
        users = []
        for user_id in user_ids:
            # æŸ¥è©¢ä½¿ç”¨è€…ï¼ˆ1æ¬¡ï¼‰
            user = await self.get_user(user_id)

            # æŸ¥è©¢è¨‚å–®ï¼ˆNæ¬¡ï¼‰
            orders = await self.get_user_orders(user_id)
            user["orders"] = orders

            users.append(user)

        return users
```

### é€£æ¥æ± å„ªåŒ–

```python
import asyncpg
from contextlib import asynccontextmanager

class ConnectionPoolOptimizer:
    """é€£æ¥æ± å„ªåŒ–"""

    def __init__(self, dsn: str):
        self.dsn = dsn
        self.pool = None

    async def init_pool(self):
        """åˆå§‹åŒ–é€£æ¥æ± """
        self.pool = await asyncpg.create_pool(
            dsn=self.dsn,
            min_size=5,      # æœ€å°é€£æ¥æ•°
            max_size=20,     # æœ€å¤§é€£æ¥æ•°
            max_queries=50000,  # å•ä¸ªé€£æ¥æœ€å¤§æŸ¥è©¢æ•°
            max_inactive_connection_lifetime=300.0,  # æœ€å¤§ç©ºé—²æ™‚é–“
            command_timeout=60.0  # å‘½ä»¤è¶…æ—¶
        )

    @asynccontextmanager
    async def get_connection(self):
# ç®¡ç†
        async with self.pool.acquire() as connection:
            yield connection

    # ä½¿ç”¨ç¤ºä¾‹
    async def query_users(self):
        """æŸ¥è©¢ä½¿ç”¨è€…"""
        async with self.get_connection() as conn:
            rows = await conn.fetch("SELECT * FROM users LIMIT 100")
            return [dict(row) for row in rows]
```

---

## ç¼“å­˜ç­–ç•¥

### å¤šçº§ç¼“å­˜

```python
import asyncio
from typing import Optional, Any
from datetime import timedelta

class MultiLevelCache:
    """å¤šçº§ç¼“å­˜ç³»çµ±"""

    def __init__(self):
        # L1: è¨˜æ†¶é«”ç¼“å­˜ï¼ˆæœ€å¿«ï¼‰
        self.memory_cache = {}

        # L2: Redisç¼“å­˜ï¼ˆä¸­ç­‰é€Ÿåº¦ï¼‰
        self.redis_client = None

        # L3: è³‡æ–™åº«ï¼ˆæœ€æ…¢ï¼‰
        self.db = None

    async def get(self, key: str) -> Optional[Any]:
        """è·å–ç¼“å­˜æ•¸æ“š"""
        # L1: æª¢æŸ¥è¨˜æ†¶é«”ç¼“å­˜
        if key in self.memory_cache:
            return self.memory_cache[key]

        # L2: æª¢æŸ¥ Redis ç¼“å­˜
        cached = await self.redis_client.get(key)
        if cached:
            value = self.deserialize(cached)
            # å›å¡«åˆ° L1
            self.memory_cache[key] = value
            return value

        # L3: ä»è³‡æ–™åº«è·å–
        value = await self.fetch_from_db(key)
        if value:
            # å¯«å…¥ L2 å’Œ L1
            await self.redis_client.setex(key, 3600, self.serialize(value))
            self.memory_cache[key] = value

        return value

    async def set(self, key: str, value: Any, ttl: int = 3600):
        """è¨­ç½®ç¼“å­˜"""
        # å¯«å…¥æ‰€æœ‰å±‚çº§
        self.memory_cache[key] = value
        await self.redis_client.setex(key, ttl, self.serialize(value))

    async def invalidate(self, key: str):
        """ä½¿ç¼“å­˜å¤±æ•ˆ"""
        # ä»æ‰€æœ‰å±‚çº§åˆªé™¤
        self.memory_cache.pop(key, None)
        await self.redis_client.delete(key)

    # ç¼“å­˜è£…é¥°å™¨
    def cached(self, ttl: int = 3600, key_prefix: str = ""):
        """ç¼“å­˜è£…é¥°å™¨"""
        def decorator(func):
            @wraps(func)
            async def wrapper(*args, **kwargs):
                # ç”Ÿæˆç¼“å­˜é”®
                cache_key = f"{key_prefix}:{func.__name__}:{hash(str(args) + str(kwargs))}"

                # å°è¯•ä»ç¼“å­˜è·å–
                cached = await self.get(cache_key)
                if cached is not None:
                    return cached

                # æ‰§è¡Œå‡½æ•°
                result = await func(*args, **kwargs)

                # å¯«å…¥ç¼“å­˜
                await self.set(cache_key, result, ttl)

                return result

            return wrapper

        return decorator

# ä½¿ç”¨ç¤ºä¾‹
cache = MultiLevelCache()

@cache.cached(ttl=1800, key_prefix="user")
async def get_user(user_id: int):
    """è·å–ä½¿ç”¨è€…è³‡è¨Šï¼ˆå¸¦ç¼“å­˜ï¼‰"""
    # å®é™…è³‡æ–™åº«æŸ¥è©¢
    return await database.fetch_user(user_id)
```

# æ›´æ–°

```python
class CacheUpdateStrategy:
# æ›´æ–°

    # 1. Cache-Asideï¼ˆæ—è·¯ç¼“å­˜ï¼‰
    async def get_with_cache_aside(self, key: str):
        """Cache-Aside ç­–ç•¥"""
        # å…ˆæŸ¥ç¼“å­˜
        cached = await self.cache.get(key)
        if cached:
            return cached

        # ç¼“å­˜æœªå‘½ä¸­ï¼ŒæŸ¥è³‡æ–™åº«
        data = await self.db.get(key)

        # å¯«å…¥ç¼“å­˜
        await self.cache.set(key, data, ttl=3600)

        return data

    async def update_with_cache_aside(self, key: str, data: dict):
# æ›´æ–°
# æ›´æ–°
        await self.db.update(key, data)

        # åˆªé™¤ç¼“å­˜ï¼ˆä¸‹æ¬¡è®€å–æ—¶é‡æ–°åŠ è½½ï¼‰
        await self.cache.delete(key)

    # 2. Write-Throughï¼ˆå†™ç©¿ï¼‰
    async def update_with_write_through(self, key: str, data: dict):
        """Write-Through ç­–ç•¥"""
# æ›´æ–°
        await asyncio.gather(
            self.db.update(key, data),
            self.cache.set(key, data, ttl=3600)
        )

    # 3. Write-Behindï¼ˆå†™å›ï¼‰
    async def update_with_write_behind(self, key: str, data: dict):
        """Write-Behind ç­–ç•¥"""
# æ›´æ–°
        await self.cache.set(key, data, ttl=3600)

# æ›´æ–°
        asyncio.create_task(self.db.update(key, data))

    # 4. Refresh-Aheadï¼ˆé¢„åˆ·æ–°ï¼‰
    async def get_with_refresh_ahead(self, key: str):
        """Refresh-Ahead ç­–ç•¥"""
        # æª¢æŸ¥ç¼“å­˜æ˜¯å¦å³å°†è¿‡æœŸ
        ttl = await self.cache.ttl(key)

        if ttl < 60:  # å‰©ä½™æ™‚é–“å°‘äº60ç§’
            # åå°åˆ·æ–°ç¼“å­˜
            asyncio.create_task(self.refresh_cache(key))

        return await self.cache.get(key)

    async def refresh_cache(self, key: str):
        """åå°åˆ·æ–°ç¼“å­˜"""
        data = await self.db.get(key)
        await self.cache.set(key, data, ttl=3600)
```

---

## ç¶²è·¯å„ªåŒ–

### é€£æ¥å¤ç”¨

```python
class ConnectionReuseOptimizer:
    """é€£æ¥å¤ç”¨å„ªåŒ–å™¨"""

    def __init__(self):
        # ä½¿ç”¨å•ä¸ª ClientSessionï¼ˆæ¨èï¼‰
        self.session = None

    async def __aenter__(self):
        """è¿›å…¥ä¸Šä¸‹æ–‡"""
        self.session = aiohttp.ClientSession(
            connector=aiohttp.TCPConnector(
                limit=100,  # æ€»é€£æ¥æ± å¤§å°
                limit_per_host=10,  # å•ä¸»æœºé€£æ¥æ•°
                keepalive_timeout=30,  # ä¿æŒé€£æ¥è¶…æ—¶
                enable_cleanup_closed=True  # æ¸…ç†é—œé–‰çš„é€£æ¥
            ),
            timeout=aiohttp.ClientTimeout(total=30)
        )
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """é€€å‡ºä¸Šä¸‹æ–‡"""
        await self.session.close()

    async def multiple_requests(self, urls: List[str]):
        """å¤šå€‹è¯·æ±‚ï¼ˆå¤ç”¨é€£æ¥ï¼‰"""
        tasks = [self.session.get(url) for url in urls]
        responses = await asyncio.gather(*tasks)

        return await asyncio.gather(*[r.json() for r in responses])

# âœ… å¥½çš„å¯¦è¸ï¼šå¤ç”¨é€£æ¥
async def good_connection_reuse():
    """å¥½çš„é€£æ¥å¤ç”¨å¯¦è¸"""
    async with ConnectionReuseOptimizer() as optimizer:
        results = await optimizer.multiple_requests([
            "https://api.example.com/data1",
            "https://api.example.com/data2",
            "https://api.example.com/data3"
        ])
        return results

# âŒ ä¸å¥½çš„å¯¦è¸ï¼šæ¯æ¬¡å‰µå»ºæ–°é€£æ¥
async def bad_connection_creation():
    """ä¸å¥½çš„é€£æ¥å‰µå»º"""
    results = []
    for url in urls:
        async with aiohttp.ClientSession() as session:
            async with session.get(url) as response:
                results.append(await response.json())
    return results
```

### è¯·æ±‚æ‰¹è™•ç†

```python
class RequestBatcher:
    """è¯·æ±‚æ‰¹è™•ç†å™¨"""

    def __init__(self, batch_size: int = 100, max_wait_time: float = 1.0):
        self.batch_size = batch_size
        self.max_wait_time = max_wait_time
        self.queue = []
        self.flush_event = asyncio.Event()

    async def add_request(self, request_data: dict):
        """æ–°å¢è¯·æ±‚åˆ°æ‰¹æ¬¡"""
        self.queue.append(request_data)

        if len(self.queue) >= self.batch_size:
            await self.flush()
        else:
            # ç­‰å¾…æ›´å¤šè¯·æ±‚æˆ–è¶…æ—¶
            try:
                await asyncio.wait_for(
                    self.flush_event.wait(),
                    timeout=self.max_wait_time
                )
            except asyncio.TimeoutError:
                await self.flush()

    async def flush(self):
        """åˆ·æ–°æ‰¹æ¬¡"""
        if not self.queue:
            return

        batch = self.queue[:]
        self.queue.clear()
        self.flush_event.clear()

        # æ‰¹é‡è™•ç†
        results = await self.process_batch(batch)

        return results

    async def process_batch(self, batch: List[dict]):
        """è™•ç†æ‰¹æ¬¡è¯·æ±‚"""
        # å°†å¤šå€‹å•ç‹¬çš„è¯·æ±‚åˆå¹¶ä¸ºä¸€ä¸ªæ‰¹é‡è¯·æ±‚
        request_body = {
            "requests": [
                {
                    "id": item["id"],
                    "data": item["data"]
                }
                for item in batch
            ]
        }

        async with aiohttp.ClientSession() as session:
            async with session.post(
                "https://api.example.com/batch",
                json=request_body
            ) as response:
                result = await response.json()

        return result["responses"]
```

---

## è¨˜æ†¶é«”å„ªåŒ–

### å¯¹è±¡æ± 

```python
from collections import deque
import weakref

class ObjectPool:
    """å¯¹è±¡æ± """

    def __init__(self, factory: Callable, max_size: int = 100):
        self.factory = factory
        self.max_size = max_size
        self.pool = deque()
        self.lock = asyncio.Lock()

    async def acquire(self) -> Any:
        """è·å–å¯¹è±¡"""
        async with self.lock:
            if self.pool:
                return self.pool.popleft()
            else:
                return self.factory()

    async def release(self, obj: Any):
        """é‡Šæ”¾å¯¹è±¡"""
        async with self.lock:
            if len(self.pool) < self.max_size:
                # é‡ç½®å¯¹è±¡ç‹€æ…‹
                if hasattr(obj, 'reset'):
                    obj.reset()

                self.pool.append(obj)

    async def __aenter__(self):
        """è¿›å…¥ä¸Šä¸‹æ–‡"""
        self.obj = await self.acquire()
        return self.obj

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """é€€å‡ºä¸Šä¸‹æ–‡"""
        await self.release(self.obj)

# ä½¿ç”¨ç¤ºä¾‹
# å‰µå»ºè³‡æ–™åº«é€£æ¥æ± 
db_pool = ObjectPool(
    factory=lambda: DatabaseConnection(),
    max_size=20
)

async def query_with_pool():
    """ä½¿ç”¨é€£æ¥æ± æŸ¥è©¢"""
    async with db_pool as connection:
        result = await connection.query("SELECT * FROM users")
        return result
```

### è¨˜æ†¶é«”å‹å¥½çš„æ•¸æ“šç»“æ„

```python
import sys
from typing import Iterator, List, Dict

class MemoryEfficientStructures:
    """è¨˜æ†¶é«”é«˜æ•ˆçš„æ•¸æ“šç»“æ„"""

    # âœ… ä½¿ç”¨ç”Ÿæˆå™¨è€Œéåˆ—è¡¨
    def process_large_dataset_generator(self, data_source: Iterator) -> Iterator:
        """ä½¿ç”¨ç”Ÿæˆå™¨è™•ç†å¤§æ•¸æ“šï¼ˆè¨˜æ†¶é«”å‹å¥½ï¼‰"""
        for item in data_source:
            processed = self.process_item(item)
            yield processed

    # âŒ ä½¿ç”¨åˆ—è¡¨ï¼ˆè¨˜æ†¶é«”æ¶ˆè€—å¤§ï¼‰
    def process_large_dataset_list(self, data_source: List) -> List:
        """ä½¿ç”¨åˆ—è¡¨è™•ç†å¤§æ•¸æ“šï¼ˆè¨˜æ†¶é«”ä¸å‹å¥½ï¼‰"""
        results = []
        for item in data_source:
            processed = self.process_item(item)
            results.append(processed)
        return results

    # âœ… ä½¿ç”¨ __slots__ å‡å°‘è¨˜æ†¶é«”
    class Point:
        """ä½¿ç”¨ __slots__ çš„ç‚¹ç±»"""
        __slots__ = ['x', 'y', 'z']

        def __init__(self, x: float, y: float, z: float = 0):
            self.x = x
            self.y = y
            self.z = z

    # âŒ ä¸ä½¿ç”¨ __slots__
    class PointLarge:
        """ä¸ä½¿ç”¨ __slots__ çš„ç‚¹ç±»"""
        def __init__(self, x: float, y: float, z: float = 0):
            self.x = x
            self.y = y
            self.z = z

    # æ¯”è¾ƒï¼šå‰µå»º100ä¸‡ä¸ªç‚¹çš„è¨˜æ†¶é«”ä½¿ç”¨
    def compare_memory_usage(self):
        """æ¯”è¾ƒè¨˜æ†¶é«”ä½¿ç”¨"""
        # ä½¿ç”¨ __slots__
        points_slots = [self.Point(i, i*2) for i in range(1000000)]
        memory_slots = sys.getsizeof(points_slots)

        # ä¸ä½¿ç”¨ __slots__
        points_large = [self.PointLarge(i, i*2) for i in range(1000000)]
        memory_large = sys.getsizeof(points_large)

        print(f"ä½¿ç”¨ __slots__: {memory_slots / 1024 / 1024:.2f} MB")
        print(f"ä¸ä½¿ç”¨ __slots__: {memory_large / 1024 / 1024:.2f} MB")
# ç‰ˆæœ¬
```

---

## å¹¶å‘æ§åˆ¶

### é™æµå™¨

```python
import asyncio
from collections import deque
from datetime import datetime, timedelta

class RateLimiter:
    """é™æµå™¨"""

    def __init__(self, max_calls: int, time_window: float):
        self.max_calls = max_calls
        self.time_window = time_window
        self.calls = deque()
        self.lock = asyncio.Lock()

    async def acquire(self):
        """è·å–è®¿é—®æƒé™"""
        async with self.lock:
            now = datetime.now()

            # æ¸…ç†è¿‡æœŸçš„èª¿ç”¨è¨˜éŒ„
            while self.calls and (now - self.calls[0]).total_seconds() > self.time_window:
                self.calls.popleft()

            # æª¢æŸ¥æ˜¯å¦è¶…è¿‡é™åˆ¶
            if len(self.calls) >= self.max_calls:
                # è¨ˆç®—éœ€è¦ç­‰å¾…çš„æ™‚é–“
                wait_time = (self.calls[0] + timedelta(seconds=self.time_window) - now).total_seconds()
                await asyncio.sleep(wait_time)

            # è¨˜éŒ„è¿™æ¬¡èª¿ç”¨
            self.calls.append(now)

# ä½¿ç”¨è£…é¥°å™¨é™æµ
def rate_limit(max_calls: int, time_window: float):
    """é™æµè£…é¥°å™¨"""
    limiter = RateLimiter(max_calls, time_window)

    def decorator(func):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            await limiter.acquire()
            return await func(*args, **kwargs)
        return wrapper

    return decorator

# ä½¿ç”¨ç¤ºä¾‹
@rate_limit(max_calls=10, time_window=60)  # æ¯åˆ†é’Ÿæœ€å¤š10æ¬¡
async def rate_limited_api_call():
    """é™æµçš„APIèª¿ç”¨"""
    # ... APIèª¿ç”¨é€»è¾‘
    pass
```

### å·¥ä½œé˜Ÿåˆ—

```python
import asyncio
from typing import Callable, Any

class WorkerQueue:
    """å·¥ä½œé˜Ÿåˆ—"""

    def __init__(self, num_workers: int = 4):
        self.num_workers = num_workers
        self.queue = asyncio.Queue()
        self.workers = []
        self.running = False

    async def start(self):
        """å•Ÿå‹•å·¥ä½œé˜Ÿåˆ—"""
        self.running = True
        self.workers = [
            asyncio.create_task(self._worker(i))
            for i in range(self.num_workers)
        ]

    async def stop(self):
        """åœæ­¢å·¥ä½œé˜Ÿåˆ—"""
        self.running = False

        # ç­‰å¾…æ‰€æœ‰ä»»å‹™å®Œæˆ
        for _ in range(self.num_workers):
            await self.queue.put(None)  # ç™¼é€åœæ­¢ä¿¡å·

        await asyncio.gather(*self.workers)

    async def submit(self, task: Callable, *args, **kwargs):
        """æäº¤ä»»å‹™"""
        await self.queue.put((task, args, kwargs))

    async def _worker(self, worker_id: int):
        """å·¥ä½œè¿›ç¨‹"""
        while self.running:
            item = await self.queue.get()

            # åœæ­¢ä¿¡å·
            if item is None:
                break

            task, args, kwargs = item

            try:
                result = await task(*args, **kwargs)
                # è™•ç†çµæœ
            except Exception as e:
                # è™•ç†é”™è¯¯
                print(f"Worker {worker_id} error: {e}")

            self.queue.task_done()

# ä½¿ç”¨ç¤ºä¾‹
queue = WorkerQueue(num_workers=8)
await queue.start()

# æäº¤ä»»å‹™
for i in range(100):
    await queue.submit(process_task, i)

# ç­‰å¾…æ‰€æœ‰ä»»å‹™å®Œæˆ
await queue.queue.join()
await queue.stop()
```

---

## ç›£æ§ä¸è°ƒä¼˜

### æ•ˆèƒ½ç›£æ§

```python
from prometheus_client import Counter, Histogram, Gauge
import time

class PerformanceMonitor:
    """æ•ˆèƒ½ç›£æ§å™¨"""

    def __init__(self):
        # è¯·æ±‚è®¡æ•°å™¨
        self.request_count = Counter(
            'requests_total',
            'Total number of requests',
            ['method', 'endpoint', 'status']
        )

        # è¯·æ±‚å»¶é²ç›´æ–¹å›¾
        self.request_latency = Histogram(
            'request_duration_seconds',
            'Request latency',
            ['endpoint']
        )

        # å¹¶å‘æ•°æŒ‡æ ‡
        self.concurrent_requests = Gauge(
            'concurrent_requests',
            'Number of concurrent requests'
        )

        # è¨˜æ†¶é«”ä½¿ç”¨
        self.memory_usage = Gauge(
            'memory_usage_bytes',
            'Memory usage in bytes'
        )

    def track_request(self, func):
        """è¯·æ±‚è¿½è¸ªè£…é¥°å™¨"""
        @wraps(func)
        async def wrapper(*args, **kwargs):
            start_time = time.time()

            # å¢åŠ å¹¶å‘è®¡æ•°
            self.concurrent_requests.inc()

            try:
                result = await func(*args, **kwargs)

                # è¨˜éŒ„æˆåŠŸè¯·æ±‚
                duration = time.time() - start_time
                self.request_count.labels(
                    method='GET',
                    endpoint=func.__name__,
                    status='success'
                ).inc()
                self.request_latency.labels(
                    endpoint=func.__name__
                ).observe(duration)

                return result

            except Exception as e:
                # è¨˜éŒ„å¤±è´¥è¯·æ±‚
                duration = time.time() - start_time
                self.request_count.labels(
                    method='GET',
                    endpoint=func.__name__,
                    status='error'
                ).inc()
                raise

            finally:
                # å‡å°‘å¹¶å‘è®¡æ•°
                self.concurrent_requests.dec()

        return wrapper

    async def collect_metrics(self):
        """æ”¶é›†æ•ˆèƒ½æŒ‡æ ‡"""
        import psutil

        # è¨˜æ†¶é«”ä½¿ç”¨
        process = psutil.Process()
        memory_info = process.memory_info()
        self.memory_usage.set(memory_info.rss)

        # å¯ä»¥æ–°å¢æ›´å¤šæŒ‡æ ‡...
```

### è‡ªå‹•è°ƒä¼˜

```python
class AutoTuner:
    """è‡ªå‹•è°ƒä¼˜å™¨"""

    def __init__(self):
        self.performance_history = []

    async def monitor_and_tune(self):
        """ç›£æ§å¹¶è‡ªå‹•è°ƒä¼˜"""
        while True:
            # æ”¶é›†æ•ˆèƒ½æŒ‡æ ‡
            metrics = await self.collect_metrics()

            # è¨˜éŒ„æ­·å²
            self.performance_history.append(metrics)

# åˆ†æ
            if len(self.performance_history) > 10:
                analysis = self.analyze_performance()

# åˆ†æ
                if analysis["needs_tuning"]:
                    await self.apply_tuning(analysis)

            # ç­‰å¾…ä¸‹æ¬¡æª¢æŸ¥
            await asyncio.sleep(60)

    def analyze_performance(self) -> dict:
# åˆ†æ
        recent = self.performance_history[-10:]

        # è¨ˆç®—å¹³å‡éŸ¿æ‡‰æ™‚é–“
        avg_latency = sum(m["latency"] for m in recent) / len(recent)

        # è¨ˆç®—é”™è¯¯ç‡
        error_rate = sum(m["errors"] for m in recent) / sum(m["requests"] for m in recent)

        # æª¢æŸ¥æ˜¯å¦éœ€è¦è°ƒä¼˜
        needs_tuning = (
            avg_latency > 0.5 or  # éŸ¿æ‡‰æ™‚é–“è¶…è¿‡500ms
            error_rate > 0.05 or   # é”™è¯¯ç‡è¶…è¿‡5%
            m["cpu_usage"] > 80     # CPUä½¿ç”¨ç‡è¶…è¿‡80%
        )

        return {
            "avg_latency": avg_latency,
            "error_rate": error_rate,
            "needs_tuning": needs_tuning,
            "suggested_actions": self.suggest_actions(recent)
        }

    def suggest_actions(self, metrics: list) -> list:
        """å»ºè­°è°ƒä¼˜æªæ–½"""
        actions = []

        avg_latency = sum(m["latency"] for m in metrics) / len(metrics)

        if avg_latency > 0.5:
            actions.append("å¢åŠ å·¥ä½œçº¿ç¨‹æ•°")
            actions.append("å¯ç”¨ç¼“å­˜")
            actions.append("å„ªåŒ–è³‡æ–™åº«æŸ¥è©¢")

        if m["memory_usage"] > 2000000000:  # 2GB
            actions.append("å‡å°‘è¨˜æ†¶é«”ä½¿ç”¨")
            actions.append("å¢åŠ è¨˜æ†¶é«”é™åˆ¶")

        return actions

    async def apply_tuning(self, analysis: dict):
        """æ‡‰ç”¨ç¨‹å¼è°ƒä¼˜æªæ–½"""
        for action in analysis["suggested_actions"]:
            print(f"æ‡‰ç”¨ç¨‹å¼è°ƒä¼˜: {action}")
            # è¿™é‡Œå¯¦ç¾å…·ä½“çš„è°ƒä¼˜é€»è¾‘
```

---

## å„ªåŒ–æ¡ˆä¾‹

### æ¡ˆä¾‹1ï¼šä»100mså„ªåŒ–åˆ°10ms

**å•é¡Œæè¿°ï¼š**
ä¸€ä¸ªä½¿ç”¨è€…æ•¸æ“šæŸ¥è©¢æ¥å£å¹³å‡éŸ¿æ‡‰æ™‚é–“ä¸º100msï¼Œéœ€è¦å„ªåŒ–åˆ°10msä»¥å†…ã€‚

**å„ªåŒ–éç¨‹ï¼š**

# åˆ†æ
```python
# ç™¼ç¾ç“¶é¢ˆ
- è³‡æ–™åº«æŸ¥è©¢: 80ms
- æ•¸æ“šåºåˆ—åŒ–: 15ms
- ç¶²è·¯å‚³è¼¸: 5ms
```

2. **å„ªåŒ–æªæ–½**

```python
# å„ªåŒ–1ï¼šæ–°å¢ç´¢å¼•
CREATE INDEX idx_users_email ON users(email);

# å„ªåŒ–2ï¼šä½¿ç”¨æ‰¹é‡æŸ¥è©¢
async def get_users_batch(self, user_ids: List[int]) -> List[Dict]:
    # å„ªåŒ–å‰ï¼šå¾ªç¯æŸ¥è©¢ï¼ˆNæ¬¡ï¼‰
    # for user_id in user_ids:
    #     user = await self.db.query("SELECT * FROM users WHERE id = ?", [user_id])

    # å„ªåŒ–åï¼šä¸€æ¬¡æŸ¥è©¢
    query = "SELECT * FROM users WHERE id = ANY(:user_ids)"
    return await self.db.fetch_all(query, {"user_ids": user_ids})

# å„ªåŒ–3ï¼šä½¿ç”¨æ›´å¿«çš„åºåˆ—åŒ–åº“
import orjson  # æ¯”jsonå¿«3-5å€

async def serialize_data(data: dict) -> bytes:
    return orjson.dumps(data)

# å„ªåŒ–4ï¼šæ–°å¢ç¼“å­˜
@cached(ttl=300, key_prefix="user")
async def get_user_cached(self, user_id: int) -> Dict:
    return await self.get_user_from_db(user_id)
```

3. **å„ªåŒ–çµæœ**
```python
# å„ªåŒ–åæ•ˆèƒ½
- è³‡æ–™åº«æŸ¥è©¢: 5ms (ä½¿ç”¨ç´¢å¼•å’Œç¼“å­˜)
- æ•¸æ“šåºåˆ—åŒ–: 2ms (ä½¿ç”¨orjson)
- ç¶²è·¯å‚³è¼¸: 3ms
# æ€»è®¡: 10ms
```

### æ¡ˆä¾‹2ï¼šè™•ç†å¤§æ•¸æ“šé›†

**å•é¡Œæè¿°ï¼š**
# åˆ†æ

**å„ªåŒ–æ–¹æ¡ˆï¼š**

```python
class BigDataProcessor:
    """å¤§æ•¸æ“šè™•ç†å™¨"""

    # âŒ åŸå§‹å¯¦ç¾ï¼ˆæ…¢ï¼‰
    async def process_slow(self, all_records: list):
        """åŸå§‹å¯¦ç¾"""
        results = []
        for record in all_records:  # 1äº¿æ¬¡å¾ªç¯
            processed = await self.process_record(record)
            results.append(processed)
        return results

    # âœ… å„ªåŒ–å¯¦ç¾ï¼ˆå¿«ï¼‰
    async def process_fast(self, all_records: list):
        """å„ªåŒ–å¯¦ç¾"""
        # 1. ä½¿ç”¨åˆ†æ‰¹è™•ç†
        batch_size = 10000
        batches = [all_records[i:i+batch_size]
                   for i in range(0, len(all_records), batch_size)]

        # 2. å¹¶å‘è™•ç†æ‰¹æ¬¡
        semaphore = asyncio.Semaphore(10)  # 10ä¸ªå¹¶å‘

        async def process_batch_with_semaphore(batch):
            async with semaphore:
                # ä½¿ç”¨ç”Ÿæˆå™¨ç¯€çœè¨˜æ†¶é«”
                return [self.process_record(r) for r in batch]

        # 3. å¹¶å‘æ‰§è¡Œ
        results = await asyncio.gather(
            *[process_batch_with_semaphore(batch) for batch in batches]
        )

        # å±•å¹³çµæœ
        return [item for batch in results for item in batch]

    # 4. ä½¿ç”¨è³‡æ–™åº«æ¸¸æ ‡ï¼ˆä¸åŠ è½½å…¨éƒ¨æ•¸æ“šï¼‰
    async def process_with_cursor(self):
        """ä½¿ç”¨æ¸¸æ ‡è™•ç†"""
        async with self.db.cursor() as cursor:
            await cursor.execute("SELECT * FROM large_table")

            batch = []
            async for record in cursor:
                batch.append(record)

                if len(batch) >= 10000:
                    # è™•ç†æ‰¹æ¬¡
                    await self.process_batch(batch)
                    batch = []

            # è™•ç†å‰©ä½™çš„
            if batch:
                await self.process_batch(batch)
```

**å„ªåŒ–çµæœï¼š**
- åŸå§‹å¯¦ç¾ï¼š~7200ç§’ï¼ˆ2å°æ—¶ï¼‰
- å„ªåŒ–å¯¦ç¾ï¼š~600ç§’ï¼ˆ10åˆ†é’Ÿï¼‰
- æ•ˆèƒ½æå‡ï¼š**12å€**

---

## æ•ˆèƒ½å„ªåŒ–æª¢æŸ¥æ¸…å–®

### ä»£ç¢¼å±‚é¢
- [ ] ä½¿ç”¨å¼‚æ­¥IOè€Œéé˜»å¡IO
- [ ] é¿å…åœ¨å¼‚æ­¥å‡½æ•°ä¸­ä½¿ç”¨é˜»å¡æ“ä½œ
- [ ] ä½¿ç”¨ asyncio.gather å¹¶å‘æ‰§è¡Œç‹¬ç«‹ä»»å‹™
- [ ] å®æ–½é€‚å½“çš„ç¼“å­˜ç­–ç•¥
- [ ] å„ªåŒ–ç®—æ³•å¤æ‚åº¦

### è³‡æ–™åº«å±‚é¢
- [ ] ä¸ºå¸¸ç”¨æŸ¥è©¢æ–°å¢ç´¢å¼•
- [ ] é¿å…N+1æŸ¥è©¢å•é¡Œ
- [ ] ä½¿ç”¨æ‰¹é‡æ“ä½œ
- [ ] åˆç†ä½¿ç”¨JOIN
- [ ] å®æ–½æŸ¥è©¢çµæœç¼“å­˜

### ç¶²è·¯å±‚é¢
- [ ] å¤ç”¨HTTPé€£æ¥
- [ ] ä½¿ç”¨è¯·æ±‚æ‰¹è™•ç†
- [ ] å¯ç”¨HTTP/2
- [ ] å®æ–½è¯·æ±‚å‹ç¼©
- [ ] ä½¿ç”¨CDNåŠ é€Ÿé™æ€è³‡æº

### è¨˜æ†¶é«”å±‚é¢
- [ ] ä½¿ç”¨ç”Ÿæˆå™¨è€Œéåˆ—è¡¨
- [ ] ä½¿ç”¨ __slots__ å‡å°‘å¯¹è±¡è¨˜æ†¶é«”
- [ ] å®æ–½å¯¹è±¡æ± 
- [ ] åŠæ—¶é‡Šæ”¾ä¸å†ä½¿ç”¨çš„è³‡æº
- [ ] ç›£æ§è¨˜æ†¶é«”ä½¿ç”¨

### ç›£æ§å±‚é¢
- [ ] è¨˜éŒ„é—œéµæ•ˆèƒ½æŒ‡æ ‡
- [ ] è¨­ç½®æ•ˆèƒ½å‘Šè­¦
# åˆ†æ
- [ ] é€²è¡Œè´Ÿè½½æ¸¬è©¦
- [ ] å®æ–½è‡ªå‹•è°ƒä¼˜

---

## åƒè€ƒèµ„æ–™

### å·¥å…·
# åˆ†æ
# åˆ†æ
- [prometheus_client](https://pypi.org/project/prometheus_client/) - Prometheus å®¢æˆ·ç«¯

### æ–‡æª”
- [Python asyncio æ–‡æª”](https://docs.python.org/3/library/asyncio.html)
- [asyncio æ•ˆèƒ½æœ€ä½³å¯¦è¸](https://docs.python.org/3/library/asyncio-sync.html)

### ç›¸é—œæ–‡æª”
# æŒ‡å—
# æŒ‡å—
- [[å¤–æ›å®‰å…¨è¦ç¯„]] - å¤–æ›å®‰å…¨é–‹ç™¼è¦ç¯„

---

*å‰µå»ºæ™‚é–“: 2026-01-31*
# æ›´æ–°
*åˆ†é¡: 3 Resources*
