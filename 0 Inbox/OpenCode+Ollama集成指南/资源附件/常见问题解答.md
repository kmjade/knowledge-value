# OpenCode + Ollama 常见问题解答
# OpenCode + Ollama Frequently Asked Questions

## 🚀 安装与设置 / Installation & Setup

### Q1: 系统要求是什么？

**A**: 基本要求：
- **操作系统**: Linux, macOS, Windows
- **内存**: 最少8GB，推荐16GB+
- **GPU**: 可选但推荐，8GB+ VRAM
- **存储**: 至少20GB可用空间
- **软件**: Node.js v18+, Python 3.8+ (可选)

### Q2: 如何在没有GPU的机器上使用？

**A**: 可以使用CPU模式，但性能较慢：
```bash
# CPU模式启动
export OLLAMA_NUM_PARALLEL=1  # 减少并行
export OLLAMA_MAX_LOADED_MODELS=1  # 减少加载模型数
ollama serve
```

**推荐模型**:
- `qwen2.5:3b` - 轻量级，CPU友好
- `qwen2.5:1.5b` - 最轻量选择

### Q3: 安装过程中权限被拒绝？

**A**: 解决方案：
```bash
# Linux/macOS
chmod +x install-opencode-ollama.sh
sudo ./install-opencode-ollama.sh

# 或者使用用户目录安装
npm config set prefix ~/.npm-global
export PATH=~/.npm-global/bin:$PATH
npm install -g @opencode-ai/cli
```

### Q4: 如何卸载和重新安装？

**A**: 完全卸载步骤：
```bash
# 停止服务
pkill ollama
pkill opencode

# 卸载OpenCode
npm uninstall -g @opencode-ai/cli

# 删除Ollama (Linux/macOS)
sudo rm -rf /usr/local/bin/ollama
sudo rm -rf ~/.ollama

# 删除配置文件
rm -rf ~/.config/opencode

# 清理环境变量
编辑 ~/.bashrc 或 ~/.zshrc，删除相关行
```

---

## 🔧 配置问题 / Configuration Issues

### Q5: OpenCode找不到本地模型？

**A**: 检查清单：
1. **Ollama服务运行**:
   ```bash
   curl -s http://localhost:11434/api/tags
   ```

2. **模型已下载**:
   ```bash
   ollama list
   ```

3. **配置文件正确**:
   ```bash
   cat ~/.config/opencode/opencode.json | jq .
   ```

4. **重载OpenCode**:
   ```bash
   opencode --reload-config
   ```

### Q6: 配置文件JSON格式错误？

**A**: 常见错误和修复：
```json
// ❌ 错误：缺少逗号
{
  "model": "ollama/qwen2.5-coder:7b"
  "provider": {  // 这里缺少逗号
    "ollama": { ... }
  }
}

// ✅ 正确
{
  "model": "ollama/qwen2.5-coder:7b",
  "provider": {
    "ollama": { ... }
  }
}
```

**验证工具**:
```bash
# 使用jq验证JSON
cat ~/.config/opencode/opencode.json | jq .

# 或使用Python验证
python3 -m json.tool ~/.config/opencode/opencode.json
```

### Q7: 如何切换不同的模型？

**A**: 方法1: 修改配置文件
```json
{
  "model": "ollama/qwen2.5-coder:14b"  // 更改这里
}
```

方法2: 命令行指定
```bash
opencode --model ollama/qwen2.5:7b
```

方法3: OpenCode界面
```
/models  # 在OpenCode中执行此命令
```

---

## 🤖 模型相关问题 / Model Related Issues

### Q8: 模型下载失败或中断？

**A**: 解决方案：
```bash
# 方法1: 重新下载
ollama pull qwen2.5-coder:7b

# 方法2: 使用代理（如果需要）
export https_proxy=http://proxy:port
ollama pull qwen2.5-coder:7b

# 方法3: 手动下载模型文件
wget https://ollama.com/models/qwen2.5-coder:7b.gguf
mv qwen2.5-coder:7b.gguf ~/.ollama/models/
```

### Q9: 模型占用内存过大？

**A**: 优化策略：
```bash
# 1. 设置最大内存使用
export OLLAMA_MAX_LOADED_MODELS=1

# 2. 使用更小的模型
ollama pull qwen2.5:3b

# 3. 启用模型压缩
ollama run qwen2.5-coder:7b
/set parameter num_ctx 4096  # 减少上下文
```

### Q10: 如何更新模型？

**A**: 更新步骤：
```bash
# 1. 查看当前版本
ollama show qwen2.5-coder:7b

# 2. 删除旧版本（可选）
ollama rm qwen2.5-coder:7b

# 3. 下载最新版本
ollama pull qwen2.5-coder:7b:latest

# 4. 验证更新
ollama show qwen2.5-coder:7b
```

---

## ⚡ 性能问题 / Performance Issues

### Q11: 响应速度很慢？

**A**: 性能优化：
```bash
# 1. GPU优化
export OLLAMA_GPU_MEMORY_FRACTION=0.9
export OLLAMA_NUM_PARALLEL=4

# 2. 减少上下文大小
ollama run qwen2.5-coder:7b
/set parameter num_ctx 2048

# 3. 使用量化模型
ollama pull qwen2.5-coder:7b-q4_0

# 4. 预加载模型
export OLLAMA_KEEP_ALIVE=24h
```

### Q12: 内存不足错误？

**A**: 内存管理：
```bash
# 1. 监控内存使用
watch -n 1 'ps aux | grep ollama'

# 2. 限制并发
export OLLAMA_NUM_PARALLEL=1

# 3. 使用交换文件
sudo swapon --show
sudo fallocate -l 4G /swapfile
sudo chmod 600 /swapfile
sudo mkswap /swapfile
sudo swapon /swapfile

# 4. 重启Ollama服务
pkill ollama && ollama serve
```

### Q13: GPU未使用？

**A**: GPU启用检查：
```bash
# 1. 检查GPU驱动
nvidia-smi

# 2. 检查CUDA
nvidia-cuda-version-control

# 3. 强制使用GPU
export OLLAMA_GPU=1

# 4. 重新安装（如果需要）
# 卸载
sudo rm -rf /usr/local/bin/ollama
# 重新安装（会自动检测GPU）
curl -fsSL https://ollama.ai/install.sh | sh
```

---

## 🔧 功能问题 / Functional Issues

### Q14: 工具调用不工作？

**A**: 检查和修复：
```bash
# 1. 确认模型支持工具调用
ollama show qwen2.5-coder:7b | grep "tool"

# 2. 使用支持工具的模型
# 推荐模型：
# - qwen2.5-coder:7b
# - qwen2.5-coder:14b  
# - deepseek-coder:6.7b

# 3. 检查OpenCode配置
grep -A 10 '"tools"' ~/.config/opencode/opencode.json
```

### Q15: 文件操作权限错误？

**A**: 权限设置：
```bash
# 1. 检查文件权限
ls -la /path/to/your/files

# 2. 设置正确权限
chmod -R 755 /path/to/your/files
chown -R $USER:$USER /path/to/your/files

# 3. OpenCode权限配置
# 在配置文件中添加：
{
  "permissions": {
    "fileSystem": {
      "allowedPaths": ["./", "/absolute/path"],
      "umask": "022"
    }
  }
}
```

### Q16: 上下文窗口不足？

**A**: 扩展上下文：
```bash
# 1. 创建大上下文模型变体
ollama run qwen2.5-coder:7b
/set parameter num_ctx 16384
/save qwen2.5-coder:7b-16k

# 2. 配置OpenCode使用大上下文模型
{
  "model": "ollama/qwen2.5-coder:7b-16k"
}

# 3. 分块处理大文件
# 将大文件分成小块分别处理
```

---

## 🌐 网络连接问题 / Network Connection Issues

### Q17: 无法连接Ollama API？

**A**: 网络诊断：
```bash
# 1. 检查服务运行
curl -v http://localhost:11434/api/tags

# 2. 检查端口占用
netstat -tlnp | grep 11434
lsof -i :11434

# 3. 检查防火墙
sudo ufw status  # Ubuntu
sudo firewall-cmd --list-all  # CentOS

# 4. 更改端口（如果需要）
export OLLAMA_HOST=0.0.0.0:11435
ollama serve
```

### Q18: 代理设置问题？

**A**: 代理配置：
```bash
# 1. 设置HTTP代理
export http_proxy=http://proxy.server:port
export https_proxy=http://proxy.server:port

# 2. Git代理
git config --global http.proxy http://proxy.server:port
git config --global https.proxy http://proxy.server:port

# 3. Ollama代理
export OLLAMA_PROXY=http://proxy.server:port

# 4. Node.js代理
npm config set proxy http://proxy.server:port
npm config set https-proxy http://proxy.server:port
```

---

## 🐛 错误代码详解 / Error Code Explanation

### 错误代码 1001: 连接超时
**原因**: OpenCode无法连接到Ollama服务
**解决**: 
1. 确认Ollama正在运行
2. 检查端口11434是否被占用
3. 检查防火墙设置

### 错误代码 1002: 模型未找到
**原因**: 指定的模型不存在或未下载
**解决**:
```bash
ollama list  # 查看可用模型
ollama pull model_name  # 下载模型
```

### 错误代码 1003: 内存不足
**原因**: 系统内存不足以运行模型
**解决**:
1. 使用更小的模型
2. 增加虚拟内存
3. 关闭其他程序释放内存

### 错误代码 1004: 配置文件错误
**原因**: opencode.json格式或内容错误
**解决**:
1. 使用JSON验证工具检查格式
2. 参考示例配置文件
3. 重新生成配置文件

---

## 🔧 高级问题 / Advanced Issues

### Q19: 如何设置多用户使用？

**A**: 多用户配置：
```bash
# 1. 系统级安装Ollama
sudo curl -fsSL https://ollama.ai/install.sh | sh
sudo usermod -a -G ollama $USER

# 2. 共享模型目录
sudo mkdir -p /opt/ollama/models
sudo chown -R ollama:ollama /opt/ollama
sudo chmod -R 755 /opt/ollama

# 3. 配置环境变量
export OLLAMA_MODELS=/opt/ollama/models
```

### Q20: 如何配置Docker部署？

**A**: Docker配置：
```dockerfile
# Dockerfile
FROM nvidia/cuda:12.1-devel-ubuntu22.04

RUN apt-get update && apt-get install -y curl git
RUN curl -fsSL https://ollama.ai/install.sh | sh
RUN ollama serve &

# 构建
docker build -t opencode-ollama .

# 运行
docker run --gpus all -p 11434:11434 opencode-ollama
```

### Q21: 如何监控系统性能？

**A**: 监控脚本：
```bash
#!/bin/bash
# monitor.sh
while true; do
    echo "$(date): GPU=$(nvidia-smi --query-gpu=utilization.gpu --format=csv,noheader,nounits)% RAM=$(free | grep Mem | awk '{printf "%.1f%%", $3/$2 * 100.0}')"
    sleep 30
done
```

---

## 📚 学习资源 / Learning Resources

### 官方文档
- **OpenCode文档**: https://opencode.ai/docs
- **Ollama文档**: https://github.com/ollama/ollama/blob/main/README.md
- **模型库**: https://ollama.ai/library

### 社区资源
- **Discord社区**: https://opencode.ai/discord
- **GitHub讨论**: https://github.com/ollama/ollama/discussions
- **Reddit**: https://reddit.com/r/LocalLLaMA

### 教程和视频
- **YouTube教程**: 搜索"OpenCode Ollama tutorial"
- **博客文章**: Medium、Dev.to上的相关文章
- **GitHub项目**: 示例项目和配置

---

## 📞 获取更多帮助 / Getting More Help

### 在线支持
- **GitHub Issues**: https://github.com/anomalyco/opencode/issues
- **官方论坛**: https://opencode.ai/community
- **Stack Overflow**: 标签 `opencode` `ollama`

### 报告问题时请包含
1. **系统信息**: 操作系统、硬件配置
2. **软件版本**: OpenCode、Ollama、模型版本
3. **错误信息**: 完整的错误日志
4. **重现步骤**: 详细的操作步骤
5. **配置文件**: 去除敏感信息后的配置

---

> [!tip] 💡 预防措施 / Preventive Measures
> - 定期备份配置文件
> - 保存重要日志信息
> - 关注官方更新通知
> - 参与社区讨论获取最新信息

---

*此FAQ持续更新，欢迎贡献问题和解决方案*